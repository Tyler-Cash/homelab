2023-07-19 23:11:56.208437 I | rookcmd: starting Rook v1.12.0 with arguments '/usr/local/bin/rook ceph operator'
2023-07-19 23:11:56.208485 I | rookcmd: flag values: --enable-machine-disruption-budget=false, --help=false, --kubeconfig=, --log-level=INFO
2023-07-19 23:11:56.208487 I | cephcmd: starting Rook-Ceph operator
2023-07-19 23:11:56.444301 I | cephcmd: base ceph version inside the rook operator image is "ceph version 17.2.6 (d7ff0d10654d2280e08f1ab989c7cdf3064446a5) quincy (stable)"
2023-07-19 23:11:56.470536 I | op-k8sutil: ROOK_CURRENT_NAMESPACE_ONLY="false" (env var)
2023-07-19 23:11:56.470546 I | operator: watching all namespaces for Ceph CRs
2023-07-19 23:11:56.470562 I | operator: setting up schemes
2023-07-19 23:11:56.471859 I | operator: setting up the controller-runtime manager
2023-07-19 23:11:56.481273 I | op-k8sutil: ROOK_DISABLE_ADMISSION_CONTROLLER="true" (configmap)
2023-07-19 23:11:56.481335 I | operator: delete webhook resources since webhook is disabled
2023-07-19 23:11:56.481340 I | operator: deleting validating webhook rook-ceph-webhook
2023-07-19 23:11:56.497663 I | operator: deleting webhook cert manager Certificate rook-admission-controller-cert
2023-07-19 23:11:56.528757 I | operator: deleting webhook cert manager Issuer "selfsigned-issuer"
2023-07-19 23:11:56.575456 I | operator: deleting validating webhook service "rook-ceph-admission-controller"
2023-07-19 23:11:56.589944 I | ceph-cluster-controller: successfully started
2023-07-19 23:11:56.590225 I | ceph-cluster-controller: enabling hotplug orchestration
2023-07-19 23:11:56.590253 I | ceph-nodedaemon-controller: successfully started
2023-07-19 23:11:56.590262 I | ceph-block-pool-controller: successfully started
2023-07-19 23:11:56.590273 I | ceph-object-store-user-controller: successfully started
2023-07-19 23:11:56.590286 I | ceph-object-realm-controller: successfully started
2023-07-19 23:11:56.590295 I | ceph-object-zonegroup-controller: successfully started
2023-07-19 23:11:56.590304 I | ceph-object-zone-controller: successfully started
2023-07-19 23:11:56.590408 I | ceph-object-controller: successfully started
2023-07-19 23:11:56.590429 I | ceph-file-controller: successfully started
2023-07-19 23:11:56.590451 I | ceph-nfs-controller: successfully started
2023-07-19 23:11:56.590465 I | ceph-rbd-mirror-controller: successfully started
2023-07-19 23:11:56.590477 I | ceph-client-controller: successfully started
2023-07-19 23:11:56.590490 I | ceph-filesystem-mirror-controller: successfully started
2023-07-19 23:11:56.590507 I | operator: rook-ceph-operator-config-controller successfully started
2023-07-19 23:11:56.625385 I | op-k8sutil: ROOK_DISABLE_ADMISSION_CONTROLLER="true" (configmap)
2023-07-19 23:11:56.625447 I | ceph-csi: rook-ceph-operator-csi-controller successfully started
2023-07-19 23:11:56.625462 I | op-bucket-prov: rook-ceph-operator-bucket-controller successfully started
2023-07-19 23:11:56.625471 I | ceph-bucket-topic: successfully started
2023-07-19 23:11:56.625488 I | ceph-bucket-notification: successfully started
2023-07-19 23:11:56.625502 I | ceph-bucket-notification: successfully started
2023-07-19 23:11:56.625508 I | ceph-fs-subvolumegroup-controller: successfully started
2023-07-19 23:11:56.625513 I | blockpool-rados-namespace-controller: successfully started
2023-07-19 23:11:56.625518 I | ceph-cosi-controller: successfully started
2023-07-19 23:11:56.627453 I | operator: starting the controller-runtime manager
2023-07-19 23:11:56.831842 I | op-k8sutil: ROOK_WATCH_FOR_NODE_FAILURE="true" (default)
2023-07-19 23:11:57.035973 I | op-k8sutil: ROOK_CEPH_COMMANDS_TIMEOUT_SECONDS="15" (configmap)
2023-07-19 23:11:57.035994 I | op-k8sutil: ROOK_LOG_LEVEL="INFO" (configmap)
2023-07-19 23:11:57.036003 I | op-k8sutil: ROOK_ENABLE_DISCOVERY_DAEMON="false" (env var)
2023-07-19 23:11:57.036448 I | ceph-cluster-controller: reconciling ceph cluster in namespace "storage"
2023-07-19 23:11:57.038716 I | op-k8sutil: CSI_ENABLE_HOST_NETWORK="true" (configmap)
2023-07-19 23:11:57.049393 I | op-k8sutil: ROOK_WATCH_FOR_NODE_FAILURE="true" (default)
2023-07-19 23:11:57.049655 I | op-k8sutil: ROOK_CEPH_ALLOW_LOOP_DEVICES="false" (configmap)
2023-07-19 23:11:57.049668 I | operator: rook-ceph-operator-config-controller done reconciling
2023-07-19 23:11:57.055089 I | ceph-cluster-controller: Found taint: Key=node.kubernetes.io/out-of-service, Value=nodeshutdown on node k8s-node-blue
2023-07-19 23:11:57.055118 I | ceph-cluster-controller: volumeInUse after split based on '^' [storage.rbd.csi.ceph.com 0001-0007-storage-000000000000000c-5e1e6d58-ba79-11ed-bb92-7e80d93390e2]
2023-07-19 23:11:57.055122 I | ceph-cluster-controller: volumeInUse after split based on '^' [storage.rbd.csi.ceph.com 0001-0007-storage-000000000000000c-609df663-ba79-11ed-bb92-7e80d93390e2]
2023-07-19 23:11:57.055125 I | ceph-cluster-controller: volumeInUse after split based on '^' [storage.rbd.csi.ceph.com 0001-0007-storage-000000000000000c-f1e2cab2-dde7-4a1e-a47c-31a764fb1a0b]
2023-07-19 23:11:57.055127 I | ceph-cluster-controller: node %q require fencing, found rbd volumes in usek8s-node-blue
2023-07-19 23:11:57.056801 I | clusterdisruption-controller: deleted all legacy node drain canary pods
2023-07-19 23:11:57.058110 I | clusterdisruption-controller: osd "rook-ceph-osd-3" is down and a possible node drain is detected
2023-07-19 23:11:57.058144 I | clusterdisruption-controller: osd "rook-ceph-osd-1" is down and a possible node drain is detected
2023-07-19 23:11:57.215301 I | ceph-spec: parsing mon endpoints: k=10.43.19.35:6789,l=10.43.201.87:6789,n=10.43.56.98:6789
2023-07-19 23:11:57.236192 I | clusterdisruption-controller: osd "rook-ceph-osd-1" is down and a possible node drain is detected
2023-07-19 23:11:57.236334 I | clusterdisruption-controller: osd "rook-ceph-osd-3" is down and a possible node drain is detected
2023-07-19 23:11:57.618950 I | ceph-spec: parsing mon endpoints: k=10.43.19.35:6789,l=10.43.201.87:6789,n=10.43.56.98:6789
2023-07-19 23:11:57.619000 I | ceph-spec: detecting the ceph image version for image quay.io/ceph/ceph:v17.2.6...
2023-07-19 23:11:57.633385 I | ceph-block-pool-controller: skipping reconcile since operator is still initializing
2023-07-19 23:11:57.813731 I | ceph-spec: parsing mon endpoints: k=10.43.19.35:6789,l=10.43.201.87:6789,n=10.43.56.98:6789
2023-07-19 23:11:57.813792 I | op-k8sutil: ROOK_OBC_WATCH_OPERATOR_NAMESPACE="true" (configmap)
2023-07-19 23:11:57.813798 I | op-bucket-prov: ceph bucket provisioner launched watching for provisioner "storage.ceph.rook.io/bucket"
2023-07-19 23:11:57.814116 I | op-bucket-prov: successfully reconciled bucket provisioner
I0719 23:11:57.814205       1 manager.go:135] objectbucket.io/provisioner-manager "msg"="starting provisioner" "name"="storage.ceph.rook.io/bucket"
I0719 23:11:58.209743       1 request.go:696] Waited for 1.155962556s due to client-side throttling, not priority and fairness, request: GET:https://10.43.0.1:443/api/v1/namespaces/storage/configmaps/rook-ceph-mon-endpoints
2023-07-19 23:11:58.212029 I | ceph-spec: parsing mon endpoints: k=10.43.19.35:6789,l=10.43.201.87:6789,n=10.43.56.98:6789
2023-07-19 23:11:58.226274 I | ceph-cluster-controller: enabling ceph mon monitoring goroutine for cluster "storage"
2023-07-19 23:11:58.226337 I | op-osd: ceph osd status in namespace "storage" check interval "1m0s"
2023-07-19 23:11:58.226341 I | ceph-cluster-controller: enabling ceph osd monitoring goroutine for cluster "storage"
2023-07-19 23:11:58.226349 I | ceph-cluster-controller: ceph status check interval is 1m0s
2023-07-19 23:11:58.226359 I | ceph-cluster-controller: enabling ceph status monitoring goroutine for cluster "storage"
2023-07-19 23:11:58.425981 I | ceph-cluster-controller: skipping ceph status since operator is still initializing
2023-07-19 23:11:58.426415 I | ceph-csi: successfully created csi config map "rook-ceph-csi-config"
E0719 23:11:58.622803       1 runtime.go:79] Observed a panic: "invalid memory address or nil pointer dereference" (runtime error: invalid memory address or nil pointer dereference)
goroutine 582 [running]:
k8s.io/apimachinery/pkg/util/runtime.logPanic({0x2007280?, 0x39a09f0})
	/home/runner/go/pkg/mod/k8s.io/apimachinery@v0.27.3/pkg/util/runtime/runtime.go:75 +0x99
k8s.io/apimachinery/pkg/util/runtime.HandleCrash({0x0, 0x0, 0xc0012c7d18?})
	/home/runner/go/pkg/mod/k8s.io/apimachinery@v0.27.3/pkg/util/runtime/runtime.go:49 +0x75
panic({0x2007280, 0x39a09f0})
	/opt/hostedtoolcache/go/1.20.5/x64/src/runtime/panic.go:884 +0x213
github.com/rook/rook/pkg/operator/ceph/cluster.listRBDPV(0x0?, 0xc001ce2000, {0xc001ba6340, 0x3, 0x0?})
	/home/runner/work/rook/rook/pkg/operator/ceph/cluster/watcher.go:269 +0x14e
github.com/rook/rook/pkg/operator/ceph/cluster.(*clientCluster).fenceNode(0xc0012c9c58, {0x2879d08?, 0xc0001b4000}, 0xc00001c400, 0xc001ce2000)
	/home/runner/work/rook/rook/pkg/operator/ceph/cluster/watcher.go:217 +0x396
github.com/rook/rook/pkg/operator/ceph/cluster.(*clientCluster).handleNodeFailure(0xc0012c9c58, {0x2879d08, 0xc0001b4000}, 0x0?, 0xc00001c400)
	/home/runner/work/rook/rook/pkg/operator/ceph/cluster/watcher.go:182 +0x4ae
github.com/rook/rook/pkg/operator/ceph/cluster.(*clientCluster).onK8sNode(0xc0012c9c58, {0x2879d08?, 0xc0001b4000}, {0x2867820?, 0xc00001c400})
	/home/runner/work/rook/rook/pkg/operator/ceph/cluster/watcher.go:84 +0xae
github.com/rook/rook/pkg/operator/ceph/cluster.predicateForNodeWatcher.func1({{0x28960d0?, 0xc00001c400?}})
	/home/runner/work/rook/rook/pkg/operator/ceph/cluster/predicate.go:42 +0xe5
sigs.k8s.io/controller-runtime/pkg/predicate.Funcs.Create(...)
	/home/runner/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.15.0/pkg/predicate/predicate.go:72
sigs.k8s.io/controller-runtime/pkg/internal/source.(*EventHandler).OnAdd(0xc000376a00, {0x22b79a0?, 0xc00001c400})
	/home/runner/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.15.0/pkg/internal/source/event_handler.go:80 +0x238
k8s.io/client-go/tools/cache.ResourceEventHandlerFuncs.OnAdd(...)
	/home/runner/go/pkg/mod/k8s.io/client-go@v0.27.3/tools/cache/controller.go:243
k8s.io/client-go/tools/cache.(*processorListener).run.func1()
	/home/runner/go/pkg/mod/k8s.io/client-go@v0.27.3/tools/cache/shared_informer.go:973 +0x148
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0x30?)
	/home/runner/go/pkg/mod/k8s.io/apimachinery@v0.27.3/pkg/util/wait/backoff.go:226 +0x3e
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc000e0a738?, {0x2860780, 0xc000d7c5a0}, 0x1, 0xc000d5ec60)
	/home/runner/go/pkg/mod/k8s.io/apimachinery@v0.27.3/pkg/util/wait/backoff.go:227 +0xb6
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0x0?, 0x3b9aca00, 0x0, 0x0?, 0x0?)
	/home/runner/go/pkg/mod/k8s.io/apimachinery@v0.27.3/pkg/util/wait/backoff.go:204 +0x89
k8s.io/apimachinery/pkg/util/wait.Until(...)
	/home/runner/go/pkg/mod/k8s.io/apimachinery@v0.27.3/pkg/util/wait/backoff.go:161
k8s.io/client-go/tools/cache.(*processorListener).run(0xc000e3d830)
	/home/runner/go/pkg/mod/k8s.io/client-go@v0.27.3/tools/cache/shared_informer.go:967 +0x6b
k8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1()
	/home/runner/go/pkg/mod/k8s.io/apimachinery@v0.27.3/pkg/util/wait/wait.go:72 +0x5a
created by k8s.io/apimachinery/pkg/util/wait.(*Group).Start
	/home/runner/go/pkg/mod/k8s.io/apimachinery@v0.27.3/pkg/util/wait/wait.go:70 +0x85
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x8 pc=0x1cfb82e]

goroutine 582 [running]:
k8s.io/apimachinery/pkg/util/runtime.HandleCrash({0x0, 0x0, 0xc0012c7d18?})
	/home/runner/go/pkg/mod/k8s.io/apimachinery@v0.27.3/pkg/util/runtime/runtime.go:56 +0xd7
panic({0x2007280, 0x39a09f0})
	/opt/hostedtoolcache/go/1.20.5/x64/src/runtime/panic.go:884 +0x213
github.com/rook/rook/pkg/operator/ceph/cluster.listRBDPV(0x0?, 0xc001ce2000, {0xc001ba6340, 0x3, 0x0?})
	/home/runner/work/rook/rook/pkg/operator/ceph/cluster/watcher.go:269 +0x14e
github.com/rook/rook/pkg/operator/ceph/cluster.(*clientCluster).fenceNode(0xc0012c9c58, {0x2879d08?, 0xc0001b4000}, 0xc00001c400, 0xc001ce2000)
	/home/runner/work/rook/rook/pkg/operator/ceph/cluster/watcher.go:217 +0x396
github.com/rook/rook/pkg/operator/ceph/cluster.(*clientCluster).handleNodeFailure(0xc0012c9c58, {0x2879d08, 0xc0001b4000}, 0x0?, 0xc00001c400)
	/home/runner/work/rook/rook/pkg/operator/ceph/cluster/watcher.go:182 +0x4ae
github.com/rook/rook/pkg/operator/ceph/cluster.(*clientCluster).onK8sNode(0xc0012c9c58, {0x2879d08?, 0xc0001b4000}, {0x2867820?, 0xc00001c400})
	/home/runner/work/rook/rook/pkg/operator/ceph/cluster/watcher.go:84 +0xae
github.com/rook/rook/pkg/operator/ceph/cluster.predicateForNodeWatcher.func1({{0x28960d0?, 0xc00001c400?}})
	/home/runner/work/rook/rook/pkg/operator/ceph/cluster/predicate.go:42 +0xe5
sigs.k8s.io/controller-runtime/pkg/predicate.Funcs.Create(...)
	/home/runner/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.15.0/pkg/predicate/predicate.go:72
sigs.k8s.io/controller-runtime/pkg/internal/source.(*EventHandler).OnAdd(0xc000376a00, {0x22b79a0?, 0xc00001c400})
	/home/runner/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.15.0/pkg/internal/source/event_handler.go:80 +0x238
k8s.io/client-go/tools/cache.ResourceEventHandlerFuncs.OnAdd(...)
	/home/runner/go/pkg/mod/k8s.io/client-go@v0.27.3/tools/cache/controller.go:243
k8s.io/client-go/tools/cache.(*processorListener).run.func1()
	/home/runner/go/pkg/mod/k8s.io/client-go@v0.27.3/tools/cache/shared_informer.go:973 +0x148
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0x30?)
	/home/runner/go/pkg/mod/k8s.io/apimachinery@v0.27.3/pkg/util/wait/backoff.go:226 +0x3e
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc000e0a738?, {0x2860780, 0xc000d7c5a0}, 0x1, 0xc000d5ec60)
	/home/runner/go/pkg/mod/k8s.io/apimachinery@v0.27.3/pkg/util/wait/backoff.go:227 +0xb6
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0x0?, 0x3b9aca00, 0x0, 0x0?, 0x0?)
	/home/runner/go/pkg/mod/k8s.io/apimachinery@v0.27.3/pkg/util/wait/backoff.go:204 +0x89
k8s.io/apimachinery/pkg/util/wait.Until(...)
	/home/runner/go/pkg/mod/k8s.io/apimachinery@v0.27.3/pkg/util/wait/backoff.go:161
k8s.io/client-go/tools/cache.(*processorListener).run(0xc000e3d830)
	/home/runner/go/pkg/mod/k8s.io/client-go@v0.27.3/tools/cache/shared_informer.go:967 +0x6b
k8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1()
	/home/runner/go/pkg/mod/k8s.io/apimachinery@v0.27.3/pkg/util/wait/wait.go:72 +0x5a
created by k8s.io/apimachinery/pkg/util/wait.(*Group).Start
	/home/runner/go/pkg/mod/k8s.io/apimachinery@v0.27.3/pkg/util/wait/wait.go:70 +0x85
