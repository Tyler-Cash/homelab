rook-ceph-cluster:
  operatorNamespace: storage
  configOverride: |
    [global]
    # Default of 0.05 is too aggressive for my cluster. (seconds)
    mon clock drift allowed = 0.1
  monitoring:
    enabled: true
    createPrometheusRules: true
    rulesNamespaceOverride: monitoring
  toolbox:
    enabled: true
  ingress:
    dashboard:
      annotations:
        cert-manager.io/cluster-issuer: "prod-issuer"
        hajimari.io/enable: "true"
        hajimari.io/group: "Storage"
        kubernetes.io/ingress-allow-http: "false" 
        kubernetes.io/ingress.class: "nginx"
        nginx.ingress.kubernetes.io/whitelist-source-range: "10.0.0.0/8"
      host:
        name: ceph.k8s.tylercash.dev
        path: ""
      tls:
      - hosts:
          - ceph.k8s.tylercash.dev
        secretName: ceph-home-tylercash-dev-tls
  cephClusterSpec:
    continueUpgradeAfterChecksEvenIfNotHealthy: false
    skipUpgradeChecks: false
    removeOSDsIfOutAndSafeToRemove: true
    storage:
      useAllNodes: true
      useAllDevices: true
    dashboard:
      enabled: true
      ssl: false
    annotations:
      all:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9102"
    placement:
      all:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                - key: role
                  operator: In
                  values:
                  - storage-node
        podAffinity:
        podAntiAffinity:
        topologySpreadConstraints:
        tolerations:
        - key: storage-node
          operator: Exists
    resources:
      mgr:
        limits:
          cpu: "1500m"
          memory: "1Gi"
        requests:
          cpu: "800m"
          memory: "512Mi"
      mon:
        limits:
          cpu: "1500m"
          memory: "1Gi"
        requests:
          cpu: "600m"
          memory: "500Mi"
      osd:
        limits:
          cpu: "700m"
          memory: "3Gi"
        requests:
          cpu: "600m"
          memory: "2Gi"
      prepareosd:
        limits:
          cpu: "500m"
          memory: "400Mi"
        requests:
          cpu: "500m"
          memory: "50Mi"
      mgr-sidecar:
        limits:
          cpu: "500m"
          memory: "100Mi"
        requests:
          cpu: "100m"
          memory: "40Mi"
      crashcollector:
        limits:
          cpu: "200m"
          memory: "60Mi"
        requests:
          cpu: "50m"
          memory: "60Mi"
      logcollector:
        limits:
          cpu: "500m"
          memory: "1Gi"
        requests:
          cpu: "100m"
          memory: "100Mi"
      cleanup:
        limits:
          cpu: "500m"
          memory: "1Gi"
        requests:
          cpu: "500m"
          memory: "100Mi"
  cephBlockPools:
  - name: ceph-blockpool
    spec:
      deviceClass: ssd
      failureDomain: host
      replicated:
        size: 3
    storageClass:
      enabled: true
      name: ceph-block
      isDefault: true
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      mountOptions: []
      parameters:
        imageFormat: "2"
        imageFeatures: layering
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: storage
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: storage
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
        csi.storage.k8s.io/node-stage-secret-namespace: storage
        csi.storage.k8s.io/fstype: ext4
  - name: ceph-blockpool-1rep
    spec:
      deviceClass: ssd
      failureDomain: host
      replicated:
        size: 2
    storageClass:
      enabled: true
      name: ceph-block-1rep
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      mountOptions: []
      parameters:
        imageFormat: "2"
        imageFeatures: layering
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: storage
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: storage
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
        csi.storage.k8s.io/node-stage-secret-namespace: storage
        csi.storage.k8s.io/fstype: ext4
  cephFileSystems:
  - name: ceph-filesystem
    spec:
      metadataPool:
        deviceClass: ssd
        replicated:
          size: 3
      dataPools:
        - name: default
          failureDomain: host
          replicated:
            size: 3
        - name: data0
          deviceClass: ssd
          failureDomain: host
          erasureCoded:
            dataChunks: 2
            codingChunks: 1
      metadataServer:
        activeCount: 1
        activeStandby: true
        resources:
          limits:
            cpu: "2000m"
            memory: "3Gi"
          requests:
            cpu: "800m"
            memory: "2Gi"
        priorityClassName: system-cluster-critical
    storageClass:
      enabled: true
      isDefault: false
      name: ceph-filesystem
      pool: data0
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      mountOptions: []
      parameters:
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: storage
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: storage
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
        csi.storage.k8s.io/node-stage-secret-namespace: storage
        csi.storage.k8s.io/fstype: ext4
  - name: ceph-filesystem-hdd
    spec:
      metadataPool:
        deviceClass: ssd
        replicated:
          size: 3
      dataPools:
        - name: default
          failureDomain: host
          replicated:
            size: 3
        - name: data0
          deviceClass: hdd
          failureDomain: host
          erasureCoded:
            dataChunks: 2
            codingChunks: 1
      metadataServer:
        activeCount: 1
        resources:
          limits:
            cpu: "1000m"
            memory: "3Gi"
          requests:
            cpu: "500m"
            memory: "2Gi"
        priorityClassName: system-cluster-critical
    storageClass:
      enabled: true
      isDefault: false
      name: ceph-filesystem-hdd
      pool: data0
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      mountOptions: []
      parameters:
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: storage
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: storage
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
        csi.storage.k8s.io/node-stage-secret-namespace: storage
        csi.storage.k8s.io/fstype: ext4
  
  cephFileSystemVolumeSnapshotClass:
    enabled: true
    name: ceph-filesystem
    isDefault: true
    deletionPolicy: Delete
    annotations: {}
    labels: {}
    parameters:

cephBlockPoolsVolumeSnapshotClass:
    enabled: true
    name: ceph-block
    isDefault: true
    deletionPolicy: Delete
    annotations: {}
    labels: {}
    parameters:
      csi.storage.k8s.io/snapshotter-secret-namespace: storage
