apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 5b589868aaecae81e03891df57c6d49bc15ba969ca47b30954a96d076c7fd5f1
      cni.projectcalico.org/containerID: 46ed3941d2c2de9c60e0f9361e0ead9b464f8332e442e400213fdf9a135d2569
      cni.projectcalico.org/podIP: 10.42.186.178/32
      cni.projectcalico.org/podIPs: 10.42.186.178/32
    creationTimestamp: "2024-08-22T18:13:49Z"
    generateName: cloudnative-pg-796ff596d-
    labels:
      app.kubernetes.io/instance: cloudnative-pg
      app.kubernetes.io/name: cloudnative-pg
      pod-template-hash: 796ff596d
    name: cloudnative-pg-796ff596d-kdfvq
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cloudnative-pg-796ff596d
      uid: e1b4b4ec-c04e-4ec0-9184-3ffb958d2718
    resourceVersion: "1330834044"
    uid: 466a4799-f38b-4d6c-b27a-3da3b88005e7
  spec:
    containers:
    - args:
      - controller
      - --leader-elect
      - --config-map-name=cnpg-controller-manager-config
      - --webhook-port=9443
      command:
      - /manager
      env:
      - name: TZ
        value: Australia/Sydney
      - name: OPERATOR_IMAGE_NAME
        value: ghcr.io/cloudnative-pg/cloudnative-pg:1.24.0
      - name: OPERATOR_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: MONITORING_QUERIES_CONFIGMAP
        value: cnpg-default-monitoring
      image: ghcr.io/cloudnative-pg/cloudnative-pg:1.24.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: 9443
          scheme: HTTPS
        initialDelaySeconds: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: manager
      ports:
      - containerPort: 8080
        name: metrics
        protocol: TCP
      - containerPort: 9443
        name: webhook-server
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: 9443
          scheme: HTTPS
        initialDelaySeconds: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsGroup: 10001
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /controller
        name: scratch-data
      - mountPath: /run/secrets/cnpg.io/webhook
        name: webhook-certificates
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qzpw7
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qzpw7
        readOnly: true
    nodeName: k8s-node-orange
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: cloudnative-pg
    serviceAccountName: cloudnative-pg
    terminationGracePeriodSeconds: 10
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - emptyDir: {}
      name: scratch-data
    - name: webhook-certificates
      secret:
        defaultMode: 420
        optional: true
        secretName: cnpg-webhook-cert
    - name: kube-api-access-qzpw7
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:59:34Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-08-22T18:13:50Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:59:43Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:59:43Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-08-22T18:13:49Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://ec98fff585f06768a0d7749eb648bcc6c567271eeaffebcd39b725a7dda29cd4
      image: ghcr.io/cloudnative-pg/cloudnative-pg:1.24.0
      imageID: ghcr.io/cloudnative-pg/cloudnative-pg@sha256:01af1b3ecd920e15fa8ff8963a249d14bb5eb7da702a9042278f49c3d099e5d3
      lastState:
        terminated:
          containerID: containerd://80b242d7a86b3942a333200f2b630164ed522c9d33a0878be3fb3ab0acedc677
          exitCode: 255
          finishedAt: "2024-09-23T15:59:18Z"
          reason: Unknown
          startedAt: "2024-09-23T15:47:50Z"
      name: manager
      ready: true
      restartCount: 5
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:59:36Z"
    hostIP: 10.0.90.12
    hostIPs:
    - ip: 10.0.90.12
    initContainerStatuses:
    - containerID: containerd://56231518e42f32467725a386654ef56e0971ed778363c31e1ef1d36a1f71ba60
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 2
      started: false
      state:
        terminated:
          containerID: containerd://56231518e42f32467725a386654ef56e0971ed778363c31e1ef1d36a1f71ba60
          exitCode: 0
          finishedAt: "2024-09-23T15:59:33Z"
          reason: Completed
          startedAt: "2024-09-23T15:59:33Z"
    phase: Running
    podIP: 10.42.186.178
    podIPs:
    - ip: 10.42.186.178
    qosClass: Burstable
    startTime: "2024-08-22T18:13:49Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 5b589868aaecae81e03891df57c6d49bc15ba969ca47b30954a96d076c7fd5f1
      cni.projectcalico.org/containerID: 352179b70cb06c1476834da0545e5c08e4ebeb3bfcac75a9ba9413927c15f480
      cni.projectcalico.org/podIP: 10.42.3.11/32
      cni.projectcalico.org/podIPs: 10.42.3.11/32
    creationTimestamp: "2024-08-22T18:13:38Z"
    generateName: cloudnative-pg-796ff596d-
    labels:
      app.kubernetes.io/instance: cloudnative-pg
      app.kubernetes.io/name: cloudnative-pg
      pod-template-hash: 796ff596d
    name: cloudnative-pg-796ff596d-mrl5v
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cloudnative-pg-796ff596d
      uid: e1b4b4ec-c04e-4ec0-9184-3ffb958d2718
    resourceVersion: "1349044285"
    uid: 8fb9cce8-9803-46d9-a550-121466bae0b8
  spec:
    containers:
    - args:
      - controller
      - --leader-elect
      - --config-map-name=cnpg-controller-manager-config
      - --webhook-port=9443
      command:
      - /manager
      env:
      - name: TZ
        value: Australia/Sydney
      - name: OPERATOR_IMAGE_NAME
        value: ghcr.io/cloudnative-pg/cloudnative-pg:1.24.0
      - name: OPERATOR_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: MONITORING_QUERIES_CONFIGMAP
        value: cnpg-default-monitoring
      image: ghcr.io/cloudnative-pg/cloudnative-pg:1.24.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: 9443
          scheme: HTTPS
        initialDelaySeconds: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: manager
      ports:
      - containerPort: 8080
        name: metrics
        protocol: TCP
      - containerPort: 9443
        name: webhook-server
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: 9443
          scheme: HTTPS
        initialDelaySeconds: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsGroup: 10001
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /controller
        name: scratch-data
      - mountPath: /run/secrets/cnpg.io/webhook
        name: webhook-certificates
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zd7v4
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zd7v4
        readOnly: true
    nodeName: k8s-node-blue
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: cloudnative-pg
    serviceAccountName: cloudnative-pg
    terminationGracePeriodSeconds: 10
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - emptyDir: {}
      name: scratch-data
    - name: webhook-certificates
      secret:
        defaultMode: 420
        optional: true
        secretName: cnpg-webhook-cert
    - name: kube-api-access-zd7v4
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-08-22T18:13:39Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-08-22T18:13:39Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-10-10T22:15:03Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-10-10T22:15:03Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-08-22T18:13:38Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://21179312a67b2dee3745db5d303f9ef787366cdbb4b0f4a45f5b1cbe6c58f550
      image: ghcr.io/cloudnative-pg/cloudnative-pg:1.24.0
      imageID: ghcr.io/cloudnative-pg/cloudnative-pg@sha256:01af1b3ecd920e15fa8ff8963a249d14bb5eb7da702a9042278f49c3d099e5d3
      lastState:
        terminated:
          containerID: containerd://a564d112f0e8e6a9aa5e1d70b02288e32771a0009d11c23d8c55248a5905dbf4
          exitCode: 255
          finishedAt: "2024-09-23T14:54:45Z"
          reason: Unknown
          startedAt: "2024-09-23T14:37:56Z"
      name: manager
      ready: true
      restartCount: 4
      started: true
      state:
        running:
          startedAt: "2024-09-23T14:54:56Z"
    hostIP: 10.0.90.10
    hostIPs:
    - ip: 10.0.90.10
    initContainerStatuses:
    - containerID: containerd://ef84262ad9ccde6f77295dd1289d621e4790028d98dd0d45f82935d879b4bf1b
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://ef84262ad9ccde6f77295dd1289d621e4790028d98dd0d45f82935d879b4bf1b
          exitCode: 0
          finishedAt: "2024-08-22T18:13:39Z"
          reason: Completed
          startedAt: "2024-08-22T18:13:39Z"
    phase: Running
    podIP: 10.42.3.11
    podIPs:
    - ip: 10.42.3.11
    qosClass: Burstable
    startTime: "2024-08-22T18:13:38Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 5b589868aaecae81e03891df57c6d49bc15ba969ca47b30954a96d076c7fd5f1
      cni.projectcalico.org/containerID: 7bf189b8469b1fc1e9950c67f6cbc6ed8b959f12c7e7e5e73bd30d3f741f9dc0
      cni.projectcalico.org/podIP: 10.42.3.29/32
      cni.projectcalico.org/podIPs: 10.42.3.29/32
    creationTimestamp: "2024-08-22T18:13:59Z"
    generateName: cloudnative-pg-796ff596d-
    labels:
      app.kubernetes.io/instance: cloudnative-pg
      app.kubernetes.io/name: cloudnative-pg
      pod-template-hash: 796ff596d
    name: cloudnative-pg-796ff596d-sgzgx
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cloudnative-pg-796ff596d
      uid: e1b4b4ec-c04e-4ec0-9184-3ffb958d2718
    resourceVersion: "1349043817"
    uid: 06f82a22-34ee-43d3-9211-b1d7ac61c874
  spec:
    containers:
    - args:
      - controller
      - --leader-elect
      - --config-map-name=cnpg-controller-manager-config
      - --webhook-port=9443
      command:
      - /manager
      env:
      - name: TZ
        value: Australia/Sydney
      - name: OPERATOR_IMAGE_NAME
        value: ghcr.io/cloudnative-pg/cloudnative-pg:1.24.0
      - name: OPERATOR_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: MONITORING_QUERIES_CONFIGMAP
        value: cnpg-default-monitoring
      image: ghcr.io/cloudnative-pg/cloudnative-pg:1.24.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: 9443
          scheme: HTTPS
        initialDelaySeconds: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: manager
      ports:
      - containerPort: 8080
        name: metrics
        protocol: TCP
      - containerPort: 9443
        name: webhook-server
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: 9443
          scheme: HTTPS
        initialDelaySeconds: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsGroup: 10001
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /controller
        name: scratch-data
      - mountPath: /run/secrets/cnpg.io/webhook
        name: webhook-certificates
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qmnp5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qmnp5
        readOnly: true
    nodeName: k8s-node-blue
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: cloudnative-pg
    serviceAccountName: cloudnative-pg
    terminationGracePeriodSeconds: 10
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - emptyDir: {}
      name: scratch-data
    - name: webhook-certificates
      secret:
        defaultMode: 420
        optional: true
        secretName: cnpg-webhook-cert
    - name: kube-api-access-qmnp5
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-08-22T18:14:00Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-08-22T18:14:00Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-10-10T22:14:57Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-10-10T22:14:57Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-08-22T18:13:59Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://bab7f72e31519a204fbbbf1e97db9d0ef3852fbc13e440f4bd88710ca8c63338
      image: ghcr.io/cloudnative-pg/cloudnative-pg:1.24.0
      imageID: ghcr.io/cloudnative-pg/cloudnative-pg@sha256:01af1b3ecd920e15fa8ff8963a249d14bb5eb7da702a9042278f49c3d099e5d3
      lastState:
        terminated:
          containerID: containerd://2fb3feceaac8e4df8ae8b644cfe08917c1376c4d7e473705ff422b7049dffe1d
          exitCode: 255
          finishedAt: "2024-10-10T22:14:49Z"
          reason: Unknown
          startedAt: "2024-08-24T13:03:52Z"
      name: manager
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2024-10-10T22:14:54Z"
    hostIP: 10.0.90.10
    hostIPs:
    - ip: 10.0.90.10
    initContainerStatuses:
    - containerID: containerd://5d4afeca415bfa96a636911fe781e546bb96dcbfbfe19478ab3158377200dcb1
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://5d4afeca415bfa96a636911fe781e546bb96dcbfbfe19478ab3158377200dcb1
          exitCode: 0
          finishedAt: "2024-08-22T18:14:00Z"
          reason: Completed
          startedAt: "2024-08-22T18:14:00Z"
    phase: Running
    podIP: 10.42.3.29
    podIPs:
    - ip: 10.42.3.29
    qosClass: Burstable
    startTime: "2024-08-22T18:13:59Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-05-21T23:06:51Z"
    generateName: csi-cephfsplugin-
    labels:
      app: csi-cephfsplugin
      contains: csi-cephfsplugin-metrics
      controller-revision-hash: d4cd8668d
      pod-template-generation: "35"
    name: csi-cephfsplugin-2jw59
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-cephfsplugin
      uid: b35ced57-1c62-4841-8edb-aa20a613f611
    resourceVersion: "1283140574"
    uid: 78f22405-420a-479b-8709-2a576afca829
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - k8s-node-blue
    containers:
    - args:
      - --v=0
      - --csi-address=/csi/csi.sock
      - --kubelet-registration-path=/var/lib/kubelet/plugins/storage.cephfs.csi.ceph.com/csi.sock
      env:
      - name: TZ
        value: Australia/Sydney
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.1
      imagePullPolicy: IfNotPresent
      name: driver-registrar
      resources:
        limits:
          cpu: 100m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 128Mi
      securityContext:
        capabilities:
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nxm5d
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --type=cephfs
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --nodeserver=true
      - --drivername=storage.cephfs.csi.ceph.com
      - --pidlimit=-1
      - --forcecephkernelclient=true
      env:
      - name: TZ
        value: Australia/Sydney
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imagePullPolicy: IfNotPresent
      name: csi-cephfsplugin
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 250m
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /csi/mountinfo
        name: ceph-csi-mountinfo
      - mountPath: /var/lib/kubelet/plugins
        mountPropagation: Bidirectional
        name: csi-plugins-dir
      - mountPath: /var/lib/kubelet/pods
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /dev
        name: host-dev
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-config
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nxm5d
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nxm5d
        readOnly: true
    nodeName: k8s-node-blue
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-cephfs-plugin-sa
    serviceAccountName: rook-csi-cephfs-plugin-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - hostPath:
        path: /var/lib/kubelet/plugins/storage.cephfs.csi.ceph.com/mountinfo
        type: DirectoryOrCreate
      name: ceph-csi-mountinfo
    - hostPath:
        path: /var/lib/kubelet/plugins/storage.cephfs.csi.ceph.com/
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet/plugins
        type: Directory
      name: csi-plugins-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: Directory
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/pods
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - configMap:
        defaultMode: 420
        items:
        - key: csi-cluster-config-json
          path: config.json
        name: rook-ceph-csi-config
      name: ceph-csi-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - hostPath:
        path: /run/mount
        type: ""
      name: host-run-mount
    - name: kube-api-access-nxm5d
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-08-19T23:16:16Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-05-21T23:06:53Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-08-19T23:16:59Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-08-19T23:16:59Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-05-21T23:06:51Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://ba9c60d8438cb5d0f6c85253ea569c50aaf747ef93b2716abc81b8d70a9b47bf
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imageID: quay.io/cephcsi/cephcsi@sha256:a338c0dec57b8033e01f277dc342607d7610411422cc3ce47f176ab059aeaf21
      lastState:
        terminated:
          containerID: containerd://c0663a221f439bcad9743db9b7079f420a2299333da6e0b2556e257f978123ff
          exitCode: 255
          finishedAt: "2024-08-19T23:16:47Z"
          reason: Error
          startedAt: "2024-08-19T23:16:17Z"
      name: csi-cephfsplugin
      ready: true
      restartCount: 5
      started: true
      state:
        running:
          startedAt: "2024-08-19T23:16:58Z"
    - containerID: containerd://f3bad23f4e6a6ba84357eb903c70f543abd06cf64ea61c2afb4cad1aeab2d11a
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.1
      imageID: registry.k8s.io/sig-storage/csi-node-driver-registrar@sha256:f25af73ee708ff9c82595ae99493cdef9295bd96953366cddf36305f82555dac
      lastState:
        terminated:
          containerID: containerd://b6b579951ee2e934d5d8c48542a175b96ddd74f6662c6ec198d12259106658b9
          exitCode: 1
          finishedAt: "2024-08-19T23:16:47Z"
          reason: Error
          startedAt: "2024-08-19T23:16:17Z"
      name: driver-registrar
      ready: true
      restartCount: 6
      started: true
      state:
        running:
          startedAt: "2024-08-19T23:16:58Z"
    hostIP: 10.0.90.10
    hostIPs:
    - ip: 10.0.90.10
    initContainerStatuses:
    - containerID: containerd://ac0fd60e9b76caefe1252932d78df959f45245c29e79ed8d089605001d35d8a7
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 3
      started: false
      state:
        terminated:
          containerID: containerd://ac0fd60e9b76caefe1252932d78df959f45245c29e79ed8d089605001d35d8a7
          exitCode: 0
          finishedAt: "2024-08-19T23:16:16Z"
          reason: Completed
          startedAt: "2024-08-19T23:16:16Z"
    phase: Running
    podIP: 10.0.90.10
    podIPs:
    - ip: 10.0.90.10
    qosClass: Burstable
    startTime: "2024-05-21T23:06:51Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-09-23T15:07:33Z"
    generateName: csi-cephfsplugin-
    labels:
      app: csi-cephfsplugin
      contains: csi-cephfsplugin-metrics
      controller-revision-hash: d4cd8668d
      pod-template-generation: "35"
    name: csi-cephfsplugin-l9zzw
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-cephfsplugin
      uid: b35ced57-1c62-4841-8edb-aa20a613f611
    resourceVersion: "1330786330"
    uid: cb277649-d712-4aa0-b20a-ce199b2df33d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - k8s-node-yellow
    containers:
    - args:
      - --v=0
      - --csi-address=/csi/csi.sock
      - --kubelet-registration-path=/var/lib/kubelet/plugins/storage.cephfs.csi.ceph.com/csi.sock
      env:
      - name: TZ
        value: Australia/Sydney
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.1
      imagePullPolicy: IfNotPresent
      name: driver-registrar
      resources:
        limits:
          cpu: 100m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 128Mi
      securityContext:
        capabilities:
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gqjxj
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --type=cephfs
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --nodeserver=true
      - --drivername=storage.cephfs.csi.ceph.com
      - --pidlimit=-1
      - --forcecephkernelclient=true
      env:
      - name: TZ
        value: Australia/Sydney
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imagePullPolicy: IfNotPresent
      name: csi-cephfsplugin
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 250m
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /csi/mountinfo
        name: ceph-csi-mountinfo
      - mountPath: /var/lib/kubelet/plugins
        mountPropagation: Bidirectional
        name: csi-plugins-dir
      - mountPath: /var/lib/kubelet/pods
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /dev
        name: host-dev
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-config
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gqjxj
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gqjxj
        readOnly: true
    nodeName: k8s-node-yellow
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-cephfs-plugin-sa
    serviceAccountName: rook-csi-cephfs-plugin-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - hostPath:
        path: /var/lib/kubelet/plugins/storage.cephfs.csi.ceph.com/mountinfo
        type: DirectoryOrCreate
      name: ceph-csi-mountinfo
    - hostPath:
        path: /var/lib/kubelet/plugins/storage.cephfs.csi.ceph.com/
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet/plugins
        type: Directory
      name: csi-plugins-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: Directory
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/pods
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - configMap:
        defaultMode: 420
        items:
        - key: csi-cluster-config-json
          path: config.json
        name: rook-ceph-csi-config
      name: ceph-csi-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - hostPath:
        path: /run/mount
        type: ""
      name: host-run-mount
    - name: kube-api-access-gqjxj
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:07:33Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:07:33Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:08:42Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:08:42Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:07:33Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://3a2f0a87515f57e9404f746261b6858921d2d346c50354311165490bbc556e3a
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imageID: quay.io/cephcsi/cephcsi@sha256:a338c0dec57b8033e01f277dc342607d7610411422cc3ce47f176ab059aeaf21
      lastState: {}
      name: csi-cephfsplugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:08:41Z"
    - containerID: containerd://0092360d33d031309f0e684b1f5b4f7ee814a37e4ceb0d8c2c6b21415b8769d1
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.1
      imageID: registry.k8s.io/sig-storage/csi-node-driver-registrar@sha256:f25af73ee708ff9c82595ae99493cdef9295bd96953366cddf36305f82555dac
      lastState: {}
      name: driver-registrar
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:08:33Z"
    hostIP: 10.0.90.14
    hostIPs:
    - ip: 10.0.90.14
    initContainerStatuses:
    - containerID: containerd://3e5c8dbee757b296314142151cb5e7ef7702d33018b0fabae9d41e18d5024332
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://3e5c8dbee757b296314142151cb5e7ef7702d33018b0fabae9d41e18d5024332
          exitCode: 0
          finishedAt: "2024-09-23T15:07:33Z"
          reason: Completed
          startedAt: "2024-09-23T15:07:33Z"
    phase: Running
    podIP: 10.0.90.14
    podIPs:
    - ip: 10.0.90.14
    qosClass: Burstable
    startTime: "2024-09-23T15:07:33Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 7f00858cf15cb0fd02af283af382575239084dbcca6be2361fe54a401399564d
      cni.projectcalico.org/podIP: 10.42.3.59/32
      cni.projectcalico.org/podIPs: 10.42.3.59/32
    creationTimestamp: "2024-08-22T01:16:45Z"
    generateName: csi-cephfsplugin-provisioner-674f666f95-
    labels:
      app: csi-cephfsplugin-provisioner
      contains: csi-cephfsplugin-metrics
      pod-template-hash: 674f666f95
    name: csi-cephfsplugin-provisioner-674f666f95-gccg7
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: csi-cephfsplugin-provisioner-674f666f95
      uid: 0a02007a-75da-4a7e-835d-af4fa7e9f0a4
    resourceVersion: "1286971533"
    uid: 08ea7673-5d4b-45e1-9176-aac94a2d29c2
  spec:
    affinity:
      nodeAffinity: {}
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - csi-cephfsplugin-provisioner
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --v=0
      - --csi-address=$(ADDRESS)
      - --leader-election=true
      - --timeout=2m30s
      - --leader-election-namespace=storage
      - --leader-election-lease-duration=2m17s
      - --leader-election-renew-deadline=1m47s
      - --leader-election-retry-period=26s
      env:
      - name: TZ
        value: Australia/Sydney
      - name: ADDRESS
        value: /csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-attacher:v4.5.1
      imagePullPolicy: IfNotPresent
      name: csi-attacher
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-n9g7x
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=2m30s
      - --leader-election=true
      - --leader-election-namespace=storage
      - --leader-election-lease-duration=2m17s
      - --leader-election-renew-deadline=1m47s
      - --leader-election-retry-period=26s
      - --extra-create-metadata=true
      env:
      - name: TZ
        value: Australia/Sydney
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-snapshotter:v7.0.2
      imagePullPolicy: IfNotPresent
      name: csi-snapshotter
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-n9g7x
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=2m30s
      - --leader-election=true
      - --leader-election-namespace=storage
      - --leader-election-lease-duration=2m17s
      - --leader-election-renew-deadline=1m47s
      - --leader-election-retry-period=26s
      - --handle-volume-inuse-error=false
      - --feature-gates=RecoverVolumeExpansionFailure=true
      env:
      - name: TZ
        value: Australia/Sydney
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-resizer:v1.10.1
      imagePullPolicy: IfNotPresent
      name: csi-resizer
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-n9g7x
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=2m30s
      - --retry-interval-start=500ms
      - --leader-election=true
      - --leader-election-namespace=storage
      - --feature-gates=HonorPVReclaimPolicy=true
      - --prevent-volume-mode-conversion=true
      - --leader-election-lease-duration=2m17s
      - --leader-election-renew-deadline=1m47s
      - --leader-election-retry-period=26s
      - --extra-create-metadata=true
      env:
      - name: TZ
        value: Australia/Sydney
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-provisioner:v4.0.1
      imagePullPolicy: IfNotPresent
      name: csi-provisioner
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-n9g7x
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --type=cephfs
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --controllerserver=true
      - --drivername=storage.cephfs.csi.ceph.com
      - --pidlimit=-1
      - --forcecephkernelclient=true
      env:
      - name: TZ
        value: Australia/Sydney
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi-provisioner.sock
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imagePullPolicy: IfNotPresent
      name: csi-cephfsplugin
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 250m
          memory: 512Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /csi
        name: socket-dir
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /dev
        name: host-dev
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-config
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-n9g7x
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-n9g7x
        readOnly: true
    nodeName: k8s-node-blue
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-cephfs-provisioner-sa
    serviceAccountName: rook-csi-cephfs-provisioner-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - emptyDir:
        medium: Memory
      name: socket-dir
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - configMap:
        defaultMode: 420
        items:
        - key: csi-cluster-config-json
          path: config.json
        name: rook-ceph-csi-config
      name: ceph-csi-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - name: kube-api-access-n9g7x
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-08-22T01:16:48Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-08-22T01:16:48Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-08-22T01:16:49Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-08-22T01:16:49Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-08-22T01:16:45Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://4713673467b5515909d12b857d051a84ec4073ae2e76078694a324720fb544a6
      image: registry.k8s.io/sig-storage/csi-attacher:v4.5.1
      imageID: registry.k8s.io/sig-storage/csi-attacher@sha256:9dcd469f02bbb7592ad61b0f848ec242f9ea2102187a0cd8407df33c2d633e9c
      lastState: {}
      name: csi-attacher
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-08-22T01:16:48Z"
    - containerID: containerd://acfb371cba04aad4abfa9e9d61532b7947011fe95c335dc81ad4b68fbee7d6d7
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imageID: quay.io/cephcsi/cephcsi@sha256:a338c0dec57b8033e01f277dc342607d7610411422cc3ce47f176ab059aeaf21
      lastState: {}
      name: csi-cephfsplugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-08-22T01:16:49Z"
    - containerID: containerd://b94bf4b725f5c4308c3d6353e7a21679e08a5dcadc3ae7c30214615f0929855e
      image: registry.k8s.io/sig-storage/csi-provisioner:v4.0.1
      imageID: registry.k8s.io/sig-storage/csi-provisioner@sha256:bf5a235b67d8aea00f5b8ec24d384a2480e1017d5458d8a63b361e9eeb1608a9
      lastState: {}
      name: csi-provisioner
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-08-22T01:16:49Z"
    - containerID: containerd://0bd6e89669c947a1d64f53efebc882fa3eb3f248230498fc2b80c3e77a86c596
      image: registry.k8s.io/sig-storage/csi-resizer:v1.10.1
      imageID: registry.k8s.io/sig-storage/csi-resizer@sha256:4ecda2818f6d88a8f217babd459fdac31588f85581aa95ac7092bb0471ff8541
      lastState: {}
      name: csi-resizer
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-08-22T01:16:49Z"
    - containerID: containerd://9f6b616315751e504f6d30b6ec1308bc410d72d950a5657497669c01a83d97e3
      image: registry.k8s.io/sig-storage/csi-snapshotter:v7.0.2
      imageID: registry.k8s.io/sig-storage/csi-snapshotter@sha256:c4b6b02737bc24906fcce57fe6626d1a36cb2b91baa971af2a5e5a919093c34e
      lastState: {}
      name: csi-snapshotter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-08-22T01:16:48Z"
    hostIP: 10.0.90.10
    hostIPs:
    - ip: 10.0.90.10
    initContainerStatuses:
    - containerID: containerd://a8c8dd490669448e6a043e6bc1989e4c3e69adf2d02d7e220c7de20aa7d25611
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://a8c8dd490669448e6a043e6bc1989e4c3e69adf2d02d7e220c7de20aa7d25611
          exitCode: 0
          finishedAt: "2024-08-22T01:16:47Z"
          reason: Completed
          startedAt: "2024-08-22T01:16:47Z"
    phase: Running
    podIP: 10.42.3.59
    podIPs:
    - ip: 10.42.3.59
    qosClass: Burstable
    startTime: "2024-08-22T01:16:45Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 18eee2c958e8ae2f7c81f82bd2435fcb2c9dfff0333a656c7c9c60c81896a2f3
      cni.projectcalico.org/podIP: 10.42.186.151/32
      cni.projectcalico.org/podIPs: 10.42.186.151/32
    creationTimestamp: "2024-05-21T23:06:52Z"
    generateName: csi-cephfsplugin-provisioner-674f666f95-
    labels:
      app: csi-cephfsplugin-provisioner
      contains: csi-cephfsplugin-metrics
      pod-template-hash: 674f666f95
    name: csi-cephfsplugin-provisioner-674f666f95-zpfn4
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: csi-cephfsplugin-provisioner-674f666f95
      uid: 0a02007a-75da-4a7e-835d-af4fa7e9f0a4
    resourceVersion: "1330833879"
    uid: 1b97a258-5aa4-4ec5-9df8-2dfef6716db3
  spec:
    affinity:
      nodeAffinity: {}
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - csi-cephfsplugin-provisioner
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --v=0
      - --csi-address=$(ADDRESS)
      - --leader-election=true
      - --timeout=2m30s
      - --leader-election-namespace=storage
      - --leader-election-lease-duration=2m17s
      - --leader-election-renew-deadline=1m47s
      - --leader-election-retry-period=26s
      env:
      - name: TZ
        value: Australia/Sydney
      - name: ADDRESS
        value: /csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-attacher:v4.5.1
      imagePullPolicy: IfNotPresent
      name: csi-attacher
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-g5wv7
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=2m30s
      - --leader-election=true
      - --leader-election-namespace=storage
      - --leader-election-lease-duration=2m17s
      - --leader-election-renew-deadline=1m47s
      - --leader-election-retry-period=26s
      - --extra-create-metadata=true
      env:
      - name: TZ
        value: Australia/Sydney
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-snapshotter:v7.0.2
      imagePullPolicy: IfNotPresent
      name: csi-snapshotter
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-g5wv7
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=2m30s
      - --leader-election=true
      - --leader-election-namespace=storage
      - --leader-election-lease-duration=2m17s
      - --leader-election-renew-deadline=1m47s
      - --leader-election-retry-period=26s
      - --handle-volume-inuse-error=false
      - --feature-gates=RecoverVolumeExpansionFailure=true
      env:
      - name: TZ
        value: Australia/Sydney
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-resizer:v1.10.1
      imagePullPolicy: IfNotPresent
      name: csi-resizer
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-g5wv7
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=2m30s
      - --retry-interval-start=500ms
      - --leader-election=true
      - --leader-election-namespace=storage
      - --feature-gates=HonorPVReclaimPolicy=true
      - --prevent-volume-mode-conversion=true
      - --leader-election-lease-duration=2m17s
      - --leader-election-renew-deadline=1m47s
      - --leader-election-retry-period=26s
      - --extra-create-metadata=true
      env:
      - name: TZ
        value: Australia/Sydney
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-provisioner:v4.0.1
      imagePullPolicy: IfNotPresent
      name: csi-provisioner
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-g5wv7
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --type=cephfs
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --controllerserver=true
      - --drivername=storage.cephfs.csi.ceph.com
      - --pidlimit=-1
      - --forcecephkernelclient=true
      env:
      - name: TZ
        value: Australia/Sydney
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi-provisioner.sock
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imagePullPolicy: IfNotPresent
      name: csi-cephfsplugin
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 250m
          memory: 512Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /csi
        name: socket-dir
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /dev
        name: host-dev
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-config
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-g5wv7
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-g5wv7
        readOnly: true
    nodeName: k8s-node-orange
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-cephfs-provisioner-sa
    serviceAccountName: rook-csi-cephfs-provisioner-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - emptyDir:
        medium: Memory
      name: socket-dir
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - configMap:
        defaultMode: 420
        items:
        - key: csi-cluster-config-json
          path: config.json
        name: rook-ceph-csi-config
      name: ceph-csi-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - name: kube-api-access-g5wv7
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:59:33Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-05-21T23:06:54Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:59:40Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:59:40Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-05-21T23:06:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://259ed53f269b49ef9be86d514597c52ddead3835a6409320871115ab273a5b90
      image: registry.k8s.io/sig-storage/csi-attacher:v4.5.1
      imageID: registry.k8s.io/sig-storage/csi-attacher@sha256:9dcd469f02bbb7592ad61b0f848ec242f9ea2102187a0cd8407df33c2d633e9c
      lastState:
        terminated:
          containerID: containerd://ece6a649e87aad2af190063ab1453262b513fe8ad1197f1f4400785d466a8b56
          exitCode: 255
          finishedAt: "2024-09-23T15:59:18Z"
          reason: Unknown
          startedAt: "2024-09-23T15:47:53Z"
      name: csi-attacher
      ready: true
      restartCount: 7
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:59:36Z"
    - containerID: containerd://5d0d4eee194d9751b142c9f90c39fee925f8b2e53045e9deb2913d5b1e21737f
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imageID: quay.io/cephcsi/cephcsi@sha256:a338c0dec57b8033e01f277dc342607d7610411422cc3ce47f176ab059aeaf21
      lastState:
        terminated:
          containerID: containerd://438f5acc01bbfe6c5bfabfaf807ea8b031b982949e9053333f012fbde2408e9b
          exitCode: 255
          finishedAt: "2024-09-23T15:59:18Z"
          reason: Unknown
          startedAt: "2024-09-23T15:47:55Z"
      name: csi-cephfsplugin
      ready: true
      restartCount: 5
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:59:39Z"
    - containerID: containerd://3bfd6f2622c8d7d28978844b9b6b88ffdc7cc18add47703c8fa8a41d60f186ac
      image: registry.k8s.io/sig-storage/csi-provisioner:v4.0.1
      imageID: registry.k8s.io/sig-storage/csi-provisioner@sha256:bf5a235b67d8aea00f5b8ec24d384a2480e1017d5458d8a63b361e9eeb1608a9
      lastState:
        terminated:
          containerID: containerd://0cb03d3e55046083103c9e510f5bbab882ca22fa6a6ddf8cda893bad75f2ec3c
          exitCode: 255
          finishedAt: "2024-09-23T15:59:18Z"
          reason: Unknown
          startedAt: "2024-09-23T15:47:55Z"
      name: csi-provisioner
      ready: true
      restartCount: 7
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:59:39Z"
    - containerID: containerd://609912d1c6f89cb49e1e7df0247dbba9da3f01e6d3a523c8c4a0aa2ba1d66cdf
      image: registry.k8s.io/sig-storage/csi-resizer:v1.10.1
      imageID: registry.k8s.io/sig-storage/csi-resizer@sha256:4ecda2818f6d88a8f217babd459fdac31588f85581aa95ac7092bb0471ff8541
      lastState:
        terminated:
          containerID: containerd://ef2af6da1d1ae4f0eb59a998124d1da928f130003d9f5272f7efe77ac254ad31
          exitCode: 255
          finishedAt: "2024-09-23T15:59:18Z"
          reason: Unknown
          startedAt: "2024-09-23T15:47:54Z"
      name: csi-resizer
      ready: true
      restartCount: 7
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:59:38Z"
    - containerID: containerd://c9218d18db6ff70d9fc3dc6bd38d2c5753fe69ff4e209763f5b3ce569e23e9f6
      image: registry.k8s.io/sig-storage/csi-snapshotter:v7.0.2
      imageID: registry.k8s.io/sig-storage/csi-snapshotter@sha256:c4b6b02737bc24906fcce57fe6626d1a36cb2b91baa971af2a5e5a919093c34e
      lastState:
        terminated:
          containerID: containerd://6cc54f86181c092cd9d8f00e58d42d1bd295827f936998fe4e21ce42356434a1
          exitCode: 255
          finishedAt: "2024-09-23T15:59:18Z"
          reason: Unknown
          startedAt: "2024-09-23T15:47:54Z"
      name: csi-snapshotter
      ready: true
      restartCount: 6
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:59:37Z"
    hostIP: 10.0.90.12
    hostIPs:
    - ip: 10.0.90.12
    initContainerStatuses:
    - containerID: containerd://4301f55b86384a48ef8b77b4c9d6c7515ce0bea22215dbaed060c741f7784980
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 5
      started: false
      state:
        terminated:
          containerID: containerd://4301f55b86384a48ef8b77b4c9d6c7515ce0bea22215dbaed060c741f7784980
          exitCode: 0
          finishedAt: "2024-09-23T15:59:33Z"
          reason: Completed
          startedAt: "2024-09-23T15:59:32Z"
    phase: Running
    podIP: 10.42.186.151
    podIPs:
    - ip: 10.42.186.151
    qosClass: Burstable
    startTime: "2024-05-21T23:06:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-05-21T23:08:41Z"
    generateName: csi-cephfsplugin-
    labels:
      app: csi-cephfsplugin
      contains: csi-cephfsplugin-metrics
      controller-revision-hash: d4cd8668d
      pod-template-generation: "35"
    name: csi-cephfsplugin-qgxd8
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-cephfsplugin
      uid: b35ced57-1c62-4841-8edb-aa20a613f611
    resourceVersion: "1330832957"
    uid: e394efa8-5f19-40b7-898b-97018b976796
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - k8s-node-orange
    containers:
    - args:
      - --v=0
      - --csi-address=/csi/csi.sock
      - --kubelet-registration-path=/var/lib/kubelet/plugins/storage.cephfs.csi.ceph.com/csi.sock
      env:
      - name: TZ
        value: Australia/Sydney
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.1
      imagePullPolicy: IfNotPresent
      name: driver-registrar
      resources:
        limits:
          cpu: 100m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 128Mi
      securityContext:
        capabilities:
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-l4llr
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --type=cephfs
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --nodeserver=true
      - --drivername=storage.cephfs.csi.ceph.com
      - --pidlimit=-1
      - --forcecephkernelclient=true
      env:
      - name: TZ
        value: Australia/Sydney
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imagePullPolicy: IfNotPresent
      name: csi-cephfsplugin
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 250m
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /csi/mountinfo
        name: ceph-csi-mountinfo
      - mountPath: /var/lib/kubelet/plugins
        mountPropagation: Bidirectional
        name: csi-plugins-dir
      - mountPath: /var/lib/kubelet/pods
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /dev
        name: host-dev
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-config
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-l4llr
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-l4llr
        readOnly: true
    nodeName: k8s-node-orange
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-cephfs-plugin-sa
    serviceAccountName: rook-csi-cephfs-plugin-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - hostPath:
        path: /var/lib/kubelet/plugins/storage.cephfs.csi.ceph.com/mountinfo
        type: DirectoryOrCreate
      name: ceph-csi-mountinfo
    - hostPath:
        path: /var/lib/kubelet/plugins/storage.cephfs.csi.ceph.com/
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet/plugins
        type: Directory
      name: csi-plugins-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: Directory
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/pods
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - configMap:
        defaultMode: 420
        items:
        - key: csi-cluster-config-json
          path: config.json
        name: rook-ceph-csi-config
      name: ceph-csi-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - hostPath:
        path: /run/mount
        type: ""
      name: host-run-mount
    - name: kube-api-access-l4llr
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:59:22Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-05-21T23:08:41Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:59:23Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:59:23Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-05-21T23:08:41Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://08affb3769970fe03936eb96eb03da97b789b4abf6bb85ec4640f4b132f6a932
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imageID: quay.io/cephcsi/cephcsi@sha256:a338c0dec57b8033e01f277dc342607d7610411422cc3ce47f176ab059aeaf21
      lastState:
        terminated:
          containerID: containerd://2907a0971765a9e06ed1e76fbdc6e83ab4aa09e53a240ad7b58e72472e2e84fe
          exitCode: 255
          finishedAt: "2024-09-23T15:59:18Z"
          reason: Unknown
          startedAt: "2024-09-23T15:47:47Z"
      name: csi-cephfsplugin
      ready: true
      restartCount: 5
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:59:23Z"
    - containerID: containerd://96d4223a03a7b1aebab57735ff8c58c3d9c1cd0cdae3d30794b579e0fadb1a3c
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.1
      imageID: registry.k8s.io/sig-storage/csi-node-driver-registrar@sha256:f25af73ee708ff9c82595ae99493cdef9295bd96953366cddf36305f82555dac
      lastState:
        terminated:
          containerID: containerd://c51fcc8ac0923a342bcaf6fa85fe1c550fe3f669fff6df17b858e0870944f2be
          exitCode: 255
          finishedAt: "2024-09-23T15:59:18Z"
          reason: Unknown
          startedAt: "2024-09-23T15:47:45Z"
      name: driver-registrar
      ready: true
      restartCount: 5
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:59:22Z"
    hostIP: 10.0.90.12
    hostIPs:
    - ip: 10.0.90.12
    initContainerStatuses:
    - containerID: containerd://d52d8ac4bf59d41e8f77317bf1d057b648840d83f18b89da58640172c3ad9df3
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 5
      started: false
      state:
        terminated:
          containerID: containerd://d52d8ac4bf59d41e8f77317bf1d057b648840d83f18b89da58640172c3ad9df3
          exitCode: 0
          finishedAt: "2024-09-23T15:59:22Z"
          reason: Completed
          startedAt: "2024-09-23T15:59:22Z"
    phase: Running
    podIP: 10.0.90.12
    podIPs:
    - ip: 10.0.90.12
    qosClass: Burstable
    startTime: "2024-05-21T23:08:41Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-05-21T23:09:24Z"
    generateName: csi-rbdplugin-
    labels:
      app: csi-rbdplugin
      contains: csi-rbdplugin-metrics
      controller-revision-hash: 85f58dcc9b
      pod-template-generation: "35"
    name: csi-rbdplugin-7xw67
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-rbdplugin
      uid: bc536cfc-dd12-4b11-a588-6dd48a5e66af
    resourceVersion: "1283141081"
    uid: 35c99317-6811-4911-bf3a-4828d8910cdb
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - k8s-node-blue
    containers:
    - args:
      - --v=0
      - --csi-address=/csi/csi.sock
      - --kubelet-registration-path=/var/lib/kubelet/plugins/storage.rbd.csi.ceph.com/csi.sock
      env:
      - name: TZ
        value: Australia/Sydney
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.1
      imagePullPolicy: IfNotPresent
      name: driver-registrar
      resources:
        limits:
          cpu: 100m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 128Mi
      securityContext:
        capabilities:
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nn5t9
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --type=rbd
      - --nodeserver=true
      - --drivername=storage.rbd.csi.ceph.com
      - --pidlimit=-1
      - --stagingpath=/var/lib/kubelet/plugins/kubernetes.io/csi/
      env:
      - name: TZ
        value: Australia/Sydney
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imagePullPolicy: IfNotPresent
      name: csi-rbdplugin
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 250m
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/lib/kubelet/pods
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /var/lib/kubelet/plugins
        mountPropagation: Bidirectional
        name: plugin-mount-dir
      - mountPath: /dev
        name: host-dev
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-configs
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /run/secrets/tokens
        name: oidc-token
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nn5t9
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nn5t9
        readOnly: true
    nodeName: k8s-node-blue
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-rbd-plugin-sa
    serviceAccountName: rook-csi-rbd-plugin-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - hostPath:
        path: /var/lib/kubelet/plugins/storage.rbd.csi.ceph.com
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet/plugins
        type: Directory
      name: plugin-mount-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: Directory
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/pods
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: ceph-csi-configs
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: csi-cluster-config-json
              path: config.json
            name: rook-ceph-csi-config
        - configMap:
            items:
            - key: csi-mapping-config-json
              path: cluster-mapping.json
            name: rook-ceph-csi-mapping-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - hostPath:
        path: /run/mount
        type: ""
      name: host-run-mount
    - name: oidc-token
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            audience: ceph-csi-kms
            expirationSeconds: 3600
            path: oidc-token
    - name: kube-api-access-nn5t9
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-08-19T23:16:16Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-05-21T23:09:25Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-08-19T23:17:03Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-08-19T23:17:03Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-05-21T23:09:24Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://0065c034bc84104b258e39b25622d5db9600e6885bb8beb20e4c5286b396a9bd
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imageID: quay.io/cephcsi/cephcsi@sha256:a338c0dec57b8033e01f277dc342607d7610411422cc3ce47f176ab059aeaf21
      lastState:
        terminated:
          containerID: containerd://3704f12a677b001923de284edfc2c91c94c4b7338062be70663ad1e1e301ff50
          exitCode: 255
          finishedAt: "2024-08-19T23:16:47Z"
          reason: Error
          startedAt: "2024-08-19T23:16:17Z"
      name: csi-rbdplugin
      ready: true
      restartCount: 5
      started: true
      state:
        running:
          startedAt: "2024-08-19T23:17:03Z"
    - containerID: containerd://13a959753e6c6e9b1adb738db493435f1741bb2458f861cde72fcfde1fa8eb04
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.1
      imageID: registry.k8s.io/sig-storage/csi-node-driver-registrar@sha256:f25af73ee708ff9c82595ae99493cdef9295bd96953366cddf36305f82555dac
      lastState:
        terminated:
          containerID: containerd://3ae8d0fcdc095bac2f867562486c4f76b4c26d6f1c9a8b814ca60d22d0932447
          exitCode: 1
          finishedAt: "2024-08-19T23:16:47Z"
          reason: Error
          startedAt: "2024-08-19T23:16:17Z"
      name: driver-registrar
      ready: true
      restartCount: 5
      started: true
      state:
        running:
          startedAt: "2024-08-19T23:17:02Z"
    hostIP: 10.0.90.10
    hostIPs:
    - ip: 10.0.90.10
    initContainerStatuses:
    - containerID: containerd://b9409780568732a51e92fe3baa0f91e2b140d295aef23e5169876689d07d8835
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 3
      started: false
      state:
        terminated:
          containerID: containerd://b9409780568732a51e92fe3baa0f91e2b140d295aef23e5169876689d07d8835
          exitCode: 0
          finishedAt: "2024-08-19T23:16:16Z"
          reason: Completed
          startedAt: "2024-08-19T23:16:16Z"
    phase: Running
    podIP: 10.0.90.10
    podIPs:
    - ip: 10.0.90.10
    qosClass: Burstable
    startTime: "2024-05-21T23:09:24Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-05-21T23:09:05Z"
    generateName: csi-rbdplugin-
    labels:
      app: csi-rbdplugin
      contains: csi-rbdplugin-metrics
      controller-revision-hash: 85f58dcc9b
      pod-template-generation: "35"
    name: csi-rbdplugin-9z6k5
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-rbdplugin
      uid: bc536cfc-dd12-4b11-a588-6dd48a5e66af
    resourceVersion: "1330832984"
    uid: 7d79d851-2ac1-4e33-9d43-48887d3b0c83
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - k8s-node-orange
    containers:
    - args:
      - --v=0
      - --csi-address=/csi/csi.sock
      - --kubelet-registration-path=/var/lib/kubelet/plugins/storage.rbd.csi.ceph.com/csi.sock
      env:
      - name: TZ
        value: Australia/Sydney
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.1
      imagePullPolicy: IfNotPresent
      name: driver-registrar
      resources:
        limits:
          cpu: 100m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 128Mi
      securityContext:
        capabilities:
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ggrl2
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --type=rbd
      - --nodeserver=true
      - --drivername=storage.rbd.csi.ceph.com
      - --pidlimit=-1
      - --stagingpath=/var/lib/kubelet/plugins/kubernetes.io/csi/
      env:
      - name: TZ
        value: Australia/Sydney
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imagePullPolicy: IfNotPresent
      name: csi-rbdplugin
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 250m
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/lib/kubelet/pods
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /var/lib/kubelet/plugins
        mountPropagation: Bidirectional
        name: plugin-mount-dir
      - mountPath: /dev
        name: host-dev
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-configs
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /run/secrets/tokens
        name: oidc-token
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ggrl2
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ggrl2
        readOnly: true
    nodeName: k8s-node-orange
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-rbd-plugin-sa
    serviceAccountName: rook-csi-rbd-plugin-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - hostPath:
        path: /var/lib/kubelet/plugins/storage.rbd.csi.ceph.com
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet/plugins
        type: Directory
      name: plugin-mount-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: Directory
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/pods
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: ceph-csi-configs
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: csi-cluster-config-json
              path: config.json
            name: rook-ceph-csi-config
        - configMap:
            items:
            - key: csi-mapping-config-json
              path: cluster-mapping.json
            name: rook-ceph-csi-mapping-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - hostPath:
        path: /run/mount
        type: ""
      name: host-run-mount
    - name: oidc-token
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            audience: ceph-csi-kms
            expirationSeconds: 3600
            path: oidc-token
    - name: kube-api-access-ggrl2
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:59:22Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-05-21T23:09:06Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:59:23Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:59:23Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-05-21T23:09:05Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://ebf3ba8d2de2d3c2190e1059ca7fc7161e77a76f1d90d89af8806a5701ffca80
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imageID: quay.io/cephcsi/cephcsi@sha256:a338c0dec57b8033e01f277dc342607d7610411422cc3ce47f176ab059aeaf21
      lastState:
        terminated:
          containerID: containerd://c4ab395bbf2ff85fb56586988916591f3aa9a87350f71ec65c3698e98a3c4c0b
          exitCode: 255
          finishedAt: "2024-09-23T15:59:18Z"
          reason: Unknown
          startedAt: "2024-09-23T15:47:45Z"
      name: csi-rbdplugin
      ready: true
      restartCount: 5
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:59:23Z"
    - containerID: containerd://99236d4fcf3f4826ec44dac128733c1d4df2a20497136dbeffaffe89acff1a53
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.1
      imageID: registry.k8s.io/sig-storage/csi-node-driver-registrar@sha256:f25af73ee708ff9c82595ae99493cdef9295bd96953366cddf36305f82555dac
      lastState:
        terminated:
          containerID: containerd://520654d0530a72640d29c38312ccdbe45150a137e17961cf71c1f1c2448e6952
          exitCode: 255
          finishedAt: "2024-09-23T15:59:18Z"
          reason: Unknown
          startedAt: "2024-09-23T15:47:44Z"
      name: driver-registrar
      ready: true
      restartCount: 5
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:59:22Z"
    hostIP: 10.0.90.12
    hostIPs:
    - ip: 10.0.90.12
    initContainerStatuses:
    - containerID: containerd://ccc1ccc281194904fc6b2dcf5c4054c814d847a31cf256287324b39897aa3077
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 5
      started: false
      state:
        terminated:
          containerID: containerd://ccc1ccc281194904fc6b2dcf5c4054c814d847a31cf256287324b39897aa3077
          exitCode: 0
          finishedAt: "2024-09-23T15:59:22Z"
          reason: Completed
          startedAt: "2024-09-23T15:59:22Z"
    phase: Running
    podIP: 10.0.90.12
    podIPs:
    - ip: 10.0.90.12
    qosClass: Burstable
    startTime: "2024-05-21T23:09:05Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-09-23T15:07:33Z"
    generateName: csi-rbdplugin-
    labels:
      app: csi-rbdplugin
      contains: csi-rbdplugin-metrics
      controller-revision-hash: 85f58dcc9b
      pod-template-generation: "35"
    name: csi-rbdplugin-bkctr
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-rbdplugin
      uid: bc536cfc-dd12-4b11-a588-6dd48a5e66af
    resourceVersion: "1330786331"
    uid: 9f2b8af0-b5b1-48da-9d2c-b974b2e26413
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - k8s-node-yellow
    containers:
    - args:
      - --v=0
      - --csi-address=/csi/csi.sock
      - --kubelet-registration-path=/var/lib/kubelet/plugins/storage.rbd.csi.ceph.com/csi.sock
      env:
      - name: TZ
        value: Australia/Sydney
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.1
      imagePullPolicy: IfNotPresent
      name: driver-registrar
      resources:
        limits:
          cpu: 100m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 128Mi
      securityContext:
        capabilities:
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w9ffp
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --type=rbd
      - --nodeserver=true
      - --drivername=storage.rbd.csi.ceph.com
      - --pidlimit=-1
      - --stagingpath=/var/lib/kubelet/plugins/kubernetes.io/csi/
      env:
      - name: TZ
        value: Australia/Sydney
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imagePullPolicy: IfNotPresent
      name: csi-rbdplugin
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 250m
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/lib/kubelet/pods
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /var/lib/kubelet/plugins
        mountPropagation: Bidirectional
        name: plugin-mount-dir
      - mountPath: /dev
        name: host-dev
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-configs
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /run/secrets/tokens
        name: oidc-token
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w9ffp
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w9ffp
        readOnly: true
    nodeName: k8s-node-yellow
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-rbd-plugin-sa
    serviceAccountName: rook-csi-rbd-plugin-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - hostPath:
        path: /var/lib/kubelet/plugins/storage.rbd.csi.ceph.com
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet/plugins
        type: Directory
      name: plugin-mount-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: Directory
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/pods
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: ceph-csi-configs
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: csi-cluster-config-json
              path: config.json
            name: rook-ceph-csi-config
        - configMap:
            items:
            - key: csi-mapping-config-json
              path: cluster-mapping.json
            name: rook-ceph-csi-mapping-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - hostPath:
        path: /run/mount
        type: ""
      name: host-run-mount
    - name: oidc-token
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            audience: ceph-csi-kms
            expirationSeconds: 3600
            path: oidc-token
    - name: kube-api-access-w9ffp
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:07:33Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:07:33Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:08:42Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:08:42Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:07:33Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://261aa8ff4b69ad4a3e7cfa456d920aef49bf501e6225984b3d36a0640a361787
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imageID: quay.io/cephcsi/cephcsi@sha256:a338c0dec57b8033e01f277dc342607d7610411422cc3ce47f176ab059aeaf21
      lastState: {}
      name: csi-rbdplugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:08:41Z"
    - containerID: containerd://d43e6d4c395f98493daa22dcba56aef92ca25654f40365f95d2adf181a43aae9
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.1
      imageID: registry.k8s.io/sig-storage/csi-node-driver-registrar@sha256:f25af73ee708ff9c82595ae99493cdef9295bd96953366cddf36305f82555dac
      lastState: {}
      name: driver-registrar
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:08:33Z"
    hostIP: 10.0.90.14
    hostIPs:
    - ip: 10.0.90.14
    initContainerStatuses:
    - containerID: containerd://f8aeb2fefe1d10d13c5e4ed19ee6be873c38a093e3d45f9c9e1bd40debab2d4b
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://f8aeb2fefe1d10d13c5e4ed19ee6be873c38a093e3d45f9c9e1bd40debab2d4b
          exitCode: 0
          finishedAt: "2024-09-23T15:07:33Z"
          reason: Completed
          startedAt: "2024-09-23T15:07:33Z"
    phase: Running
    podIP: 10.0.90.14
    podIPs:
    - ip: 10.0.90.14
    qosClass: Burstable
    startTime: "2024-09-23T15:07:33Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: db88d96e15a5864ea693aec16da5068be4be582589dac3cf455a99fee32d6c6b
      cni.projectcalico.org/podIP: 10.42.3.31/32
      cni.projectcalico.org/podIPs: 10.42.3.31/32
    creationTimestamp: "2024-08-22T01:16:45Z"
    generateName: csi-rbdplugin-provisioner-678946cbb-
    labels:
      app: csi-rbdplugin-provisioner
      contains: csi-rbdplugin-metrics
      pod-template-hash: 678946cbb
    name: csi-rbdplugin-provisioner-678946cbb-fmz66
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: csi-rbdplugin-provisioner-678946cbb
      uid: e60429d2-0668-499e-95fc-a6a3ae8e26d6
    resourceVersion: "1286971398"
    uid: c2c71293-88f2-409d-aded-4a232377b4d3
  spec:
    affinity:
      nodeAffinity: {}
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - csi-rbdplugin-provisioner
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=2m30s
      - --retry-interval-start=500ms
      - --leader-election=true
      - --leader-election-namespace=storage
      - --leader-election-lease-duration=2m17s
      - --leader-election-renew-deadline=1m47s
      - --leader-election-retry-period=26s
      - --default-fstype=ext4
      - --extra-create-metadata=true
      - --prevent-volume-mode-conversion=true
      - --feature-gates=HonorPVReclaimPolicy=true
      env:
      - name: TZ
        value: Australia/Sydney
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-provisioner:v4.0.1
      imagePullPolicy: IfNotPresent
      name: csi-provisioner
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gbw8z
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=2m30s
      - --leader-election=true
      - --leader-election-namespace=storage
      - --leader-election-lease-duration=2m17s
      - --leader-election-renew-deadline=1m47s
      - --leader-election-retry-period=26s
      - --handle-volume-inuse-error=false
      - --feature-gates=RecoverVolumeExpansionFailure=true
      env:
      - name: TZ
        value: Australia/Sydney
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-resizer:v1.10.1
      imagePullPolicy: IfNotPresent
      name: csi-resizer
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gbw8z
        readOnly: true
    - args:
      - --v=0
      - --timeout=2m30s
      - --csi-address=$(ADDRESS)
      - --leader-election=true
      - --leader-election-namespace=storage
      - --leader-election-lease-duration=2m17s
      - --leader-election-renew-deadline=1m47s
      - --leader-election-retry-period=26s
      - --default-fstype=ext4
      env:
      - name: TZ
        value: Australia/Sydney
      - name: ADDRESS
        value: /csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-attacher:v4.5.1
      imagePullPolicy: IfNotPresent
      name: csi-attacher
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gbw8z
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=2m30s
      - --leader-election=true
      - --leader-election-namespace=storage
      - --leader-election-lease-duration=2m17s
      - --leader-election-renew-deadline=1m47s
      - --leader-election-retry-period=26s
      - --extra-create-metadata=true
      env:
      - name: TZ
        value: Australia/Sydney
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-snapshotter:v7.0.2
      imagePullPolicy: IfNotPresent
      name: csi-snapshotter
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gbw8z
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --type=rbd
      - --controllerserver=true
      - --drivername=storage.rbd.csi.ceph.com
      - --pidlimit=-1
      env:
      - name: TZ
        value: Australia/Sydney
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi-provisioner.sock
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imagePullPolicy: IfNotPresent
      name: csi-rbdplugin
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 250m
          memory: 512Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /csi
        name: socket-dir
      - mountPath: /dev
        name: host-dev
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-configs
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/secrets/tokens
        name: oidc-token
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gbw8z
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gbw8z
        readOnly: true
    nodeName: k8s-node-blue
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-rbd-provisioner-sa
    serviceAccountName: rook-csi-rbd-provisioner-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - emptyDir:
        medium: Memory
      name: socket-dir
    - name: ceph-csi-configs
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: csi-cluster-config-json
              path: config.json
            name: rook-ceph-csi-config
        - configMap:
            items:
            - key: csi-mapping-config-json
              path: cluster-mapping.json
            name: rook-ceph-csi-mapping-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - name: oidc-token
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            audience: ceph-csi-kms
            expirationSeconds: 3600
            path: oidc-token
    - name: kube-api-access-gbw8z
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-08-22T01:16:47Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-08-22T01:16:47Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-08-22T01:16:48Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-08-22T01:16:48Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-08-22T01:16:45Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://ab03ebfd78dc5d00d960a5b778c34cf2dbd6637c6f9a38241fc64ad31411ac7b
      image: registry.k8s.io/sig-storage/csi-attacher:v4.5.1
      imageID: registry.k8s.io/sig-storage/csi-attacher@sha256:9dcd469f02bbb7592ad61b0f848ec242f9ea2102187a0cd8407df33c2d633e9c
      lastState: {}
      name: csi-attacher
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-08-22T01:16:47Z"
    - containerID: containerd://5f0b10c467ccfe518af18ac0a4adadce217b484780109b75db1eade47304afa9
      image: registry.k8s.io/sig-storage/csi-provisioner:v4.0.1
      imageID: registry.k8s.io/sig-storage/csi-provisioner@sha256:bf5a235b67d8aea00f5b8ec24d384a2480e1017d5458d8a63b361e9eeb1608a9
      lastState: {}
      name: csi-provisioner
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-08-22T01:16:47Z"
    - containerID: containerd://dfb6132c69dda62f43e99e0fe913989e35b4304a8e48b1aae3f32273ae234536
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imageID: quay.io/cephcsi/cephcsi@sha256:a338c0dec57b8033e01f277dc342607d7610411422cc3ce47f176ab059aeaf21
      lastState: {}
      name: csi-rbdplugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-08-22T01:16:48Z"
    - containerID: containerd://fcd00a9d013781cf89dc8dd67df4dd7057d57966f14d6566e717128d63b33c26
      image: registry.k8s.io/sig-storage/csi-resizer:v1.10.1
      imageID: registry.k8s.io/sig-storage/csi-resizer@sha256:4ecda2818f6d88a8f217babd459fdac31588f85581aa95ac7092bb0471ff8541
      lastState: {}
      name: csi-resizer
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-08-22T01:16:47Z"
    - containerID: containerd://f50625c78017f6a187ba7795e56fa6eb8de1aab931551e06629f24bc77087d3e
      image: registry.k8s.io/sig-storage/csi-snapshotter:v7.0.2
      imageID: registry.k8s.io/sig-storage/csi-snapshotter@sha256:c4b6b02737bc24906fcce57fe6626d1a36cb2b91baa971af2a5e5a919093c34e
      lastState: {}
      name: csi-snapshotter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-08-22T01:16:48Z"
    hostIP: 10.0.90.10
    hostIPs:
    - ip: 10.0.90.10
    initContainerStatuses:
    - containerID: containerd://119d423c0a4a72d8caa4181261ebb8d7475e0d4c37ef725f96be1690bee748ce
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://119d423c0a4a72d8caa4181261ebb8d7475e0d4c37ef725f96be1690bee748ce
          exitCode: 0
          finishedAt: "2024-08-22T01:16:47Z"
          reason: Completed
          startedAt: "2024-08-22T01:16:47Z"
    phase: Running
    podIP: 10.42.3.31
    podIPs:
    - ip: 10.42.3.31
    qosClass: Burstable
    startTime: "2024-08-22T01:16:45Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: b432399eee4d85388fafbb4020ceb8f0ea34cdf1ea30d7d87881df04b0d1de34
      cni.projectcalico.org/podIP: 10.42.186.188/32
      cni.projectcalico.org/podIPs: 10.42.186.188/32
    creationTimestamp: "2024-05-21T23:06:52Z"
    generateName: csi-rbdplugin-provisioner-678946cbb-
    labels:
      app: csi-rbdplugin-provisioner
      contains: csi-rbdplugin-metrics
      pod-template-hash: 678946cbb
    name: csi-rbdplugin-provisioner-678946cbb-zt25g
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: csi-rbdplugin-provisioner-678946cbb
      uid: e60429d2-0668-499e-95fc-a6a3ae8e26d6
    resourceVersion: "1330833596"
    uid: 4416afe4-b8f8-44dc-9aaf-828892753920
  spec:
    affinity:
      nodeAffinity: {}
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - csi-rbdplugin-provisioner
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=2m30s
      - --retry-interval-start=500ms
      - --leader-election=true
      - --leader-election-namespace=storage
      - --leader-election-lease-duration=2m17s
      - --leader-election-renew-deadline=1m47s
      - --leader-election-retry-period=26s
      - --default-fstype=ext4
      - --extra-create-metadata=true
      - --prevent-volume-mode-conversion=true
      - --feature-gates=HonorPVReclaimPolicy=true
      env:
      - name: TZ
        value: Australia/Sydney
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-provisioner:v4.0.1
      imagePullPolicy: IfNotPresent
      name: csi-provisioner
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gtgkn
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=2m30s
      - --leader-election=true
      - --leader-election-namespace=storage
      - --leader-election-lease-duration=2m17s
      - --leader-election-renew-deadline=1m47s
      - --leader-election-retry-period=26s
      - --handle-volume-inuse-error=false
      - --feature-gates=RecoverVolumeExpansionFailure=true
      env:
      - name: TZ
        value: Australia/Sydney
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-resizer:v1.10.1
      imagePullPolicy: IfNotPresent
      name: csi-resizer
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gtgkn
        readOnly: true
    - args:
      - --v=0
      - --timeout=2m30s
      - --csi-address=$(ADDRESS)
      - --leader-election=true
      - --leader-election-namespace=storage
      - --leader-election-lease-duration=2m17s
      - --leader-election-renew-deadline=1m47s
      - --leader-election-retry-period=26s
      - --default-fstype=ext4
      env:
      - name: TZ
        value: Australia/Sydney
      - name: ADDRESS
        value: /csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-attacher:v4.5.1
      imagePullPolicy: IfNotPresent
      name: csi-attacher
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gtgkn
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=2m30s
      - --leader-election=true
      - --leader-election-namespace=storage
      - --leader-election-lease-duration=2m17s
      - --leader-election-renew-deadline=1m47s
      - --leader-election-retry-period=26s
      - --extra-create-metadata=true
      env:
      - name: TZ
        value: Australia/Sydney
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-snapshotter:v7.0.2
      imagePullPolicy: IfNotPresent
      name: csi-snapshotter
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gtgkn
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --type=rbd
      - --controllerserver=true
      - --drivername=storage.rbd.csi.ceph.com
      - --pidlimit=-1
      env:
      - name: TZ
        value: Australia/Sydney
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi-provisioner.sock
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imagePullPolicy: IfNotPresent
      name: csi-rbdplugin
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 250m
          memory: 512Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /csi
        name: socket-dir
      - mountPath: /dev
        name: host-dev
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-configs
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/secrets/tokens
        name: oidc-token
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gtgkn
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gtgkn
        readOnly: true
    nodeName: k8s-node-orange
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-rbd-provisioner-sa
    serviceAccountName: rook-csi-rbd-provisioner-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - emptyDir:
        medium: Memory
      name: socket-dir
    - name: ceph-csi-configs
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: csi-cluster-config-json
              path: config.json
            name: rook-ceph-csi-config
        - configMap:
            items:
            - key: csi-mapping-config-json
              path: cluster-mapping.json
            name: rook-ceph-csi-mapping-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - name: oidc-token
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            audience: ceph-csi-kms
            expirationSeconds: 3600
            path: oidc-token
    - name: kube-api-access-gtgkn
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:59:28Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-05-21T23:06:53Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:59:37Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:59:37Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-05-21T23:06:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://cdc81f9fdde40c97b599767aaa29c8af5391195efadb243fa64e2db3de9b0f91
      image: registry.k8s.io/sig-storage/csi-attacher:v4.5.1
      imageID: registry.k8s.io/sig-storage/csi-attacher@sha256:9dcd469f02bbb7592ad61b0f848ec242f9ea2102187a0cd8407df33c2d633e9c
      lastState:
        terminated:
          containerID: containerd://9dd871abaeba6f16b18bc42671c704b98d677493d553f29acb3da7b4330f4fcf
          exitCode: 255
          finishedAt: "2024-09-23T15:59:18Z"
          reason: Unknown
          startedAt: "2024-09-23T15:47:55Z"
      name: csi-attacher
      ready: true
      restartCount: 6
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:59:32Z"
    - containerID: containerd://19da80b79e6f0223a322a60479db6abcffb66717edb696444545591a1bbe5e1d
      image: registry.k8s.io/sig-storage/csi-provisioner:v4.0.1
      imageID: registry.k8s.io/sig-storage/csi-provisioner@sha256:bf5a235b67d8aea00f5b8ec24d384a2480e1017d5458d8a63b361e9eeb1608a9
      lastState:
        terminated:
          containerID: containerd://b7061cc69bdcf115eb9aaad1ff50bcb1b5457d7b8679fe030e32b670a6bbf669
          exitCode: 255
          finishedAt: "2024-09-23T15:59:18Z"
          reason: Unknown
          startedAt: "2024-09-23T15:47:54Z"
      name: csi-provisioner
      ready: true
      restartCount: 7
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:59:29Z"
    - containerID: containerd://39f3ab879541e2dfa36543f448f176328eb41c34fdfca79ed3980ac7442facb7
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imageID: quay.io/cephcsi/cephcsi@sha256:a338c0dec57b8033e01f277dc342607d7610411422cc3ce47f176ab059aeaf21
      lastState:
        terminated:
          containerID: containerd://d3583a2e707a8116719d4275f6bdfaa08ed972fee2390cd1a7fa78d2f685e406
          exitCode: 255
          finishedAt: "2024-09-23T15:59:18Z"
          reason: Unknown
          startedAt: "2024-09-23T15:47:56Z"
      name: csi-rbdplugin
      ready: true
      restartCount: 5
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:59:36Z"
    - containerID: containerd://ce1e8f72efbd36b6db0f2a50a2245163767f9e862024fb048fc281632988f1c9
      image: registry.k8s.io/sig-storage/csi-resizer:v1.10.1
      imageID: registry.k8s.io/sig-storage/csi-resizer@sha256:4ecda2818f6d88a8f217babd459fdac31588f85581aa95ac7092bb0471ff8541
      lastState:
        terminated:
          containerID: containerd://ede43a2c815cacdc8a7d1901d60b973e6f93c5929a3ce68728c79dd2e57d9a50
          exitCode: 255
          finishedAt: "2024-09-23T15:59:18Z"
          reason: Unknown
          startedAt: "2024-09-23T15:47:55Z"
      name: csi-resizer
      ready: true
      restartCount: 7
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:59:30Z"
    - containerID: containerd://6dff75e7d7b74a90241645a1d2d481bc224a1caf45c46862c26d1c4a87fef02f
      image: registry.k8s.io/sig-storage/csi-snapshotter:v7.0.2
      imageID: registry.k8s.io/sig-storage/csi-snapshotter@sha256:c4b6b02737bc24906fcce57fe6626d1a36cb2b91baa971af2a5e5a919093c34e
      lastState:
        terminated:
          containerID: containerd://7b41a7816425c9967fca56ac36698e182ec3a9b24fabe922c969d4535754b445
          exitCode: 255
          finishedAt: "2024-09-23T15:59:18Z"
          reason: Unknown
          startedAt: "2024-09-23T15:47:56Z"
      name: csi-snapshotter
      ready: true
      restartCount: 7
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:59:34Z"
    hostIP: 10.0.90.12
    hostIPs:
    - ip: 10.0.90.12
    initContainerStatuses:
    - containerID: containerd://dc03c02f23596f405f48814f27e8425a2c0e656cec489360701ed0136519d1c7
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 5
      started: false
      state:
        terminated:
          containerID: containerd://dc03c02f23596f405f48814f27e8425a2c0e656cec489360701ed0136519d1c7
          exitCode: 0
          finishedAt: "2024-09-23T15:59:28Z"
          reason: Completed
          startedAt: "2024-09-23T15:59:28Z"
    phase: Running
    podIP: 10.42.186.188
    podIPs:
    - ip: 10.42.186.188
    qosClass: Burstable
    startTime: "2024-05-21T23:06:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: dcea377e394ad788f303cb7f79675f7fb700bd400f9b60527f243a71a0dd68d1
      cni.projectcalico.org/podIP: 10.42.3.39/32
      cni.projectcalico.org/podIPs: 10.42.3.39/32
    creationTimestamp: "2024-08-31T01:03:08Z"
    generateName: lancache-6cbdcb8c75-
    labels:
      app.kubernetes.io/instance: lancache
      app.kubernetes.io/name: lancache
      pod-template-hash: 6cbdcb8c75
    name: lancache-6cbdcb8c75-v4ftr
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: lancache-6cbdcb8c75
      uid: 3dd29679-80ac-4493-8190-3864f3445527
    resourceVersion: "1349044154"
    uid: 007e3fd2-9f35-49c3-9111-4b8b5b1d7049
  spec:
    automountServiceAccountToken: true
    containers:
    - env:
      - name: TZ
        value: Australia/Sydney
      - name: CACHE_DISK_SIZE
        value: 5000g
      - name: CACHE_INDEX_SIZE
        value: 1000m
      image: lancachenet/monolithic:latest
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /lancache-heartbeat
          port: 80
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: lancache
      ports:
      - containerPort: 80
        name: http
        protocol: TCP
      - containerPort: 443
        name: https
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /lancache-heartbeat
          port: 80
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      startupProbe:
        failureThreshold: 30
        httpGet:
          path: /lancache-heartbeat
          port: 80
          scheme: HTTP
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /data/cache
        name: cache
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-f5z9q
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-f5z9q
        readOnly: true
    nodeName: k8s-node-blue
    nodeSelector:
      kubernetes.io/arch: amd64
    preemptionPolicy: PreemptLowerPriority
    priority: 100
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - name: cache
      persistentVolumeClaim:
        claimName: lancache-cache
    - name: kube-api-access-f5z9q
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-08-31T01:03:15Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-08-31T01:03:15Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-10-10T22:15:02Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-10-10T22:15:02Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-08-31T01:03:08Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://4611b0de6ccc2d0276174072f4d8c580af8028c91525f944b638bed38d6170d9
      image: docker.io/lancachenet/monolithic:latest
      imageID: docker.io/lancachenet/monolithic@sha256:b72d6b909b9e3fb7b521e90aab97479f7977bf6bee97e89a095e1afdbd6d3b85
      lastState: {}
      name: lancache
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-08-31T01:03:15Z"
    hostIP: 10.0.90.10
    hostIPs:
    - ip: 10.0.90.10
    initContainerStatuses:
    - containerID: containerd://e6584648d8fc128dc7e9d78844e63bf40760fc2ba76d8d98f7de489345733409
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://e6584648d8fc128dc7e9d78844e63bf40760fc2ba76d8d98f7de489345733409
          exitCode: 0
          finishedAt: "2024-08-31T01:03:15Z"
          reason: Completed
          startedAt: "2024-08-31T01:03:15Z"
    phase: Running
    podIP: 10.42.3.39
    podIPs:
    - ip: 10.42.3.39
    qosClass: Burstable
    startTime: "2024-08-31T01:03:08Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: a37415573add6c584ebb2fe5a79294cdcaf3d8effbe16f8b7ea315c6c4ae49cc
      cni.projectcalico.org/podIP: 10.42.3.95/32
      cni.projectcalico.org/podIPs: 10.42.3.95/32
    creationTimestamp: "2024-08-31T05:33:15Z"
    generateName: local-path-storage-local-path-provisioner-547567b747-
    labels:
      app.kubernetes.io/instance: local-path-storage
      app.kubernetes.io/name: local-path-provisioner
      pod-template-hash: 547567b747
    name: local-path-storage-local-path-provisioner-547567b747-zcvr9
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: local-path-storage-local-path-provisioner-547567b747
      uid: ee800aba-1636-4d72-9623-16d429914301
    resourceVersion: "1303954494"
    uid: 5733d4cb-78a5-44fb-8fef-b53fa61e838b
  spec:
    containers:
    - command:
      - local-path-provisioner
      - --debug
      - start
      - --config
      - /etc/config/config.json
      - --service-account-name
      - local-path-storage-local-path-provisioner
      - --provisioner-name
      - cluster.local/local-path-storage-local-path-provisioner
      - --helper-image
      - busybox:latest
      - --configmap-name
      - local-path-config
      env:
      - name: TZ
        value: Australia/Sydney
      - name: POD_NAMESPACE
        value: storage
      image: rancher/local-path-provisioner:v0.0.23
      imagePullPolicy: IfNotPresent
      name: local-path-provisioner
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/config/
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-s5rz8
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-s5rz8
        readOnly: true
    nodeName: k8s-node-blue
    preemptionPolicy: PreemptLowerPriority
    priority: 100
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: local-path-storage-local-path-provisioner
    serviceAccountName: local-path-storage-local-path-provisioner
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - configMap:
        defaultMode: 420
        name: local-path-config
      name: config-volume
    - name: kube-api-access-s5rz8
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-08-31T05:33:16Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-08-31T05:33:16Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-08-31T05:33:17Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-08-31T05:33:17Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-08-31T05:33:15Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://d861e1b196ce8ed8284a553ece960a365000815e70d236d5275860e1f4fbdc5d
      image: docker.io/rancher/local-path-provisioner:v0.0.23
      imageID: docker.io/rancher/local-path-provisioner@sha256:db1a3225290dd8be481a1965fc7040954d0aa0e1f86a77c92816d7c62a02ae5c
      lastState: {}
      name: local-path-provisioner
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-08-31T05:33:16Z"
    hostIP: 10.0.90.10
    hostIPs:
    - ip: 10.0.90.10
    initContainerStatuses:
    - containerID: containerd://68aed841fe434ec443344d2536d073f95e9044e1b53301ca091fddd2168e4466
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://68aed841fe434ec443344d2536d073f95e9044e1b53301ca091fddd2168e4466
          exitCode: 0
          finishedAt: "2024-08-31T05:33:16Z"
          reason: Completed
          startedAt: "2024-08-31T05:33:16Z"
    phase: Running
    podIP: 10.42.3.95
    podIPs:
    - ip: 10.42.3.95
    qosClass: Burstable
    startTime: "2024-08-31T05:33:15Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: c72f5f5b275491f34be2cebb65d0c1e4c2ef09325d33ae76a6e4cc6097d2af5d
      cni.projectcalico.org/podIP: 10.42.3.24/32
      cni.projectcalico.org/podIPs: 10.42.3.24/32
      prometheus.io/port: "9102"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-09-22T00:07:52Z"
    generateName: rook-ceph-crashcollector-k8s-node-blue-66774879f9-
    labels:
      app: rook-ceph-crashcollector
      ceph-version: 18.2.4-0
      ceph_daemon_id: crash
      crashcollector: crash
      kubernetes.io/hostname: k8s-node-blue
      node_name: k8s-node-blue
      pod-template-hash: 66774879f9
      rook_cluster: storage
    name: rook-ceph-crashcollector-k8s-node-blue-66774879f9-5x7wk
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-crashcollector-k8s-node-blue-66774879f9
      uid: 8578f421-46a0-4326-9f52-1d4c3de5e926
    resourceVersion: "1328740235"
    uid: 8a797994-2300-4fe4-8d34-4238597ae1aa
  spec:
    containers:
    - command:
      - ceph-crash
      env:
      - name: TZ
        value: Australia/Sydney
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.4
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST) -k /etc/ceph/crash-collector-keyring-store/keyring
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: ceph-crash
      resources:
        limits:
          cpu: 500m
          memory: 60Mi
        requests:
          cpu: 50m
          memory: 60Mi
      securityContext:
        privileged: false
        runAsGroup: 167
        runAsUser: 167
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /etc/ceph/crash-collector-keyring-store/
        name: rook-ceph-crash-collector-keyring
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5x8kl
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5x8kl
        readOnly: true
    - args:
      - /var/lib/ceph/crash/posted
      command:
      - mkdir
      - -p
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: make-container-crash-dir
      resources:
        limits:
          cpu: 500m
          memory: 60Mi
        requests:
          cpu: 50m
          memory: 60Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5x8kl
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: 500m
          memory: 60Mi
        requests:
          cpu: 50m
          memory: 60Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5x8kl
        readOnly: true
    nodeName: k8s-node-blue
    nodeSelector:
      kubernetes.io/hostname: k8s-node-blue
    preemptionPolicy: PreemptLowerPriority
    priority: 100
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-default
    serviceAccountName: rook-ceph-default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 5
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/storage/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/storage/crash
        type: ""
      name: rook-ceph-crash
    - name: rook-ceph-crash-collector-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-crash-collector-keyring
    - name: kube-api-access-5x8kl
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-09-22T00:07:53Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-09-22T00:07:55Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-09-22T00:07:56Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-09-22T00:07:56Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-09-22T00:07:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a8ff2f64e2684d164c638a522d3e88bc15abbdc948d00c666aed4f0dd4534f37
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: ceph-crash
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-09-22T00:07:55Z"
    hostIP: 10.0.90.10
    hostIPs:
    - ip: 10.0.90.10
    initContainerStatuses:
    - containerID: containerd://23d055f0e40553bd427a57d19bd133c2a2a2c5817453a760ea15f596f631e9ad
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://23d055f0e40553bd427a57d19bd133c2a2a2c5817453a760ea15f596f631e9ad
          exitCode: 0
          finishedAt: "2024-09-22T00:07:53Z"
          reason: Completed
          startedAt: "2024-09-22T00:07:53Z"
    - containerID: containerd://bc5b3fe839bf6ea344f943cddd6bf597ea8ee9388a44619e718eedc6f0abdd35
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: make-container-crash-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://bc5b3fe839bf6ea344f943cddd6bf597ea8ee9388a44619e718eedc6f0abdd35
          exitCode: 0
          finishedAt: "2024-09-22T00:07:53Z"
          reason: Completed
          startedAt: "2024-09-22T00:07:53Z"
    - containerID: containerd://b8782fe108e81bdccfc94adcd8969ac4162e1d598d2fffec87c38644eed959b8
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://b8782fe108e81bdccfc94adcd8969ac4162e1d598d2fffec87c38644eed959b8
          exitCode: 0
          finishedAt: "2024-09-22T00:07:54Z"
          reason: Completed
          startedAt: "2024-09-22T00:07:54Z"
    phase: Running
    podIP: 10.42.3.24
    podIPs:
    - ip: 10.42.3.24
    qosClass: Burstable
    startTime: "2024-09-22T00:07:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 5aecc3c3445adc054d7f6f15b0eae36250fb19622cc568947ee2184cfb3656b6
      cni.projectcalico.org/podIP: 10.42.186.158/32
      cni.projectcalico.org/podIPs: 10.42.186.158/32
      prometheus.io/port: "9102"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-09-22T00:07:54Z"
    generateName: rook-ceph-crashcollector-k8s-node-orange-7778f59bb5-
    labels:
      app: rook-ceph-crashcollector
      ceph-version: 18.2.4-0
      ceph_daemon_id: crash
      crashcollector: crash
      kubernetes.io/hostname: k8s-node-orange
      node_name: k8s-node-orange
      pod-template-hash: 7778f59bb5
      rook_cluster: storage
    name: rook-ceph-crashcollector-k8s-node-orange-7778f59bb5-9q7fc
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-crashcollector-k8s-node-orange-7778f59bb5
      uid: 71b0eb04-cb6c-4f3e-b1ad-1c9dafa36451
    resourceVersion: "1330833870"
    uid: c29f8213-f4f0-4467-a9f6-812abd55593f
  spec:
    containers:
    - command:
      - ceph-crash
      env:
      - name: TZ
        value: Australia/Sydney
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.4
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST) -k /etc/ceph/crash-collector-keyring-store/keyring
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: ceph-crash
      resources:
        limits:
          cpu: 500m
          memory: 60Mi
        requests:
          cpu: 50m
          memory: 60Mi
      securityContext:
        privileged: false
        runAsGroup: 167
        runAsUser: 167
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /etc/ceph/crash-collector-keyring-store/
        name: rook-ceph-crash-collector-keyring
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6rmcl
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6rmcl
        readOnly: true
    - args:
      - /var/lib/ceph/crash/posted
      command:
      - mkdir
      - -p
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: make-container-crash-dir
      resources:
        limits:
          cpu: 500m
          memory: 60Mi
        requests:
          cpu: 50m
          memory: 60Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6rmcl
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: 500m
          memory: 60Mi
        requests:
          cpu: 50m
          memory: 60Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6rmcl
        readOnly: true
    nodeName: k8s-node-orange
    nodeSelector:
      kubernetes.io/hostname: k8s-node-orange
    preemptionPolicy: PreemptLowerPriority
    priority: 100
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-default
    serviceAccountName: rook-ceph-default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 5
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/storage/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/storage/crash
        type: ""
      name: rook-ceph-crash
    - name: rook-ceph-crash-collector-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-crash-collector-keyring
    - name: kube-api-access-6rmcl
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:59:35Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-09-22T00:07:58Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:59:39Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:59:39Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-09-22T00:07:54Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://69a472e8e939a98d5666a6bdf2609828f86104d11e06f5a8a7f31ac18385d948
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState:
        terminated:
          containerID: containerd://b4d611aed3c28ea32264d54cc2bb7ed1b49fb177df237c37b685115aad4dc84e
          exitCode: 255
          finishedAt: "2024-09-23T15:59:18Z"
          reason: Unknown
          startedAt: "2024-09-23T15:47:54Z"
      name: ceph-crash
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:59:39Z"
    hostIP: 10.0.90.12
    hostIPs:
    - ip: 10.0.90.12
    initContainerStatuses:
    - containerID: containerd://483f56f6f19bfa18ee90ec4fe70b0c7ba992a4f12869d615aaa2eb39d36d4e07
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 2
      started: false
      state:
        terminated:
          containerID: containerd://483f56f6f19bfa18ee90ec4fe70b0c7ba992a4f12869d615aaa2eb39d36d4e07
          exitCode: 0
          finishedAt: "2024-09-23T15:59:34Z"
          reason: Completed
          startedAt: "2024-09-23T15:59:34Z"
    - containerID: containerd://1f1954866b5262ab34ce4e058eecfb72ea52a86bf4602d64434258042792b4eb
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: make-container-crash-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://1f1954866b5262ab34ce4e058eecfb72ea52a86bf4602d64434258042792b4eb
          exitCode: 0
          finishedAt: "2024-09-23T15:59:37Z"
          reason: Completed
          startedAt: "2024-09-23T15:59:37Z"
    - containerID: containerd://3eb5d36c4005085e5e7a07105fbc829f483dd655af399ff4577edc18a5716884
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://3eb5d36c4005085e5e7a07105fbc829f483dd655af399ff4577edc18a5716884
          exitCode: 0
          finishedAt: "2024-09-23T15:59:38Z"
          reason: Completed
          startedAt: "2024-09-23T15:59:38Z"
    phase: Running
    podIP: 10.42.186.158
    podIPs:
    - ip: 10.42.186.158
    qosClass: Burstable
    startTime: "2024-09-22T00:07:54Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: ecd5cb14113e5822cddcaefbb77c93940051693ed5ed7e6c714ad6dcb95958d8
      cni.projectcalico.org/podIP: 10.42.205.122/32
      cni.projectcalico.org/podIPs: 10.42.205.122/32
      prometheus.io/port: "9102"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-09-23T15:32:45Z"
    generateName: rook-ceph-crashcollector-k8s-node-yellow-5dcdd75cc9-
    labels:
      app: rook-ceph-crashcollector
      ceph-version: 18.2.4-0
      ceph_daemon_id: crash
      crashcollector: crash
      kubernetes.io/hostname: k8s-node-yellow
      node_name: k8s-node-yellow
      pod-template-hash: 5dcdd75cc9
      rook_cluster: storage
    name: rook-ceph-crashcollector-k8s-node-yellow-5dcdd75cc9-mbwpz
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-crashcollector-k8s-node-yellow-5dcdd75cc9
      uid: fda58a78-4deb-46b4-b1fb-06039fa2ab8a
    resourceVersion: "1330807877"
    uid: a2530974-b159-405b-ab40-fc68f50f4f43
  spec:
    containers:
    - command:
      - ceph-crash
      env:
      - name: TZ
        value: Australia/Sydney
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.4
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST) -k /etc/ceph/crash-collector-keyring-store/keyring
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: ceph-crash
      resources:
        limits:
          cpu: 500m
          memory: 60Mi
        requests:
          cpu: 50m
          memory: 60Mi
      securityContext:
        privileged: false
        runAsGroup: 167
        runAsUser: 167
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /etc/ceph/crash-collector-keyring-store/
        name: rook-ceph-crash-collector-keyring
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cd77t
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cd77t
        readOnly: true
    - args:
      - /var/lib/ceph/crash/posted
      command:
      - mkdir
      - -p
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: make-container-crash-dir
      resources:
        limits:
          cpu: 500m
          memory: 60Mi
        requests:
          cpu: 50m
          memory: 60Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cd77t
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: 500m
          memory: 60Mi
        requests:
          cpu: 50m
          memory: 60Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cd77t
        readOnly: true
    nodeName: k8s-node-yellow
    nodeSelector:
      kubernetes.io/hostname: k8s-node-yellow
    preemptionPolicy: PreemptLowerPriority
    priority: 100
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-default
    serviceAccountName: rook-ceph-default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/storage/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/storage/crash
        type: ""
      name: rook-ceph-crash
    - name: rook-ceph-crash-collector-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-crash-collector-keyring
    - name: kube-api-access-cd77t
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:32:46Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:32:48Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:32:49Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:32:49Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:32:45Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://e5a961fb3f731b3812141953df7c7efa504666eed9921267d1351ebe7364063d
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: ceph-crash
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:32:49Z"
    hostIP: 10.0.90.14
    hostIPs:
    - ip: 10.0.90.14
    initContainerStatuses:
    - containerID: containerd://2d33961f20ef27f8286f34633c970c5ee6ca4ba683714ebc568fa7a29013b359
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://2d33961f20ef27f8286f34633c970c5ee6ca4ba683714ebc568fa7a29013b359
          exitCode: 0
          finishedAt: "2024-09-23T15:32:46Z"
          reason: Completed
          startedAt: "2024-09-23T15:32:46Z"
    - containerID: containerd://49463472bc3a57f3febb84847de594635b4e3789660fb97549e7758260e2ce27
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: make-container-crash-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://49463472bc3a57f3febb84847de594635b4e3789660fb97549e7758260e2ce27
          exitCode: 0
          finishedAt: "2024-09-23T15:32:47Z"
          reason: Completed
          startedAt: "2024-09-23T15:32:47Z"
    - containerID: containerd://70d5618aa64bbf78b56636a520a1eef64996c4f22946e47ee5ae983690c4e793
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://70d5618aa64bbf78b56636a520a1eef64996c4f22946e47ee5ae983690c4e793
          exitCode: 0
          finishedAt: "2024-09-23T15:32:48Z"
          reason: Completed
          startedAt: "2024-09-23T15:32:48Z"
    phase: Running
    podIP: 10.42.205.122
    podIPs:
    - ip: 10.42.205.122
    qosClass: Burstable
    startTime: "2024-09-23T15:32:45Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: b63b3159f888e1d3134a3ff05e7d75ccd10fabc562aa260dbeb615607de3be7c
      cni.projectcalico.org/podIP: 10.42.3.17/32
      cni.projectcalico.org/podIPs: 10.42.3.17/32
      prometheus.io/port: "9102"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-10-03T22:29:50Z"
    generateName: rook-ceph-exporter-k8s-node-blue-6ff5cdd86b-
    labels:
      app: rook-ceph-exporter
      ceph-version: 18.2.4-0
      ceph_daemon_id: exporter
      kubernetes.io/hostname: k8s-node-blue
      node_name: k8s-node-blue
      pod-template-hash: 6ff5cdd86b
      rook-version: v1.15.3
      rook_cluster: storage
    name: rook-ceph-exporter-k8s-node-blue-6ff5cdd86b-gg4px
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-exporter-k8s-node-blue-6ff5cdd86b
      uid: e383a22c-5781-4fea-ad1b-ef4b2c5c880a
    resourceVersion: "1341799102"
    uid: 01f6f9a2-8fcc-4c5d-9fb0-53cfd2197f2e
  spec:
    containers:
    - args:
      - --sock-dir
      - /run/ceph
      - --port
      - "9926"
      - --prio-limit
      - "5"
      - --stats-period
      - "5"
      command:
      - ceph-exporter
      env:
      - name: TZ
        value: Australia/Sydney
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.4
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST) -n client.ceph-exporter -k /etc/ceph/exporter-keyring-store/keyring
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: ceph-exporter
      resources:
        limits:
          memory: 128Mi
        requests:
          cpu: 50m
          memory: 50Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /etc/ceph/exporter-keyring-store/
        name: rook-ceph-exporter-keyring
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-z5pk4
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-z5pk4
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          memory: 128Mi
        requests:
          cpu: 50m
          memory: 50Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-z5pk4
        readOnly: true
    nodeName: k8s-node-blue
    nodeSelector:
      kubernetes.io/hostname: k8s-node-blue
    preemptionPolicy: PreemptLowerPriority
    priority: 100
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-default
    serviceAccountName: rook-ceph-default
    terminationGracePeriodSeconds: 2
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 5
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/storage/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/storage/crash
        type: ""
      name: rook-ceph-crash
    - name: rook-ceph-exporter-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-exporter-keyring
    - name: kube-api-access-z5pk4
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-10-03T22:29:51Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-10-03T22:29:52Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-10-03T22:29:53Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-10-03T22:29:53Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-10-03T22:29:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://0c4200803ea7929df8bd484456880e45371e5c37e410229ab280058dc9d22ca5
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: ceph-exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-10-03T22:29:52Z"
    hostIP: 10.0.90.10
    hostIPs:
    - ip: 10.0.90.10
    initContainerStatuses:
    - containerID: containerd://b04aee60b804a225e78f54c83076b6b64a124efc44bd5d434521afdfbfff20b7
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://b04aee60b804a225e78f54c83076b6b64a124efc44bd5d434521afdfbfff20b7
          exitCode: 0
          finishedAt: "2024-10-03T22:29:51Z"
          reason: Completed
          startedAt: "2024-10-03T22:29:50Z"
    - containerID: containerd://763ee15e7df2329e48574f1b42f6afec6139897468aa6837ec320768e29e2701
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://763ee15e7df2329e48574f1b42f6afec6139897468aa6837ec320768e29e2701
          exitCode: 0
          finishedAt: "2024-10-03T22:29:51Z"
          reason: Completed
          startedAt: "2024-10-03T22:29:51Z"
    phase: Running
    podIP: 10.42.3.17
    podIPs:
    - ip: 10.42.3.17
    qosClass: Burstable
    startTime: "2024-10-03T22:29:50Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 5fc30b588e47b546de0148cdf6ea9776dc066b5ea597ec4d034da539d655556f
      cni.projectcalico.org/podIP: 10.42.186.130/32
      cni.projectcalico.org/podIPs: 10.42.186.130/32
      prometheus.io/port: "9102"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-10-03T22:29:53Z"
    generateName: rook-ceph-exporter-k8s-node-orange-759b94f68d-
    labels:
      app: rook-ceph-exporter
      ceph-version: 18.2.4-0
      ceph_daemon_id: exporter
      kubernetes.io/hostname: k8s-node-orange
      node_name: k8s-node-orange
      pod-template-hash: 759b94f68d
      rook-version: v1.15.3
      rook_cluster: storage
    name: rook-ceph-exporter-k8s-node-orange-759b94f68d-9lj6n
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-exporter-k8s-node-orange-759b94f68d
      uid: f3a68b37-c044-4234-a06a-7e2b9dd028fe
    resourceVersion: "1341799308"
    uid: dadc7bb4-f108-4b43-a631-7aa61af22373
  spec:
    containers:
    - args:
      - --sock-dir
      - /run/ceph
      - --port
      - "9926"
      - --prio-limit
      - "5"
      - --stats-period
      - "5"
      command:
      - ceph-exporter
      env:
      - name: TZ
        value: Australia/Sydney
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.4
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST) -n client.ceph-exporter -k /etc/ceph/exporter-keyring-store/keyring
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: ceph-exporter
      resources:
        limits:
          memory: 128Mi
        requests:
          cpu: 50m
          memory: 50Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /etc/ceph/exporter-keyring-store/
        name: rook-ceph-exporter-keyring
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-k6f66
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-k6f66
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          memory: 128Mi
        requests:
          cpu: 50m
          memory: 50Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-k6f66
        readOnly: true
    nodeName: k8s-node-orange
    nodeSelector:
      kubernetes.io/hostname: k8s-node-orange
    preemptionPolicy: PreemptLowerPriority
    priority: 100
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-default
    serviceAccountName: rook-ceph-default
    terminationGracePeriodSeconds: 2
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 5
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/storage/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/storage/crash
        type: ""
      name: rook-ceph-crash
    - name: rook-ceph-exporter-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-exporter-keyring
    - name: kube-api-access-k6f66
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-10-03T22:29:55Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-10-03T22:29:56Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-10-03T22:29:57Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-10-03T22:29:57Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-10-03T22:29:53Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://fd8f03746aa3c3f77d47e5ac90b5676c0281819d1c0a390089eae63e414507da
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: ceph-exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-10-03T22:29:56Z"
    hostIP: 10.0.90.12
    hostIPs:
    - ip: 10.0.90.12
    initContainerStatuses:
    - containerID: containerd://006bd9b9be84a9f1d8f14b7ce8a0ec369b1f950e372b4fe8e0b2ea7c65cf8423
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://006bd9b9be84a9f1d8f14b7ce8a0ec369b1f950e372b4fe8e0b2ea7c65cf8423
          exitCode: 0
          finishedAt: "2024-10-03T22:29:54Z"
          reason: Completed
          startedAt: "2024-10-03T22:29:54Z"
    - containerID: containerd://41c08d5817a16e8ecc0a2610685b1798c70cc3a406b6282febbb3e8be370415d
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://41c08d5817a16e8ecc0a2610685b1798c70cc3a406b6282febbb3e8be370415d
          exitCode: 0
          finishedAt: "2024-10-03T22:29:55Z"
          reason: Completed
          startedAt: "2024-10-03T22:29:55Z"
    phase: Running
    podIP: 10.42.186.130
    podIPs:
    - ip: 10.42.186.130
    qosClass: Burstable
    startTime: "2024-10-03T22:29:53Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 6ac546fecb9e7a51b1939cbf2bffe77797f9270387b6465056774e0a8026487f
      cni.projectcalico.org/podIP: 10.42.205.102/32
      cni.projectcalico.org/podIPs: 10.42.205.102/32
      prometheus.io/port: "9102"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-10-03T22:29:55Z"
    generateName: rook-ceph-exporter-k8s-node-yellow-56f89b8c6d-
    labels:
      app: rook-ceph-exporter
      ceph-version: 18.2.4-0
      ceph_daemon_id: exporter
      kubernetes.io/hostname: k8s-node-yellow
      node_name: k8s-node-yellow
      pod-template-hash: 56f89b8c6d
      rook-version: v1.15.3
      rook_cluster: storage
    name: rook-ceph-exporter-k8s-node-yellow-56f89b8c6d-pw9hh
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-exporter-k8s-node-yellow-56f89b8c6d
      uid: 1b17d38c-5d88-4834-8db4-4907d358480b
    resourceVersion: "1341799357"
    uid: 1af53bf3-5865-4685-a2ae-ed7653fc4f55
  spec:
    containers:
    - args:
      - --sock-dir
      - /run/ceph
      - --port
      - "9926"
      - --prio-limit
      - "5"
      - --stats-period
      - "5"
      command:
      - ceph-exporter
      env:
      - name: TZ
        value: Australia/Sydney
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.4
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST) -n client.ceph-exporter -k /etc/ceph/exporter-keyring-store/keyring
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: ceph-exporter
      resources:
        limits:
          memory: 128Mi
        requests:
          cpu: 50m
          memory: 50Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /etc/ceph/exporter-keyring-store/
        name: rook-ceph-exporter-keyring
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vgvdp
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vgvdp
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          memory: 128Mi
        requests:
          cpu: 50m
          memory: 50Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vgvdp
        readOnly: true
    nodeName: k8s-node-yellow
    nodeSelector:
      kubernetes.io/hostname: k8s-node-yellow
    preemptionPolicy: PreemptLowerPriority
    priority: 100
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-default
    serviceAccountName: rook-ceph-default
    terminationGracePeriodSeconds: 2
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/storage/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/storage/crash
        type: ""
      name: rook-ceph-crash
    - name: rook-ceph-exporter-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-exporter-keyring
    - name: kube-api-access-vgvdp
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-10-03T22:29:56Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-10-03T22:29:57Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-10-03T22:29:58Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-10-03T22:29:58Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-10-03T22:29:55Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://afd42eb270704ec27ce87ce8e49d175abaaf59a3f95390349625a84c7874ca50
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: ceph-exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-10-03T22:29:57Z"
    hostIP: 10.0.90.14
    hostIPs:
    - ip: 10.0.90.14
    initContainerStatuses:
    - containerID: containerd://485701c14054aaa6e2de61015ac49b0fdda829dcf0efa28dcc90f5ecd3668fe6
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://485701c14054aaa6e2de61015ac49b0fdda829dcf0efa28dcc90f5ecd3668fe6
          exitCode: 0
          finishedAt: "2024-10-03T22:29:56Z"
          reason: Completed
          startedAt: "2024-10-03T22:29:55Z"
    - containerID: containerd://b873a1355e4f1c960214e2e360e231da61a84f44cc0aeb2f28d3b1ee7a99d196
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://b873a1355e4f1c960214e2e360e231da61a84f44cc0aeb2f28d3b1ee7a99d196
          exitCode: 0
          finishedAt: "2024-10-03T22:29:56Z"
          reason: Completed
          startedAt: "2024-10-03T22:29:56Z"
    phase: Running
    podIP: 10.42.205.102
    podIPs:
    - ip: 10.42.205.102
    qosClass: Burstable
    startTime: "2024-10-03T22:29:55Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: fb8af9ea3d0119addb16341bb71dac9a94ebf3c0e6ea63d2ecfba12e4b84e422
      cni.projectcalico.org/podIP: 10.42.186.180/32
      cni.projectcalico.org/podIPs: 10.42.186.180/32
    creationTimestamp: "2024-05-23T15:53:36Z"
    generateName: rook-ceph-mds-ceph-filesystem-b-567df46d7-
    labels:
      app: rook-ceph-mds
      app.kubernetes.io/component: cephfilesystems.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: ceph-filesystem-b
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mds
      app.kubernetes.io/part-of: ceph-filesystem
      ceph_daemon_id: ceph-filesystem-b
      ceph_daemon_type: mds
      mds: ceph-filesystem-b
      pod-template-hash: 567df46d7
      rook.io/operator-namespace: storage
      rook_cluster: storage
      rook_file_system: ceph-filesystem
    name: rook-ceph-mds-ceph-filesystem-b-567df46d7-8n4ps
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-mds-ceph-filesystem-b-567df46d7
      uid: f6d72e58-4dca-4aa5-8f11-478f59ad0459
    resourceVersion: "1330833906"
    uid: 0193c9b4-d3f5-4969-92cf-eae426c2d8d2
  spec:
    affinity: {}
    containers:
    - args:
      - --fsid=9dff65cc-5b8f-413f-a1ed-8bfb6b93684a
      - --keyring=/etc/ceph/keyring-store/keyring
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=ceph-filesystem-b
      - --setuser=ceph
      - --setgroup=ceph
      - --foreground
      - --public-addr=$(ROOK_POD_IP)
      command:
      - ceph-mds
      env:
      - name: TZ
        value: Australia/Sydney
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.2
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - timeout
          - "20"
          - sh
          - -c
          - |
            #!/usr/bin/env bash
            # do not use 'set -e -u' etc. because it is important to only fail this probe when failure is certain
            # spurious failures risk destabilizing ceph or the filesystem

            MDS_ID="ceph-filesystem-b"
            FILESYSTEM_NAME="ceph-filesystem"
            KEYRING="/etc/ceph/keyring-store/keyring"

            outp="$(ceph fs dump --mon-host="$ROOK_CEPH_MON_HOST" --mon-initial-members="$ROOK_CEPH_MON_INITIAL_MEMBERS" --keyring "$KEYRING" --format json)"
            rc=$?
            if [ $rc -ne 0 ]; then
                echo "ceph MDS dump check failed with the following output:"
                echo "$outp"
                echo "passing probe to avoid restarting MDS. cannot determine if MDS is unhealthy. restarting MDS risks destabilizing ceph/filesystem, which is likely unreachable or in error state"
                exit 0
            fi

            # get the active and standby MDS in the fs map
            standbyMds=$(echo "$outp" | jq ".standbys | map(.name) | any(.[]; . == \"$MDS_ID\")")
            activeMds=$(echo "$outp" | jq ".filesystems[] | select(.mdsmap.fs_name == \"$FILESYSTEM_NAME\") | .mdsmap.info | map(.name) | any(.[]; . == \"$MDS_ID\")")

            if [[ $standbyMds == true || $activeMds == true ]]; then
                echo "MDS ID present in MDS map, no need to re-start the container"
                exit 0
            fi

            echo "Error: MDS ID not present in MDS map"
            exit 1
        failureThreshold: 5
        initialDelaySeconds: 30
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 25
      name: mds
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mds-ceph-filesystem-b-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mds/ceph-ceph-filesystem-b
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-97ftd
        readOnly: true
      workingDir: /var/log/ceph
    - command:
      - /bin/bash
      - -x
      - -e
      - -m
      - -c
      - "\nCEPH_CLIENT_ID=ceph-mds.ceph-filesystem-b\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
        edit the logrotate file to only rotate a specific daemon log\n# otherwise
        we will logrotate log files without reloading certain daemons\n# this might
        happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
        \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
        --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
        rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed --in-place
        \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif [ \"$LOG_MAX_SIZE\"
        != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE at the 4th line of the
        logrotate config file with 4 spaces to maintain indentation\n\tsed --in-place
        \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\" \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile
        true; do\n\t# we don't force the logrorate but we let the logrotate binary
        handle the rotation based on user's input for periodicity and size\n\tlogrotate
        --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep 15m\ndone\n"
      env:
      - name: TZ
        value: Australia/Sydney
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      name: log-collector
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-97ftd
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-97ftd
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      - /var/lib/ceph/mds/ceph-ceph-filesystem-b
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mds-ceph-filesystem-b-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mds/ceph-ceph-filesystem-b
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-97ftd
        readOnly: true
    nodeName: k8s-node-orange
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-default
    serviceAccountName: rook-ceph-default
    shareProcessNamespace: true
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 5
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - name: rook-ceph-mds-ceph-filesystem-b-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-mds-ceph-filesystem-b-keyring
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/storage/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/storage/crash
        type: ""
      name: rook-ceph-crash
    - emptyDir: {}
      name: ceph-daemon-data
    - name: kube-api-access-97ftd
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:59:35Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-05-23T15:53:38Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:59:39Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:59:39Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-05-23T15:53:36Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a30030d062bb483234b492a1db9081d8b1ec9f650031b0347fd802cc886a0110
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:06ddc3ef5b66f2dcc6d16e41842d33a3d7f497849981b0842672ef9014a96726
      lastState:
        terminated:
          containerID: containerd://374d37ffa5eadd64a50d32820e93977cceb9df1e332a0c2109b7d0a6571ae501
          exitCode: 255
          finishedAt: "2024-09-23T15:59:18Z"
          reason: Unknown
          startedAt: "2024-09-23T15:47:55Z"
      name: log-collector
      ready: true
      restartCount: 5
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:59:39Z"
    - containerID: containerd://482db757c9344fefca2e1220516591a88a818abb8d80a6dcb0d20da2fe4f5613
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:06ddc3ef5b66f2dcc6d16e41842d33a3d7f497849981b0842672ef9014a96726
      lastState:
        terminated:
          containerID: containerd://5f2ba2a0044f8955f9951b9ee0b9257e0a4dc76ad57f01933f1538dd5adc870c
          exitCode: 255
          finishedAt: "2024-09-23T15:59:18Z"
          reason: Unknown
          startedAt: "2024-09-23T15:47:54Z"
      name: mds
      ready: true
      restartCount: 55
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:59:38Z"
    hostIP: 10.0.90.12
    hostIPs:
    - ip: 10.0.90.12
    initContainerStatuses:
    - containerID: containerd://fa69fe79498455f24cf633bdeb447c86c8b6afe587ec08028e4aee668dcba389
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 5
      started: false
      state:
        terminated:
          containerID: containerd://fa69fe79498455f24cf633bdeb447c86c8b6afe587ec08028e4aee668dcba389
          exitCode: 0
          finishedAt: "2024-09-23T15:59:34Z"
          reason: Completed
          startedAt: "2024-09-23T15:59:34Z"
    - containerID: containerd://398d1beb8ebba0bee8ece44972f3a66465c0d10c0614b68c217dfc0531097fcb
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:06ddc3ef5b66f2dcc6d16e41842d33a3d7f497849981b0842672ef9014a96726
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://398d1beb8ebba0bee8ece44972f3a66465c0d10c0614b68c217dfc0531097fcb
          exitCode: 0
          finishedAt: "2024-09-23T15:59:37Z"
          reason: Completed
          startedAt: "2024-09-23T15:59:37Z"
    phase: Running
    podIP: 10.42.186.180
    podIPs:
    - ip: 10.42.186.180
    qosClass: Burstable
    startTime: "2024-05-23T15:53:36Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 058aa3d0bdd0bc2d983381ebdab1efd4436ac03088c394fba50fc28767836242
      cni.projectcalico.org/podIP: 10.42.186.138/32
      cni.projectcalico.org/podIPs: 10.42.186.138/32
    creationTimestamp: "2024-10-14T12:13:04Z"
    generateName: rook-ceph-mds-ceph-filesystem-hdd-a-7bd5c89ccb-
    labels:
      app: rook-ceph-mds
      app.kubernetes.io/component: cephfilesystems.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: ceph-filesystem-hdd-a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mds
      app.kubernetes.io/part-of: ceph-filesystem-hdd
      ceph_daemon_id: ceph-filesystem-hdd-a
      ceph_daemon_type: mds
      mds: ceph-filesystem-hdd-a
      pod-template-hash: 7bd5c89ccb
      rook.io/operator-namespace: storage
      rook_cluster: storage
      rook_file_system: ceph-filesystem-hdd
    name: rook-ceph-mds-ceph-filesystem-hdd-a-7bd5c89ccb-94htf
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-mds-ceph-filesystem-hdd-a-7bd5c89ccb
      uid: 062d091c-0eae-4367-9f96-8d8d27485f3c
    resourceVersion: "1352841462"
    uid: fd12bc2c-18a6-4e2e-9c6d-95bf01c0ec11
  spec:
    affinity: {}
    containers:
    - args:
      - --fsid=9dff65cc-5b8f-413f-a1ed-8bfb6b93684a
      - --keyring=/etc/ceph/keyring-store/keyring
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=ceph-filesystem-hdd-a
      - --setuser=ceph
      - --setgroup=ceph
      - --foreground
      - --public-addr=$(ROOK_POD_IP)
      command:
      - ceph-mds
      env:
      - name: TZ
        value: Australia/Sydney
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.4
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - timeout
          - "20"
          - sh
          - -c
          - |
            #!/usr/bin/env bash
            # do not use 'set -e -u' etc. because it is important to only fail this probe when failure is certain
            # spurious failures risk destabilizing ceph or the filesystem

            MDS_ID="ceph-filesystem-hdd-a"
            FILESYSTEM_NAME="ceph-filesystem-hdd"
            KEYRING="/etc/ceph/keyring-store/keyring"

            outp="$(ceph fs dump --mon-host="$ROOK_CEPH_MON_HOST" --mon-initial-members="$ROOK_CEPH_MON_INITIAL_MEMBERS" --keyring "$KEYRING" --format json)"
            rc=$?
            if [ $rc -ne 0 ]; then
                echo "ceph MDS dump check failed with the following output:"
                echo "$outp"
                echo "passing probe to avoid restarting MDS. cannot determine if MDS is unhealthy. restarting MDS risks destabilizing ceph/filesystem, which is likely unreachable or in error state"
                exit 0
            fi

            # get the active and standby MDS in the fs map
            standbyMds=$(echo "$outp" | jq ".standbys | map(.name) | any(.[]; . == \"$MDS_ID\")")
            activeMds=$(echo "$outp" | jq ".filesystems[] | select(.mdsmap.fs_name == \"$FILESYSTEM_NAME\") | .mdsmap.info | map(.name) | any(.[]; . == \"$MDS_ID\")")

            if [[ $standbyMds == true || $activeMds == true ]]; then
                echo "MDS ID present in MDS map, no need to re-start the container"
                exit 0
            fi

            echo "Error: MDS ID not present in MDS map"
            exit 1
        failureThreshold: 5
        initialDelaySeconds: 30
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 25
      name: mds
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mds-ceph-filesystem-hdd-a-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mds/ceph-ceph-filesystem-hdd-a
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pbhcx
        readOnly: true
      workingDir: /var/log/ceph
    - command:
      - /bin/bash
      - -x
      - -e
      - -m
      - -c
      - "\nCEPH_CLIENT_ID=ceph-mds.ceph-filesystem-hdd-a\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
        edit the logrotate file to only rotate a specific daemon log\n# otherwise
        we will logrotate log files without reloading certain daemons\n# this might
        happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
        \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
        --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
        rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed --in-place
        \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif [ \"$LOG_MAX_SIZE\"
        != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE at the 4th line of the
        logrotate config file with 4 spaces to maintain indentation\n\tsed --in-place
        \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\" \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile
        true; do\n\t# we don't force the logrorate but we let the logrotate binary
        handle the rotation based on user's input for periodicity and size\n\tlogrotate
        --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep 15m\ndone\n"
      env:
      - name: TZ
        value: Australia/Sydney
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: log-collector
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pbhcx
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pbhcx
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      - /var/lib/ceph/mds/ceph-ceph-filesystem-hdd-a
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mds-ceph-filesystem-hdd-a-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mds/ceph-ceph-filesystem-hdd-a
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pbhcx
        readOnly: true
    nodeName: k8s-node-orange
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-default
    serviceAccountName: rook-ceph-default
    shareProcessNamespace: true
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 5
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - name: rook-ceph-mds-ceph-filesystem-hdd-a-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-mds-ceph-filesystem-hdd-a-keyring
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/storage/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/storage/crash
        type: ""
      name: rook-ceph-crash
    - emptyDir: {}
      name: ceph-daemon-data
    - name: kube-api-access-pbhcx
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-10-14T12:13:05Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-10-14T12:13:06Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-10-14T12:13:07Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-10-14T12:13:07Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-10-14T12:13:04Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://4fdf232105793274eb073cb0a61cfdaf250088421b55553163fb7d3aa389884b
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: log-collector
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-10-14T12:13:06Z"
    - containerID: containerd://0ecf32baa201dc20700573cfb54f5ca45836cdaf0d40213a24b132283f510f8b
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: mds
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-10-14T12:13:06Z"
    hostIP: 10.0.90.12
    hostIPs:
    - ip: 10.0.90.12
    initContainerStatuses:
    - containerID: containerd://f56488fb876460590fe93415d153a18bcf3390474a0874f415da2221929b6d31
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://f56488fb876460590fe93415d153a18bcf3390474a0874f415da2221929b6d31
          exitCode: 0
          finishedAt: "2024-10-14T12:13:05Z"
          reason: Completed
          startedAt: "2024-10-14T12:13:05Z"
    - containerID: containerd://4204ebd2224bd91f512cb71841c9bfa48dd9a1244d707625ada40bf5a1ea5842
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://4204ebd2224bd91f512cb71841c9bfa48dd9a1244d707625ada40bf5a1ea5842
          exitCode: 0
          finishedAt: "2024-10-14T12:13:05Z"
          reason: Completed
          startedAt: "2024-10-14T12:13:05Z"
    phase: Running
    podIP: 10.42.186.138
    podIPs:
    - ip: 10.42.186.138
    qosClass: Burstable
    startTime: "2024-10-14T12:13:04Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: efb902a3699a851f401c5e99ed305db5f2e8d2b6d458ba4487836e093831fa1d
      cni.projectcalico.org/podIP: 10.42.186.144/32
      cni.projectcalico.org/podIPs: 10.42.186.144/32
    creationTimestamp: "2024-10-03T10:10:27Z"
    generateName: rook-ceph-mds-ceph-filesystem-hdd-b-7bb4c5fd46-
    labels:
      app: rook-ceph-mds
      app.kubernetes.io/component: cephfilesystems.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: ceph-filesystem-hdd-b
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mds
      app.kubernetes.io/part-of: ceph-filesystem-hdd
      ceph_daemon_id: ceph-filesystem-hdd-b
      ceph_daemon_type: mds
      mds: ceph-filesystem-hdd-b
      pod-template-hash: 7bb4c5fd46
      rook.io/operator-namespace: storage
      rook_cluster: storage
      rook_file_system: ceph-filesystem-hdd
    name: rook-ceph-mds-ceph-filesystem-hdd-b-7bb4c5fd46-9wg9q
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-mds-ceph-filesystem-hdd-b-7bb4c5fd46
      uid: be8d0942-53ed-4c08-9887-0cc0e2dcb9ea
    resourceVersion: "1341264695"
    uid: a21da99f-4eba-4722-ac04-c4ae0bd877a8
  spec:
    affinity: {}
    containers:
    - args:
      - --fsid=9dff65cc-5b8f-413f-a1ed-8bfb6b93684a
      - --keyring=/etc/ceph/keyring-store/keyring
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=ceph-filesystem-hdd-b
      - --setuser=ceph
      - --setgroup=ceph
      - --foreground
      - --public-addr=$(ROOK_POD_IP)
      command:
      - ceph-mds
      env:
      - name: TZ
        value: Australia/Sydney
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.4
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - timeout
          - "20"
          - sh
          - -c
          - |
            #!/usr/bin/env bash
            # do not use 'set -e -u' etc. because it is important to only fail this probe when failure is certain
            # spurious failures risk destabilizing ceph or the filesystem

            MDS_ID="ceph-filesystem-hdd-b"
            FILESYSTEM_NAME="ceph-filesystem-hdd"
            KEYRING="/etc/ceph/keyring-store/keyring"

            outp="$(ceph fs dump --mon-host="$ROOK_CEPH_MON_HOST" --mon-initial-members="$ROOK_CEPH_MON_INITIAL_MEMBERS" --keyring "$KEYRING" --format json)"
            rc=$?
            if [ $rc -ne 0 ]; then
                echo "ceph MDS dump check failed with the following output:"
                echo "$outp"
                echo "passing probe to avoid restarting MDS. cannot determine if MDS is unhealthy. restarting MDS risks destabilizing ceph/filesystem, which is likely unreachable or in error state"
                exit 0
            fi

            # get the active and standby MDS in the fs map
            standbyMds=$(echo "$outp" | jq ".standbys | map(.name) | any(.[]; . == \"$MDS_ID\")")
            activeMds=$(echo "$outp" | jq ".filesystems[] | select(.mdsmap.fs_name == \"$FILESYSTEM_NAME\") | .mdsmap.info | map(.name) | any(.[]; . == \"$MDS_ID\")")

            if [[ $standbyMds == true || $activeMds == true ]]; then
                echo "MDS ID present in MDS map, no need to re-start the container"
                exit 0
            fi

            echo "Error: MDS ID not present in MDS map"
            exit 1
        failureThreshold: 5
        initialDelaySeconds: 30
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 25
      name: mds
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mds-ceph-filesystem-hdd-b-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mds/ceph-ceph-filesystem-hdd-b
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pmg9k
        readOnly: true
      workingDir: /var/log/ceph
    - command:
      - /bin/bash
      - -x
      - -e
      - -m
      - -c
      - "\nCEPH_CLIENT_ID=ceph-mds.ceph-filesystem-hdd-b\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
        edit the logrotate file to only rotate a specific daemon log\n# otherwise
        we will logrotate log files without reloading certain daemons\n# this might
        happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
        \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
        --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
        rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed --in-place
        \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif [ \"$LOG_MAX_SIZE\"
        != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE at the 4th line of the
        logrotate config file with 4 spaces to maintain indentation\n\tsed --in-place
        \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\" \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile
        true; do\n\t# we don't force the logrorate but we let the logrotate binary
        handle the rotation based on user's input for periodicity and size\n\tlogrotate
        --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep 15m\ndone\n"
      env:
      - name: TZ
        value: Australia/Sydney
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: log-collector
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pmg9k
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pmg9k
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      - /var/lib/ceph/mds/ceph-ceph-filesystem-hdd-b
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mds-ceph-filesystem-hdd-b-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mds/ceph-ceph-filesystem-hdd-b
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pmg9k
        readOnly: true
    nodeName: k8s-node-orange
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-default
    serviceAccountName: rook-ceph-default
    shareProcessNamespace: true
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 5
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - name: rook-ceph-mds-ceph-filesystem-hdd-b-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-mds-ceph-filesystem-hdd-b-keyring
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/storage/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/storage/crash
        type: ""
      name: rook-ceph-crash
    - emptyDir: {}
      name: ceph-daemon-data
    - name: kube-api-access-pmg9k
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-10-03T10:10:29Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-10-03T10:10:30Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-10-03T10:10:31Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-10-03T10:10:31Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-10-03T10:10:27Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://e4ca56bf7e83fcfb67df389a218b034797171e423278f9d5993bfcc8e7e617ff
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: log-collector
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-10-03T10:10:30Z"
    - containerID: containerd://3be6d3d6c981ec4dda1e7044a29dcd7ce0daabf3c6ad5cd11587dcedb690a31e
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: mds
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-10-03T10:10:30Z"
    hostIP: 10.0.90.12
    hostIPs:
    - ip: 10.0.90.12
    initContainerStatuses:
    - containerID: containerd://f60bfcb8ffb1da9f6d0aa585b3e3ae76cad1d141b85c892d91c44c33bedddce0
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://f60bfcb8ffb1da9f6d0aa585b3e3ae76cad1d141b85c892d91c44c33bedddce0
          exitCode: 0
          finishedAt: "2024-10-03T10:10:28Z"
          reason: Completed
          startedAt: "2024-10-03T10:10:28Z"
    - containerID: containerd://dd41fcbfb1c46a3cd3abc5ca47821fb870f5e8b73962df3d196e1a277b3a1aa2
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://dd41fcbfb1c46a3cd3abc5ca47821fb870f5e8b73962df3d196e1a277b3a1aa2
          exitCode: 0
          finishedAt: "2024-10-03T10:10:29Z"
          reason: Completed
          startedAt: "2024-10-03T10:10:29Z"
    phase: Running
    podIP: 10.42.186.144
    podIPs:
    - ip: 10.42.186.144
    qosClass: Burstable
    startTime: "2024-10-03T10:10:27Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 18012fe6e42d399f077a8f09d8703189cfb0728b12bd0b4be8810c33d200ba4f
      cni.projectcalico.org/podIP: 10.42.205.90/32
      cni.projectcalico.org/podIPs: 10.42.205.90/32
      prometheus.io/port: "9102"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-10-03T22:30:19Z"
    generateName: rook-ceph-mgr-a-74dc494dbc-
    labels:
      app: rook-ceph-mgr
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mgr
      app.kubernetes.io/part-of: storage
      ceph_daemon_id: a
      ceph_daemon_type: mgr
      instance: a
      mgr: a
      mgr_role: active
      pod-template-hash: 74dc494dbc
      rook.io/operator-namespace: storage
      rook_cluster: storage
    name: rook-ceph-mgr-a-74dc494dbc-5rcwz
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-mgr-a-74dc494dbc
      uid: 028cba0b-0ccc-495f-a671-d6a0fb49ecc2
    resourceVersion: "1341800575"
    uid: 2b1bf22d-28df-4f88-91ff-772a307f8742
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app: rook-ceph-mgr
              rook_cluster: storage
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --fsid=9dff65cc-5b8f-413f-a1ed-8bfb6b93684a
      - --keyring=/etc/ceph/keyring-store/keyring
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=a
      - --setuser=ceph
      - --setgroup=ceph
      - --client-mount-uid=0
      - --client-mount-gid=0
      - --foreground
      - --public-addr=$(ROOK_POD_IP)
      command:
      - ceph-mgr
      env:
      - name: TZ
        value: Australia/Sydney
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.4
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: ROOK_OPERATOR_NAMESPACE
        value: storage
      - name: ROOK_CEPH_CLUSTER_CRD_VERSION
        value: v1
      - name: ROOK_CEPH_CLUSTER_CRD_NAME
        value: storage
      - name: CEPH_ARGS
        value: --mon-host $(ROOK_CEPH_MON_HOST) --keyring /etc/ceph/keyring-store/keyring
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.a.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with the
            following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: mgr
      ports:
      - containerPort: 6800
        name: mgr
        protocol: TCP
      - containerPort: 9283
        name: http-metrics
        protocol: TCP
      - containerPort: 8443
        name: dashboard
        protocol: TCP
      resources:
        limits:
          cpu: "1"
          memory: 2Gi
        requests:
          cpu: 250m
          memory: 1Gi
      securityContext:
        privileged: false
      startupProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.a.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with the
            following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
        failureThreshold: 6
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mgr-a-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mgr/ceph-a
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jrhjf
        readOnly: true
      workingDir: /var/log/ceph
    - args:
      - ceph
      - mgr
      - watch-active
      env:
      - name: TZ
        value: Australia/Sydney
      - name: ROOK_CLUSTER_ID
        value: 9f4e59e5-14c0-4d39-ac52-c4fcf6bd6c10
      - name: ROOK_CLUSTER_NAME
        value: storage
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: storage
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_CEPH_USERNAME
        valueFrom:
          secretKeyRef:
            key: ceph-username
            name: rook-ceph-mon
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: ROOK_DASHBOARD_ENABLED
        value: "true"
      - name: ROOK_MONITORING_ENABLED
        value: "true"
      - name: ROOK_UPDATE_INTERVAL
        value: 15s
      - name: ROOK_DAEMON_NAME
        value: a
      - name: ROOK_CEPH_VERSION
        value: ceph version 18.2.4-0 reef
      image: docker.io/rook/ceph:v1.15.3
      imagePullPolicy: IfNotPresent
      name: watch-active
      resources:
        limits:
          cpu: 500m
          memory: 100Mi
        requests:
          cpu: 100m
          memory: 40Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /var/lib/rook
        name: rook-config
      - mountPath: /etc/ceph
        name: default-config-dir
      - mountPath: /var/lib/rook-ceph-mon
        name: ceph-admin-secret
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jrhjf
        readOnly: true
    - command:
      - /bin/bash
      - -x
      - -e
      - -m
      - -c
      - "\nCEPH_CLIENT_ID=ceph-mgr.a\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
        edit the logrotate file to only rotate a specific daemon log\n# otherwise
        we will logrotate log files without reloading certain daemons\n# this might
        happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
        \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
        --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
        rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed --in-place
        \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif [ \"$LOG_MAX_SIZE\"
        != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE at the 4th line of the
        logrotate config file with 4 spaces to maintain indentation\n\tsed --in-place
        \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\" \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile
        true; do\n\t# we don't force the logrorate but we let the logrotate binary
        handle the rotation based on user's input for periodicity and size\n\tlogrotate
        --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep 15m\ndone\n"
      env:
      - name: TZ
        value: Australia/Sydney
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: log-collector
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jrhjf
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jrhjf
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      - /var/lib/ceph/mgr/ceph-a
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: "1"
          memory: 2Gi
        requests:
          cpu: 250m
          memory: 1Gi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mgr-a-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mgr/ceph-a
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jrhjf
        readOnly: true
    nodeName: k8s-node-yellow
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-mgr
    serviceAccountName: rook-ceph-mgr
    shareProcessNamespace: true
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 5
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - name: rook-ceph-mgr-a-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-mgr-a-keyring
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/storage/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/storage/crash
        type: ""
      name: rook-ceph-crash
    - emptyDir: {}
      name: ceph-daemon-data
    - emptyDir: {}
      name: rook-config
    - emptyDir: {}
      name: default-config-dir
    - name: ceph-admin-secret
      secret:
        defaultMode: 420
        items:
        - key: ceph-secret
          path: secret.keyring
        secretName: rook-ceph-mon
    - name: kube-api-access-jrhjf
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-10-03T22:30:20Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-10-03T22:30:21Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-10-03T22:30:40Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-10-03T22:30:40Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-10-03T22:30:19Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://2233e5d11801a19924b27c14db1429a230510470d6942e16bf87ebab20981037
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: log-collector
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-10-03T22:30:21Z"
    - containerID: containerd://5a52d56a6030908f53225e58d37a477ea8268a33c9264f41757515b2723ed33c
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: mgr
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-10-03T22:30:21Z"
    - containerID: containerd://e92edc406e6be970f761ef1a3b749909b98ff87bbf00c86e060e00a08f17460b
      image: docker.io/rook/ceph:v1.15.3
      imageID: docker.io/rook/ceph@sha256:2bd2af8b9ec5651c703fa4f0d8419da5e9a50d4575520ee4e921bf3cec481332
      lastState: {}
      name: watch-active
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-10-03T22:30:21Z"
    hostIP: 10.0.90.14
    hostIPs:
    - ip: 10.0.90.14
    initContainerStatuses:
    - containerID: containerd://c9c88595528e218fc1332c4d65e24936c8aee478b234c615331c3c382f8da957
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://c9c88595528e218fc1332c4d65e24936c8aee478b234c615331c3c382f8da957
          exitCode: 0
          finishedAt: "2024-10-03T22:30:20Z"
          reason: Completed
          startedAt: "2024-10-03T22:30:20Z"
    - containerID: containerd://919a3120f9f6dbfbd7534ff578d594f90f6f6b566928ea3e49a9cce6b176fb3a
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://919a3120f9f6dbfbd7534ff578d594f90f6f6b566928ea3e49a9cce6b176fb3a
          exitCode: 0
          finishedAt: "2024-10-03T22:30:20Z"
          reason: Completed
          startedAt: "2024-10-03T22:30:20Z"
    phase: Running
    podIP: 10.42.205.90
    podIPs:
    - ip: 10.42.205.90
    qosClass: Burstable
    startTime: "2024-10-03T22:30:19Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 76e074e4fe88852925532f0d1da46ba0832416bfe5f66890edfeaf627ac1c716
      cni.projectcalico.org/podIP: 10.42.186.184/32
      cni.projectcalico.org/podIPs: 10.42.186.184/32
      prometheus.io/port: "9102"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-10-03T22:30:43Z"
    generateName: rook-ceph-mgr-b-6855b7bb4b-
    labels:
      app: rook-ceph-mgr
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: b
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mgr
      app.kubernetes.io/part-of: storage
      ceph_daemon_id: b
      ceph_daemon_type: mgr
      instance: b
      mgr: b
      mgr_role: standby
      pod-template-hash: 6855b7bb4b
      rook.io/operator-namespace: storage
      rook_cluster: storage
    name: rook-ceph-mgr-b-6855b7bb4b-r9sxp
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-mgr-b-6855b7bb4b
      uid: 56157285-6d85-4cb3-8568-5f7b75e42ce9
    resourceVersion: "1341801047"
    uid: 7487df50-c565-4a2b-9632-5780c6949535
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app: rook-ceph-mgr
              rook_cluster: storage
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --fsid=9dff65cc-5b8f-413f-a1ed-8bfb6b93684a
      - --keyring=/etc/ceph/keyring-store/keyring
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=b
      - --setuser=ceph
      - --setgroup=ceph
      - --client-mount-uid=0
      - --client-mount-gid=0
      - --foreground
      - --public-addr=$(ROOK_POD_IP)
      command:
      - ceph-mgr
      env:
      - name: TZ
        value: Australia/Sydney
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.4
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: ROOK_OPERATOR_NAMESPACE
        value: storage
      - name: ROOK_CEPH_CLUSTER_CRD_VERSION
        value: v1
      - name: ROOK_CEPH_CLUSTER_CRD_NAME
        value: storage
      - name: CEPH_ARGS
        value: --mon-host $(ROOK_CEPH_MON_HOST) --keyring /etc/ceph/keyring-store/keyring
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.b.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with the
            following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: mgr
      ports:
      - containerPort: 6800
        name: mgr
        protocol: TCP
      - containerPort: 9283
        name: http-metrics
        protocol: TCP
      - containerPort: 8443
        name: dashboard
        protocol: TCP
      resources:
        limits:
          cpu: "1"
          memory: 2Gi
        requests:
          cpu: 250m
          memory: 1Gi
      securityContext:
        privileged: false
      startupProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.b.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with the
            following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
        failureThreshold: 6
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mgr-b-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mgr/ceph-b
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vcnr9
        readOnly: true
      workingDir: /var/log/ceph
    - args:
      - ceph
      - mgr
      - watch-active
      env:
      - name: TZ
        value: Australia/Sydney
      - name: ROOK_CLUSTER_ID
        value: 9f4e59e5-14c0-4d39-ac52-c4fcf6bd6c10
      - name: ROOK_CLUSTER_NAME
        value: storage
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: storage
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_CEPH_USERNAME
        valueFrom:
          secretKeyRef:
            key: ceph-username
            name: rook-ceph-mon
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: ROOK_DASHBOARD_ENABLED
        value: "true"
      - name: ROOK_MONITORING_ENABLED
        value: "true"
      - name: ROOK_UPDATE_INTERVAL
        value: 15s
      - name: ROOK_DAEMON_NAME
        value: b
      - name: ROOK_CEPH_VERSION
        value: ceph version 18.2.4-0 reef
      image: docker.io/rook/ceph:v1.15.3
      imagePullPolicy: IfNotPresent
      name: watch-active
      resources:
        limits:
          cpu: 500m
          memory: 100Mi
        requests:
          cpu: 100m
          memory: 40Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /var/lib/rook
        name: rook-config
      - mountPath: /etc/ceph
        name: default-config-dir
      - mountPath: /var/lib/rook-ceph-mon
        name: ceph-admin-secret
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vcnr9
        readOnly: true
    - command:
      - /bin/bash
      - -x
      - -e
      - -m
      - -c
      - "\nCEPH_CLIENT_ID=ceph-mgr.b\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
        edit the logrotate file to only rotate a specific daemon log\n# otherwise
        we will logrotate log files without reloading certain daemons\n# this might
        happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
        \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
        --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
        rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed --in-place
        \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif [ \"$LOG_MAX_SIZE\"
        != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE at the 4th line of the
        logrotate config file with 4 spaces to maintain indentation\n\tsed --in-place
        \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\" \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile
        true; do\n\t# we don't force the logrorate but we let the logrotate binary
        handle the rotation based on user's input for periodicity and size\n\tlogrotate
        --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep 15m\ndone\n"
      env:
      - name: TZ
        value: Australia/Sydney
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: log-collector
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vcnr9
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vcnr9
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      - /var/lib/ceph/mgr/ceph-b
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: "1"
          memory: 2Gi
        requests:
          cpu: 250m
          memory: 1Gi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mgr-b-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mgr/ceph-b
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vcnr9
        readOnly: true
    nodeName: k8s-node-orange
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-mgr
    serviceAccountName: rook-ceph-mgr
    shareProcessNamespace: true
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 5
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - name: rook-ceph-mgr-b-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-mgr-b-keyring
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/storage/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/storage/crash
        type: ""
      name: rook-ceph-crash
    - emptyDir: {}
      name: ceph-daemon-data
    - emptyDir: {}
      name: rook-config
    - emptyDir: {}
      name: default-config-dir
    - name: ceph-admin-secret
      secret:
        defaultMode: 420
        items:
        - key: ceph-secret
          path: secret.keyring
        secretName: rook-ceph-mon
    - name: kube-api-access-vcnr9
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-10-03T22:30:44Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-10-03T22:30:45Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-10-03T22:31:03Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-10-03T22:31:03Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-10-03T22:30:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://890dc2e33babc0373430e26c46d7277f90862b8615beeab3c2bcba9d8241371f
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: log-collector
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-10-03T22:30:46Z"
    - containerID: containerd://1e88a316fb13f2ee700f327559d96e10fe12899c555aa54751c06afb7e6103fa
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: mgr
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-10-03T22:30:45Z"
    - containerID: containerd://d24d2ca8ffa5e4ceaccbe1ce807e1a23bddd9d30d02446724915a4ca3428e13a
      image: docker.io/rook/ceph:v1.15.3
      imageID: docker.io/rook/ceph@sha256:2bd2af8b9ec5651c703fa4f0d8419da5e9a50d4575520ee4e921bf3cec481332
      lastState: {}
      name: watch-active
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-10-03T22:30:45Z"
    hostIP: 10.0.90.12
    hostIPs:
    - ip: 10.0.90.12
    initContainerStatuses:
    - containerID: containerd://3dc93aa6ef7e8b5ce05e715310632546435bba70f2cb4085de03938a6830c67a
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://3dc93aa6ef7e8b5ce05e715310632546435bba70f2cb4085de03938a6830c67a
          exitCode: 0
          finishedAt: "2024-10-03T22:30:44Z"
          reason: Completed
          startedAt: "2024-10-03T22:30:44Z"
    - containerID: containerd://02856e424eb95db569a114afe9ed227a57a258886112d2d10f44adb444846733
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://02856e424eb95db569a114afe9ed227a57a258886112d2d10f44adb444846733
          exitCode: 0
          finishedAt: "2024-10-03T22:30:44Z"
          reason: Completed
          startedAt: "2024-10-03T22:30:44Z"
    phase: Running
    podIP: 10.42.186.184
    podIPs:
    - ip: 10.42.186.184
    qosClass: Burstable
    startTime: "2024-10-03T22:30:43Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: d5ec62ffb1ba6089a5c9d0d051551c88ed16b45fd1e00df2580084b291aad032
      cni.projectcalico.org/podIP: 10.42.186.165/32
      cni.projectcalico.org/podIPs: 10.42.186.165/32
      prometheus.io/port: "9102"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-08-17T17:52:27Z"
    generateName: rook-ceph-mon-bk-f9cccbccc-
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: bk
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: storage
      ceph_daemon_id: bk
      ceph_daemon_type: mon
      mon: bk
      mon_cluster: storage
      pod-template-hash: f9cccbccc
      rook.io/operator-namespace: storage
      rook_cluster: storage
    name: rook-ceph-mon-bk-f9cccbccc-kgbs7
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-mon-bk-f9cccbccc
      uid: c6b2e486-d81f-4b60-8294-d2ad46da7e06
    resourceVersion: "1330834490"
    uid: 1f881705-0cd1-491b-9a2f-34eea919a496
  spec:
    affinity: {}
    containers:
    - args:
      - --fsid=9dff65cc-5b8f-413f-a1ed-8bfb6b93684a
      - --keyring=/etc/ceph/keyring-store/keyring
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=bk
      - --setuser=ceph
      - --setgroup=ceph
      - --foreground
      - --public-addr=10.43.254.129
      - --setuser-match-path=/var/lib/ceph/mon/ceph-bk/store.db
      - --public-bind-addr=$(ROOK_POD_IP)
      command:
      - ceph-mon
      env:
      - name: TZ
        value: Australia/Sydney
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.4
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.bk.asok mon_status
            2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check
            failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: mon
      ports:
      - containerPort: 3300
        name: tcp-msgr2
        protocol: TCP
      - containerPort: 6789
        name: tcp-msgr1
        protocol: TCP
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 500Mi
      securityContext:
        privileged: false
      startupProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.bk.asok mon_status
            2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check
            failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 6
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-bk
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kl84j
        readOnly: true
      workingDir: /var/log/ceph
    - command:
      - /bin/bash
      - -x
      - -e
      - -m
      - -c
      - "\nCEPH_CLIENT_ID=ceph-mon.bk\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
        edit the logrotate file to only rotate a specific daemon log\n# otherwise
        we will logrotate log files without reloading certain daemons\n# this might
        happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
        \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
        --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
        rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed --in-place
        \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif [ \"$LOG_MAX_SIZE\"
        != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE at the 4th line of the
        logrotate config file with 4 spaces to maintain indentation\n\tsed --in-place
        \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\" \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile
        true; do\n\t# we don't force the logrorate but we let the logrotate binary
        handle the rotation based on user's input for periodicity and size\n\tlogrotate
        --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep 15m\ndone\n"
      env:
      - name: TZ
        value: Australia/Sydney
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: log-collector
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kl84j
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kl84j
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      - /var/lib/ceph/mon/ceph-bk
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 500Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-bk
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kl84j
        readOnly: true
    - args:
      - --fsid=9dff65cc-5b8f-413f-a1ed-8bfb6b93684a
      - --keyring=/etc/ceph/keyring-store/keyring
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=bk
      - --setuser=ceph
      - --setgroup=ceph
      - --public-addr=10.43.254.129
      - --mkfs
      command:
      - ceph-mon
      env:
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.4
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: init-mon-fs
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 500Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-bk
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kl84j
        readOnly: true
    nodeName: k8s-node-orange
    nodeSelector:
      kubernetes.io/hostname: k8s-node-orange
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-default
    serviceAccountName: rook-ceph-default
    shareProcessNamespace: true
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - name: rook-ceph-mons-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-mons-keyring
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/storage/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/storage/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /var/lib/rook/mon-bk/data
        type: ""
      name: ceph-daemon-data
    - name: kube-api-access-kl84j
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:59:34Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-08-17T17:52:32Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:59:53Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:59:53Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-08-17T17:52:28Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://b8f612d4cd807512018689809383433acad33c6c7408f326e3b1a90a9a4f13e2
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState:
        terminated:
          containerID: containerd://64ab3204d9217ecd555feccd1b1e81c7db05b0d3a7d2d16ed966cdc3c12c1318
          exitCode: 255
          finishedAt: "2024-09-23T15:59:18Z"
          reason: Unknown
          startedAt: "2024-09-23T15:47:57Z"
      name: log-collector
      ready: true
      restartCount: 3
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:59:39Z"
    - containerID: containerd://7b04b9468a861ba1823f99c16267e902515a934170351724dce00007dbc056c7
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState:
        terminated:
          containerID: containerd://4a0dcf51136cb749014286236221de8db7ba9db19c409bf589d9645460d498a2
          exitCode: 255
          finishedAt: "2024-09-23T15:59:18Z"
          reason: Unknown
          startedAt: "2024-09-23T15:47:56Z"
      name: mon
      ready: true
      restartCount: 3
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:59:39Z"
    hostIP: 10.0.90.12
    hostIPs:
    - ip: 10.0.90.12
    initContainerStatuses:
    - containerID: containerd://3ca451efd8c97ca2bc58368b20570b7fcc6de12585cd08f29ea0d24555fc80ff
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 3
      started: false
      state:
        terminated:
          containerID: containerd://3ca451efd8c97ca2bc58368b20570b7fcc6de12585cd08f29ea0d24555fc80ff
          exitCode: 0
          finishedAt: "2024-09-23T15:59:34Z"
          reason: Completed
          startedAt: "2024-09-23T15:59:34Z"
    - containerID: containerd://df6afc23e87c2e074659da41829d8418683cc754efd9485039b265cff1de933e
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://df6afc23e87c2e074659da41829d8418683cc754efd9485039b265cff1de933e
          exitCode: 0
          finishedAt: "2024-09-23T15:59:36Z"
          reason: Completed
          startedAt: "2024-09-23T15:59:36Z"
    - containerID: containerd://ca85e17cd58c2e3bc576f3f44aae08cfeab020955a6014379e26b442dd798a20
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: init-mon-fs
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://ca85e17cd58c2e3bc576f3f44aae08cfeab020955a6014379e26b442dd798a20
          exitCode: 0
          finishedAt: "2024-09-23T15:59:38Z"
          reason: Completed
          startedAt: "2024-09-23T15:59:37Z"
    phase: Running
    podIP: 10.42.186.165
    podIPs:
    - ip: 10.42.186.165
    qosClass: Burstable
    startTime: "2024-08-17T17:52:28Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: e559b021ea85664214f5b3343ad844c5a196ca52c9a4b6db208099cbbe9e0939
      cni.projectcalico.org/podIP: 10.42.205.76/32
      cni.projectcalico.org/podIPs: 10.42.205.76/32
      prometheus.io/port: "9102"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-09-23T15:00:14Z"
    generateName: rook-ceph-mon-bl-7c9786f6-
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: bl
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: storage
      ceph_daemon_id: bl
      ceph_daemon_type: mon
      mon: bl
      mon_cluster: storage
      pod-template-hash: 7c9786f6
      rook.io/operator-namespace: storage
      rook_cluster: storage
    name: rook-ceph-mon-bl-7c9786f6-4xwfv
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-mon-bl-7c9786f6
      uid: bfbf8afb-de04-4fb8-aa4d-dfbe62820202
    resourceVersion: "1330786541"
    uid: f2d0480f-5b02-4f30-a007-673450537b69
  spec:
    affinity: {}
    containers:
    - args:
      - --fsid=9dff65cc-5b8f-413f-a1ed-8bfb6b93684a
      - --keyring=/etc/ceph/keyring-store/keyring
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=bl
      - --setuser=ceph
      - --setgroup=ceph
      - --foreground
      - --public-addr=10.43.198.225
      - --setuser-match-path=/var/lib/ceph/mon/ceph-bl/store.db
      - --public-bind-addr=$(ROOK_POD_IP)
      command:
      - ceph-mon
      env:
      - name: TZ
        value: Australia/Sydney
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.4
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.bl.asok mon_status
            2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check
            failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: mon
      ports:
      - containerPort: 3300
        name: tcp-msgr2
        protocol: TCP
      - containerPort: 6789
        name: tcp-msgr1
        protocol: TCP
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 500Mi
      securityContext:
        privileged: false
      startupProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.bl.asok mon_status
            2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check
            failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 6
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-bl
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-f55c7
        readOnly: true
      workingDir: /var/log/ceph
    - command:
      - /bin/bash
      - -x
      - -e
      - -m
      - -c
      - "\nCEPH_CLIENT_ID=ceph-mon.bl\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
        edit the logrotate file to only rotate a specific daemon log\n# otherwise
        we will logrotate log files without reloading certain daemons\n# this might
        happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
        \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
        --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
        rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed --in-place
        \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif [ \"$LOG_MAX_SIZE\"
        != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE at the 4th line of the
        logrotate config file with 4 spaces to maintain indentation\n\tsed --in-place
        \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\" \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile
        true; do\n\t# we don't force the logrorate but we let the logrotate binary
        handle the rotation based on user's input for periodicity and size\n\tlogrotate
        --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep 15m\ndone\n"
      env:
      - name: TZ
        value: Australia/Sydney
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: log-collector
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-f55c7
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-f55c7
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      - /var/lib/ceph/mon/ceph-bl
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 500Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-bl
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-f55c7
        readOnly: true
    - args:
      - --fsid=9dff65cc-5b8f-413f-a1ed-8bfb6b93684a
      - --keyring=/etc/ceph/keyring-store/keyring
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=bl
      - --setuser=ceph
      - --setgroup=ceph
      - --public-addr=10.43.198.225
      - --mkfs
      command:
      - ceph-mon
      env:
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.4
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: init-mon-fs
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 500Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-bl
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-f55c7
        readOnly: true
    nodeName: k8s-node-yellow
    nodeSelector:
      kubernetes.io/hostname: k8s-node-yellow
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-default
    serviceAccountName: rook-ceph-default
    shareProcessNamespace: true
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - name: rook-ceph-mons-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-mons-keyring
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/storage/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/storage/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /var/lib/rook/mon-bl/data
        type: ""
      name: ceph-daemon-data
    - name: kube-api-access-f55c7
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:08:31Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:08:39Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:08:53Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:08:53Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:07:33Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://2963ecfc64b845ae8fc5c1a81eb3ac83b09022d0e2ce7ebae62b63ea7685b9c0
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: log-collector
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:08:39Z"
    - containerID: containerd://f8e348e750d6e4697f686e6dfd45f2927dd886d3193db0381af613a74cc5a831
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: mon
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:08:39Z"
    hostIP: 10.0.90.14
    hostIPs:
    - ip: 10.0.90.14
    initContainerStatuses:
    - containerID: containerd://daea2678fe2fc139474ab0da45ff01bd67ff4b69618c90ab4f4a5229fc29abd7
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://daea2678fe2fc139474ab0da45ff01bd67ff4b69618c90ab4f4a5229fc29abd7
          exitCode: 0
          finishedAt: "2024-09-23T15:08:30Z"
          reason: Completed
          startedAt: "2024-09-23T15:08:30Z"
    - containerID: containerd://e6420133091ab90e7d6546f6e2f92c9b9a5ca8b40055035e57a99779de41530a
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://e6420133091ab90e7d6546f6e2f92c9b9a5ca8b40055035e57a99779de41530a
          exitCode: 0
          finishedAt: "2024-09-23T15:08:36Z"
          reason: Completed
          startedAt: "2024-09-23T15:08:36Z"
    - containerID: containerd://d5f201cf23f9032d2ae4694b0fc65f6d9af63b8b6c9c51e9bf9981b18ba8b8a0
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: init-mon-fs
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://d5f201cf23f9032d2ae4694b0fc65f6d9af63b8b6c9c51e9bf9981b18ba8b8a0
          exitCode: 0
          finishedAt: "2024-09-23T15:08:38Z"
          reason: Completed
          startedAt: "2024-09-23T15:08:38Z"
    phase: Running
    podIP: 10.42.205.76
    podIPs:
    - ip: 10.42.205.76
    qosClass: Burstable
    startTime: "2024-09-23T15:07:33Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 730957e567f34197ac7208b01c063a26a3b09772eb775a5285720ee44e14c115
      cni.projectcalico.org/podIP: 10.42.3.19/32
      cni.projectcalico.org/podIPs: 10.42.3.19/32
      prometheus.io/port: "9102"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-08-19T12:46:01Z"
    generateName: rook-ceph-mon-o-779dfd7fd5-
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: o
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: storage
      ceph_daemon_id: o
      ceph_daemon_type: mon
      mon: o
      mon_cluster: storage
      pod-template-hash: 779dfd7fd5
      rook.io/operator-namespace: storage
      rook_cluster: storage
    name: rook-ceph-mon-o-779dfd7fd5-zhg8b
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-mon-o-779dfd7fd5
      uid: 319929e4-2f40-439f-80ff-6207e7257b57
    resourceVersion: "1349044660"
    uid: 30d846fe-b4a8-415d-954b-919eeec93f29
  spec:
    affinity: {}
    containers:
    - args:
      - --fsid=9dff65cc-5b8f-413f-a1ed-8bfb6b93684a
      - --keyring=/etc/ceph/keyring-store/keyring
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=o
      - --setuser=ceph
      - --setgroup=ceph
      - --foreground
      - --public-addr=10.43.101.90
      - --setuser-match-path=/var/lib/ceph/mon/ceph-o/store.db
      - --public-bind-addr=$(ROOK_POD_IP)
      command:
      - ceph-mon
      env:
      - name: TZ
        value: Australia/Sydney
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.4
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.o.asok mon_status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with the
            following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: mon
      ports:
      - containerPort: 3300
        name: tcp-msgr2
        protocol: TCP
      - containerPort: 6789
        name: tcp-msgr1
        protocol: TCP
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 500Mi
      securityContext:
        privileged: false
      startupProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.o.asok mon_status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with the
            following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
        failureThreshold: 6
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-o
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-tpfpw
        readOnly: true
      workingDir: /var/log/ceph
    - command:
      - /bin/bash
      - -x
      - -e
      - -m
      - -c
      - "\nCEPH_CLIENT_ID=ceph-mon.o\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
        edit the logrotate file to only rotate a specific daemon log\n# otherwise
        we will logrotate log files without reloading certain daemons\n# this might
        happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
        \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
        --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
        rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed --in-place
        \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif [ \"$LOG_MAX_SIZE\"
        != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE at the 4th line of the
        logrotate config file with 4 spaces to maintain indentation\n\tsed --in-place
        \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\" \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile
        true; do\n\t# we don't force the logrorate but we let the logrotate binary
        handle the rotation based on user's input for periodicity and size\n\tlogrotate
        --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep 15m\ndone\n"
      env:
      - name: TZ
        value: Australia/Sydney
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: log-collector
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-tpfpw
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-tpfpw
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      - /var/lib/ceph/mon/ceph-o
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 500Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-o
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-tpfpw
        readOnly: true
    - args:
      - --fsid=9dff65cc-5b8f-413f-a1ed-8bfb6b93684a
      - --keyring=/etc/ceph/keyring-store/keyring
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=o
      - --setuser=ceph
      - --setgroup=ceph
      - --public-addr=10.43.101.90
      - --mkfs
      command:
      - ceph-mon
      env:
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.4
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: init-mon-fs
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 500Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-o
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-tpfpw
        readOnly: true
    nodeName: k8s-node-blue
    nodeSelector:
      kubernetes.io/hostname: k8s-node-blue
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-default
    serviceAccountName: rook-ceph-default
    shareProcessNamespace: true
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - name: rook-ceph-mons-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-mons-keyring
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/storage/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/storage/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /var/lib/rook/mon-o/data
        type: ""
      name: ceph-daemon-data
    - name: kube-api-access-tpfpw
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-08-19T23:16:50Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-08-19T12:58:02Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-10-10T22:15:11Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-10-10T22:15:11Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-08-19T12:57:27Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://78a82f91cf3d5d0719f14681d621773539bd8249e50bc020e2c271536d37df77
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState:
        terminated:
          containerID: containerd://7c6865602d0488d48e8a88b5e1e3216590d125fee17fda6d132ce8fed04b2d06
          exitCode: 255
          finishedAt: "2024-08-19T23:16:01Z"
          reason: Unknown
          startedAt: "2024-08-19T12:58:02Z"
      name: log-collector
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2024-08-19T23:16:52Z"
    - containerID: containerd://c2823a345e6f3c1e73f9325e3f08a5bce05d0b73d063cab2348593c128d28905
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState:
        terminated:
          containerID: containerd://c84b2443ca38958f4654bf16de2f126f6bfe2f19de2835af41e19d5783ad43d8
          exitCode: 137
          finishedAt: "2024-08-19T18:54:22Z"
          reason: OOMKilled
          startedAt: "2024-08-19T12:58:02Z"
      name: mon
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2024-08-19T23:16:52Z"
    hostIP: 10.0.90.10
    hostIPs:
    - ip: 10.0.90.10
    initContainerStatuses:
    - containerID: containerd://d422920632141d5386beeb53c545fada26f0b1f349a1b8a0cb5f5b0d7b0d978e
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 1
      started: false
      state:
        terminated:
          containerID: containerd://d422920632141d5386beeb53c545fada26f0b1f349a1b8a0cb5f5b0d7b0d978e
          exitCode: 0
          finishedAt: "2024-08-19T23:16:49Z"
          reason: Completed
          startedAt: "2024-08-19T23:16:49Z"
    - containerID: containerd://c0f4e2e6289f2bfb22bc476145b61d30ef40b5bb7a07592b8270cf559f8fefe4
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://c0f4e2e6289f2bfb22bc476145b61d30ef40b5bb7a07592b8270cf559f8fefe4
          exitCode: 0
          finishedAt: "2024-08-19T23:16:50Z"
          reason: Completed
          startedAt: "2024-08-19T23:16:50Z"
    - containerID: containerd://43680fdec36711336bf3900de2635f25e388ab55c21affd52aa54c28d1bdc61f
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: init-mon-fs
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://43680fdec36711336bf3900de2635f25e388ab55c21affd52aa54c28d1bdc61f
          exitCode: 0
          finishedAt: "2024-08-19T23:16:51Z"
          reason: Completed
          startedAt: "2024-08-19T23:16:51Z"
    phase: Running
    podIP: 10.42.3.19
    podIPs:
    - ip: 10.42.3.19
    qosClass: Burstable
    startTime: "2024-08-19T12:57:28Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: e6644ba2e8ca5e7f0a6d323597e1b1e7826a6e521a210c49ba8ae4820dfeee5c
      cni.projectcalico.org/podIP: 10.42.205.87/32
      cni.projectcalico.org/podIPs: 10.42.205.87/32
      kubectl.kubernetes.io/restartedAt: "2024-01-21T02:03:37Z"
    creationTimestamp: "2024-10-03T22:29:39Z"
    generateName: rook-ceph-operator-56cb68bbc8-
    labels:
      app: rook-ceph-operator
      helm.sh/chart: rook-ceph-v1.15.3
      pod-template-hash: 56cb68bbc8
    name: rook-ceph-operator-56cb68bbc8-cqn8v
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-operator-56cb68bbc8
      uid: 28f0db06-75ca-4746-be5f-e3b0b6a0eff6
    resourceVersion: "1341798736"
    uid: a10a158e-b0d2-470e-aefa-d9f55e590699
  spec:
    containers:
    - args:
      - ceph
      - operator
      env:
      - name: TZ
        value: Australia/Sydney
      - name: ROOK_CURRENT_NAMESPACE_ONLY
        value: "false"
      - name: ROOK_HOSTPATH_REQUIRES_PRIVILEGED
        value: "false"
      - name: ROOK_DISABLE_DEVICE_HOTPLUG
        value: "false"
      - name: ROOK_DISCOVER_DEVICES_INTERVAL
        value: 60m
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: docker.io/rook/ceph:v1.15.3
      imagePullPolicy: IfNotPresent
      name: rook-ceph-operator
      resources:
        limits:
          memory: 512Mi
        requests:
          cpu: 500m
          memory: 300Mi
      securityContext:
        capabilities:
          drop:
          - ALL
        runAsGroup: 2016
        runAsNonRoot: true
        runAsUser: 2016
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /var/lib/rook
        name: rook-config
      - mountPath: /etc/ceph
        name: default-config-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wsx84
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wsx84
        readOnly: true
    nodeName: k8s-node-yellow
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-system
    serviceAccountName: rook-ceph-system
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 5
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - emptyDir: {}
      name: rook-config
    - emptyDir: {}
      name: default-config-dir
    - name: kube-api-access-wsx84
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-10-03T22:29:40Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-10-03T22:29:40Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-10-03T22:29:47Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-10-03T22:29:47Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-10-03T22:29:39Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://8d75a946eecc4b17e873a8f211b57f465d40a2d0d635b5095cf72e857f35ba0d
      image: docker.io/rook/ceph:v1.15.3
      imageID: docker.io/rook/ceph@sha256:2bd2af8b9ec5651c703fa4f0d8419da5e9a50d4575520ee4e921bf3cec481332
      lastState: {}
      name: rook-ceph-operator
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-10-03T22:29:47Z"
    hostIP: 10.0.90.14
    hostIPs:
    - ip: 10.0.90.14
    initContainerStatuses:
    - containerID: containerd://a843c3bfed9d7ba7ab8e083467526d1bcee2c06c7ab6060389a8a33e499d1a4f
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://a843c3bfed9d7ba7ab8e083467526d1bcee2c06c7ab6060389a8a33e499d1a4f
          exitCode: 0
          finishedAt: "2024-10-03T22:29:40Z"
          reason: Completed
          startedAt: "2024-10-03T22:29:40Z"
    phase: Running
    podIP: 10.42.205.87
    podIPs:
    - ip: 10.42.205.87
    qosClass: Burstable
    startTime: "2024-10-03T22:29:39Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: fc5b9771c33453c848bce85ecb0e56878ec7defca3c1f9bfed6f61272074d3c4
      cni.projectcalico.org/podIP: 10.42.186.128/32
      cni.projectcalico.org/podIPs: 10.42.186.128/32
      prometheus.io/port: "9102"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-08-13T03:09:19Z"
    generateName: rook-ceph-osd-0-5f68f58677-
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "0"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: storage
      ceph-osd-id: "0"
      ceph_daemon_id: "0"
      ceph_daemon_type: osd
      device-class: hdd
      failure-domain: k8s-node-orange
      osd: "0"
      osd-store: bluestore
      pod-template-hash: 5f68f58677
      portable: "false"
      rook.io/operator-namespace: storage
      rook_cluster: storage
      topology-location-host: k8s-node-orange
      topology-location-root: default
    name: rook-ceph-osd-0-5f68f58677-2kzg8
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-osd-0-5f68f58677
      uid: 1bfd0941-538f-4a29-8982-d393d38671a0
    resourceVersion: "1330834707"
    uid: 77a495ac-8226-42b4-aa21-03819b2a2f11
  spec:
    affinity: {}
    containers:
    - args:
      - --foreground
      - --id
      - "0"
      - --fsid
      - 9dff65cc-5b8f-413f-a1ed-8bfb6b93684a
      - --setuser
      - ceph
      - --setgroup
      - ceph
      - --crush-location=root=default host=k8s-node-orange
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --ms-learn-addr-from-peer=false
      command:
      - ceph-osd
      env:
      - name: TZ
        value: Australia/Sydney
      - name: ROOK_NODE_NAME
        value: k8s-node-orange
      - name: ROOK_CLUSTER_ID
        value: 9f4e59e5-14c0-4d39-ac52-c4fcf6bd6c10
      - name: ROOK_CLUSTER_NAME
        value: storage
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: storage
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: k8s-node-orange
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.4
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_OSD_RESTART_INTERVAL
        value: "0"
      - name: ROOK_OSD_UUID
        value: 6a2da046-949a-4f33-a040-8a687ccb95d0
      - name: ROOK_OSD_ID
        value: "0"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sdb
      - name: ROOK_CV_MODE
        value: raw
      - name: ROOK_OSD_DEVICE_CLASS
        value: hdd
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.0.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: osd
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      startupProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.0.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 720
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-0
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-d2d7c
        readOnly: true
      workingDir: /var/log/ceph
    - command:
      - /bin/bash
      - -x
      - -e
      - -m
      - -c
      - "\nCEPH_CLIENT_ID=ceph-osd.0\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
        edit the logrotate file to only rotate a specific daemon log\n# otherwise
        we will logrotate log files without reloading certain daemons\n# this might
        happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
        \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
        --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
        rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed --in-place
        \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif [ \"$LOG_MAX_SIZE\"
        != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE at the 4th line of the
        logrotate config file with 4 spaces to maintain indentation\n\tsed --in-place
        \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\" \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile
        true; do\n\t# we don't force the logrorate but we let the logrotate binary
        handle the rotation based on user's input for periodicity and size\n\tlogrotate
        --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep 15m\ndone\n"
      env:
      - name: TZ
        value: Australia/Sydney
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: log-collector
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-d2d7c
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-d2d7c
        readOnly: true
    - command:
      - /bin/bash
      - -c
      - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables are
        unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=9dff65cc-5b8f-413f-a1ed-8bfb6b93684a\nOSD_UUID=6a2da046-949a-4f33-a040-8a687ccb95d0\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
        \"ceph.conf\" must have the \"fsid\" global configuration to activate encrypted
        OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
        This limitation will be removed later. After that, we can remove this\n# fsid
        injection code. Probably a good time is when to remove Quincy support.\n#
        https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
        /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
        = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif not
        config.has_section('global'):\n    config['global'] = {}\n\nif not config.has_option('global','fsid'):\n
        \   config['global']['fsid'] = '$CEPH_FSID'\n\nwith open('/etc/ceph/ceph.conf',
        'w') as configfile:\n    config.write(configfile)\n\"\n\n# create new keyring\nceph
        -n client.admin auth get-or-create osd.\"$OSD_ID\" mon 'allow profile osd'
        mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
        \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary directory\n\t#
        this is needed because when the init container exits, the tmpfs goes away
        and its content with it\n\t# this will result in the emptydir to be empty
        when accessed by the main osd container\n\tcp --verbose --no-dereference \"$OSD_DATA_DIR\"/*
        \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't need it anymore\n\tumount
        \"$OSD_DATA_DIR\"\n\n\t# copy back the content of the tmpfs into the original
        osd directory\n\tcp --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t#
        retain ownership of files to the ceph user/group\n\tchown --verbose --recursive
        ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm --recursive
        --force \"$TMP_DIR\"\nelse\n\t# 'ceph-volume raw list' (which the osd-prepare
        job uses to report OSDs on nodes)\n\t#  returns user-friendly device names
        which can change when systems reboot. To\n\t# keep OSD pods from crashing
        repeatedly after a reboot, we need to check if the\n\t# block device we have
        is still correct, and if it isn't correct, we need to\n\t# scan all the disks
        to find the right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device()
        {\n\t\t# jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
        python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
        sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
        == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device: '
        + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
        \ # don't keep processing once the disk is found\nsys.exit('no disk found
        with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
        > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
        '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device < \"$OSD_LIST\";
        then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device
        < \"$OSD_LIST\")\"\n\tfi\n\t[[ -z \"$DEVICE\" ]] && { echo \"no device\" ;
        exit 1 ; }\n\n\t# If a kernel device name change happens and a block device
        file\n\t# in the OSD directory becomes missing, this OSD fails to start\n\t#
        continuously. This problem can be resolved by confirming\n\t# the validity
        of the device file and recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
        [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ; then\n\t\trm
        $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports bluestore
        so we don't need to pass a store flag\n\tceph-volume raw activate --device
        \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
      env:
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sdb
      - name: ROOK_METADATA_DEVICE
      - name: ROOK_WAL_DEVICE
      - name: ROOK_OSD_ID
        value: "0"
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: activate
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-0
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /etc/temp-ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /etc/ceph/admin-keyring-store/
        name: rook-ceph-admin-keyring
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-d2d7c
        readOnly: true
    - args:
      - bluefs-bdev-expand
      - --path
      - /var/lib/ceph/osd/ceph-0
      command:
      - ceph-bluestore-tool
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: expand-bluefs
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-0
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-d2d7c
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-0
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-d2d7c
        readOnly: true
    nodeName: k8s-node-orange
    nodeSelector:
      kubernetes.io/hostname: k8s-node-orange
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    shareProcessNamespace: true
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/storage/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/storage/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - hostPath:
        path: /run/udev
        type: ""
      name: run-udev
    - hostPath:
        path: /var/lib/rook/storage/9dff65cc-5b8f-413f-a1ed-8bfb6b93684a_6a2da046-949a-4f33-a040-8a687ccb95d0
        type: DirectoryOrCreate
      name: activate-osd
    - name: rook-ceph-admin-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-admin-keyring
    - name: kube-api-access-d2d7c
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:59:32Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-08-13T03:09:35Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:59:59Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:59:59Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-08-13T03:09:19Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a3043eef0177f4d5eb2e40948ce22c1e75b629f3fe8bdd35a4218269c57323f2
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState:
        terminated:
          containerID: containerd://4ebabca92e3d9afb9da34379f7c8a9a4ab2fea79e7dca5d9d907f7ec3d1c7a29
          exitCode: 255
          finishedAt: "2024-09-23T15:59:18Z"
          reason: Unknown
          startedAt: "2024-09-23T15:48:17Z"
      name: log-collector
      ready: true
      restartCount: 4
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:59:47Z"
    - containerID: containerd://50c3d7c32d218a2e0a7e99cda8e10400e9403859379dd600a97552dedf2ce0d5
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState:
        terminated:
          containerID: containerd://2eebde1b348c9d612afb73c406910ce2c9cbdf64e572b94c2511026c2465ed9c
          exitCode: 255
          finishedAt: "2024-09-23T15:59:18Z"
          reason: Unknown
          startedAt: "2024-09-23T15:48:16Z"
      name: osd
      ready: true
      restartCount: 4
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:59:47Z"
    hostIP: 10.0.90.12
    hostIPs:
    - ip: 10.0.90.12
    initContainerStatuses:
    - containerID: containerd://cec66b2104f78f6ee4827cc731f1cb5525397a0c8c2a022297069bd808233b62
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 4
      started: false
      state:
        terminated:
          containerID: containerd://cec66b2104f78f6ee4827cc731f1cb5525397a0c8c2a022297069bd808233b62
          exitCode: 0
          finishedAt: "2024-09-23T15:59:32Z"
          reason: Completed
          startedAt: "2024-09-23T15:59:32Z"
    - containerID: containerd://3c3de714bd0c11d67a151d7c3cbc02903f94d789d2d6614954329b8a35209ab0
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: activate
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://3c3de714bd0c11d67a151d7c3cbc02903f94d789d2d6614954329b8a35209ab0
          exitCode: 0
          finishedAt: "2024-09-23T15:59:39Z"
          reason: Completed
          startedAt: "2024-09-23T15:59:35Z"
    - containerID: containerd://825c88f4433c9c0a2f6bfd555cd84c2af1aeba6d9c775b80930246dfdd6f1f6b
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: expand-bluefs
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://825c88f4433c9c0a2f6bfd555cd84c2af1aeba6d9c775b80930246dfdd6f1f6b
          exitCode: 0
          finishedAt: "2024-09-23T15:59:46Z"
          reason: Completed
          startedAt: "2024-09-23T15:59:39Z"
    - containerID: containerd://39146449a8bd18df2dcc4a5c6896b3804ab0cb0c3206fe8cfa1b0d5b27f891cb
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://39146449a8bd18df2dcc4a5c6896b3804ab0cb0c3206fe8cfa1b0d5b27f891cb
          exitCode: 0
          finishedAt: "2024-09-23T15:59:46Z"
          reason: Completed
          startedAt: "2024-09-23T15:59:46Z"
    phase: Running
    podIP: 10.42.186.128
    podIPs:
    - ip: 10.42.186.128
    qosClass: Burstable
    startTime: "2024-08-13T03:09:19Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 1e15dcf857274f6c01910a89af3abf6f94159bacafd87dfe5d921628edf65f2c
      cni.projectcalico.org/podIP: 10.42.3.56/32
      cni.projectcalico.org/podIPs: 10.42.3.56/32
      prometheus.io/port: "9102"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-08-19T12:46:35Z"
    generateName: rook-ceph-osd-1-6d9449df7b-
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "1"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: storage
      ceph-osd-id: "1"
      ceph_daemon_id: "1"
      ceph_daemon_type: osd
      device-class: ssd
      failure-domain: k8s-node-blue
      osd: "1"
      osd-store: bluestore
      pod-template-hash: 6d9449df7b
      portable: "false"
      rook.io/operator-namespace: storage
      rook_cluster: storage
      topology-location-host: k8s-node-blue
      topology-location-root: default
    name: rook-ceph-osd-1-6d9449df7b-tgxlz
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-osd-1-6d9449df7b
      uid: 3284cbce-ede0-4382-85b8-d003d56db745
    resourceVersion: "1349044527"
    uid: eae3229d-0f19-4513-9007-3e79c177f1ec
  spec:
    affinity: {}
    containers:
    - args:
      - --foreground
      - --id
      - "1"
      - --fsid
      - 9dff65cc-5b8f-413f-a1ed-8bfb6b93684a
      - --setuser
      - ceph
      - --setgroup
      - ceph
      - --crush-location=root=default host=k8s-node-blue
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --ms-learn-addr-from-peer=false
      command:
      - ceph-osd
      env:
      - name: TZ
        value: Australia/Sydney
      - name: ROOK_NODE_NAME
        value: k8s-node-blue
      - name: ROOK_CLUSTER_ID
        value: 9f4e59e5-14c0-4d39-ac52-c4fcf6bd6c10
      - name: ROOK_CLUSTER_NAME
        value: storage
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: storage
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: k8s-node-blue
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.4
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_OSD_RESTART_INTERVAL
        value: "0"
      - name: ROOK_OSD_UUID
        value: e86cecb6-7073-45d9-9173-c599045572ec
      - name: ROOK_OSD_ID
        value: "1"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sdc
      - name: ROOK_CV_MODE
        value: raw
      - name: ROOK_OSD_DEVICE_CLASS
        value: ssd
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.1.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: osd
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      startupProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.1.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 720
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-1
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jz488
        readOnly: true
      workingDir: /var/log/ceph
    - command:
      - /bin/bash
      - -x
      - -e
      - -m
      - -c
      - "\nCEPH_CLIENT_ID=ceph-osd.1\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
        edit the logrotate file to only rotate a specific daemon log\n# otherwise
        we will logrotate log files without reloading certain daemons\n# this might
        happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
        \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
        --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
        rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed --in-place
        \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif [ \"$LOG_MAX_SIZE\"
        != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE at the 4th line of the
        logrotate config file with 4 spaces to maintain indentation\n\tsed --in-place
        \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\" \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile
        true; do\n\t# we don't force the logrorate but we let the logrotate binary
        handle the rotation based on user's input for periodicity and size\n\tlogrotate
        --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep 15m\ndone\n"
      env:
      - name: TZ
        value: Australia/Sydney
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: log-collector
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jz488
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jz488
        readOnly: true
    - command:
      - /bin/bash
      - -c
      - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables are
        unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=9dff65cc-5b8f-413f-a1ed-8bfb6b93684a\nOSD_UUID=e86cecb6-7073-45d9-9173-c599045572ec\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
        \"ceph.conf\" must have the \"fsid\" global configuration to activate encrypted
        OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
        This limitation will be removed later. After that, we can remove this\n# fsid
        injection code. Probably a good time is when to remove Quincy support.\n#
        https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
        /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
        = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif not
        config.has_section('global'):\n    config['global'] = {}\n\nif not config.has_option('global','fsid'):\n
        \   config['global']['fsid'] = '$CEPH_FSID'\n\nwith open('/etc/ceph/ceph.conf',
        'w') as configfile:\n    config.write(configfile)\n\"\n\n# create new keyring\nceph
        -n client.admin auth get-or-create osd.\"$OSD_ID\" mon 'allow profile osd'
        mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
        \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary directory\n\t#
        this is needed because when the init container exits, the tmpfs goes away
        and its content with it\n\t# this will result in the emptydir to be empty
        when accessed by the main osd container\n\tcp --verbose --no-dereference \"$OSD_DATA_DIR\"/*
        \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't need it anymore\n\tumount
        \"$OSD_DATA_DIR\"\n\n\t# copy back the content of the tmpfs into the original
        osd directory\n\tcp --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t#
        retain ownership of files to the ceph user/group\n\tchown --verbose --recursive
        ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm --recursive
        --force \"$TMP_DIR\"\nelse\n\t# 'ceph-volume raw list' (which the osd-prepare
        job uses to report OSDs on nodes)\n\t#  returns user-friendly device names
        which can change when systems reboot. To\n\t# keep OSD pods from crashing
        repeatedly after a reboot, we need to check if the\n\t# block device we have
        is still correct, and if it isn't correct, we need to\n\t# scan all the disks
        to find the right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device()
        {\n\t\t# jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
        python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
        sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
        == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device: '
        + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
        \ # don't keep processing once the disk is found\nsys.exit('no disk found
        with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
        > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
        '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device < \"$OSD_LIST\";
        then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device
        < \"$OSD_LIST\")\"\n\tfi\n\t[[ -z \"$DEVICE\" ]] && { echo \"no device\" ;
        exit 1 ; }\n\n\t# If a kernel device name change happens and a block device
        file\n\t# in the OSD directory becomes missing, this OSD fails to start\n\t#
        continuously. This problem can be resolved by confirming\n\t# the validity
        of the device file and recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
        [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ; then\n\t\trm
        $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports bluestore
        so we don't need to pass a store flag\n\tceph-volume raw activate --device
        \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
      env:
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sdc
      - name: ROOK_METADATA_DEVICE
      - name: ROOK_WAL_DEVICE
      - name: ROOK_OSD_ID
        value: "1"
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: activate
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-1
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /etc/temp-ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /etc/ceph/admin-keyring-store/
        name: rook-ceph-admin-keyring
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jz488
        readOnly: true
    - args:
      - bluefs-bdev-expand
      - --path
      - /var/lib/ceph/osd/ceph-1
      command:
      - ceph-bluestore-tool
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: expand-bluefs
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-1
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jz488
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-1
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jz488
        readOnly: true
    nodeName: k8s-node-blue
    nodeSelector:
      kubernetes.io/hostname: k8s-node-blue
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    shareProcessNamespace: true
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/storage/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/storage/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - hostPath:
        path: /run/udev
        type: ""
      name: run-udev
    - hostPath:
        path: /var/lib/rook/storage/9dff65cc-5b8f-413f-a1ed-8bfb6b93684a_e86cecb6-7073-45d9-9173-c599045572ec
        type: DirectoryOrCreate
      name: activate-osd
    - name: rook-ceph-admin-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-admin-keyring
    - name: kube-api-access-jz488
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-08-19T23:16:50Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-08-19T12:58:46Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-10-10T22:15:07Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-10-10T22:15:07Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-08-19T12:57:27Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://4d6e50116fc2e961e35b56ea2aa5b44229697f57f938ad503fb5c985cdef3dc2
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState:
        terminated:
          containerID: containerd://22045467cd3e5a27ecf49d2f3cd8af6626d766afa30d2a17002a7100f22ed770
          exitCode: 255
          finishedAt: "2024-08-19T23:16:01Z"
          reason: Unknown
          startedAt: "2024-08-19T12:58:46Z"
      name: log-collector
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2024-08-19T23:17:23Z"
    - containerID: containerd://003f5ecad577a9741b5d7a9bb0b7fa672bf5ba636188b7d75b0d75757f031f11
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState:
        terminated:
          containerID: containerd://442adc6eb7ab58ddcc3d9269b25488c8477d56c7cad17ee22cfd075361f8abad
          exitCode: 255
          finishedAt: "2024-08-19T23:16:01Z"
          reason: Unknown
          startedAt: "2024-08-19T12:58:46Z"
      name: osd
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2024-08-19T23:17:23Z"
    hostIP: 10.0.90.10
    hostIPs:
    - ip: 10.0.90.10
    initContainerStatuses:
    - containerID: containerd://69125d768852f3b650ca9c27e9e2f3eb8d3f6d6e7432b847bb9f7247f76399cd
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 1
      started: false
      state:
        terminated:
          containerID: containerd://69125d768852f3b650ca9c27e9e2f3eb8d3f6d6e7432b847bb9f7247f76399cd
          exitCode: 0
          finishedAt: "2024-08-19T23:16:50Z"
          reason: Completed
          startedAt: "2024-08-19T23:16:50Z"
    - containerID: containerd://68b5af320a8ef40cbbb7cec4d2aab47610cd4409141557f84c7dc33991ad74f1
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: activate
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://68b5af320a8ef40cbbb7cec4d2aab47610cd4409141557f84c7dc33991ad74f1
          exitCode: 0
          finishedAt: "2024-08-19T23:17:08Z"
          reason: Completed
          startedAt: "2024-08-19T23:16:51Z"
    - containerID: containerd://668fdd418d87e7e23bb79a50f10f7e070326ac61eda068f4ff7c3d2826be8965
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: expand-bluefs
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://668fdd418d87e7e23bb79a50f10f7e070326ac61eda068f4ff7c3d2826be8965
          exitCode: 0
          finishedAt: "2024-08-19T23:17:22Z"
          reason: Completed
          startedAt: "2024-08-19T23:17:09Z"
    - containerID: containerd://7a68889db01a55abed04b4a5a3e8f244b2a805957f31fc6201bff2cc5ae185bc
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://7a68889db01a55abed04b4a5a3e8f244b2a805957f31fc6201bff2cc5ae185bc
          exitCode: 0
          finishedAt: "2024-08-19T23:17:22Z"
          reason: Completed
          startedAt: "2024-08-19T23:17:22Z"
    phase: Running
    podIP: 10.42.3.56
    podIPs:
    - ip: 10.42.3.56
    qosClass: Burstable
    startTime: "2024-08-19T12:57:28Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 102cf51c862e1f1e3a98ba10243bd13ff7be8f6bf9700a1d27c281535e63ff53
      cni.projectcalico.org/podIP: 10.42.186.176/32
      cni.projectcalico.org/podIPs: 10.42.186.176/32
      prometheus.io/port: "9102"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-08-13T03:10:39Z"
    generateName: rook-ceph-osd-2-5b697b8f76-
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "2"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: storage
      ceph-osd-id: "2"
      ceph_daemon_id: "2"
      ceph_daemon_type: osd
      device-class: hdd
      failure-domain: k8s-node-orange
      osd: "2"
      osd-store: bluestore
      pod-template-hash: 5b697b8f76
      portable: "false"
      rook.io/operator-namespace: storage
      rook_cluster: storage
      topology-location-host: k8s-node-orange
      topology-location-root: default
    name: rook-ceph-osd-2-5b697b8f76-jjl86
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-osd-2-5b697b8f76
      uid: e90c386b-f43b-4b4e-8f38-a56e1dc5b477
    resourceVersion: "1346579958"
    uid: 05ff4234-07f9-401c-ae69-faea9f801ab4
  spec:
    affinity: {}
    containers:
    - args:
      - --foreground
      - --id
      - "2"
      - --fsid
      - 9dff65cc-5b8f-413f-a1ed-8bfb6b93684a
      - --setuser
      - ceph
      - --setgroup
      - ceph
      - --crush-location=root=default host=k8s-node-orange
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --ms-learn-addr-from-peer=false
      command:
      - ceph-osd
      env:
      - name: TZ
        value: Australia/Sydney
      - name: ROOK_NODE_NAME
        value: k8s-node-orange
      - name: ROOK_CLUSTER_ID
        value: 9f4e59e5-14c0-4d39-ac52-c4fcf6bd6c10
      - name: ROOK_CLUSTER_NAME
        value: storage
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: storage
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: k8s-node-orange
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.4
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_OSD_RESTART_INTERVAL
        value: "0"
      - name: ROOK_OSD_UUID
        value: 843fddd0-103a-491c-992a-43a322c2a381
      - name: ROOK_OSD_ID
        value: "2"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sda
      - name: ROOK_CV_MODE
        value: raw
      - name: ROOK_OSD_DEVICE_CLASS
        value: hdd
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.2.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: osd
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      startupProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.2.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 720
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-2
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bfbkd
        readOnly: true
      workingDir: /var/log/ceph
    - command:
      - /bin/bash
      - -x
      - -e
      - -m
      - -c
      - "\nCEPH_CLIENT_ID=ceph-osd.2\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
        edit the logrotate file to only rotate a specific daemon log\n# otherwise
        we will logrotate log files without reloading certain daemons\n# this might
        happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
        \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
        --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
        rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed --in-place
        \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif [ \"$LOG_MAX_SIZE\"
        != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE at the 4th line of the
        logrotate config file with 4 spaces to maintain indentation\n\tsed --in-place
        \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\" \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile
        true; do\n\t# we don't force the logrorate but we let the logrotate binary
        handle the rotation based on user's input for periodicity and size\n\tlogrotate
        --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep 15m\ndone\n"
      env:
      - name: TZ
        value: Australia/Sydney
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: log-collector
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bfbkd
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bfbkd
        readOnly: true
    - command:
      - /bin/bash
      - -c
      - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables are
        unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=9dff65cc-5b8f-413f-a1ed-8bfb6b93684a\nOSD_UUID=843fddd0-103a-491c-992a-43a322c2a381\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
        \"ceph.conf\" must have the \"fsid\" global configuration to activate encrypted
        OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
        This limitation will be removed later. After that, we can remove this\n# fsid
        injection code. Probably a good time is when to remove Quincy support.\n#
        https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
        /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
        = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif not
        config.has_section('global'):\n    config['global'] = {}\n\nif not config.has_option('global','fsid'):\n
        \   config['global']['fsid'] = '$CEPH_FSID'\n\nwith open('/etc/ceph/ceph.conf',
        'w') as configfile:\n    config.write(configfile)\n\"\n\n# create new keyring\nceph
        -n client.admin auth get-or-create osd.\"$OSD_ID\" mon 'allow profile osd'
        mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
        \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary directory\n\t#
        this is needed because when the init container exits, the tmpfs goes away
        and its content with it\n\t# this will result in the emptydir to be empty
        when accessed by the main osd container\n\tcp --verbose --no-dereference \"$OSD_DATA_DIR\"/*
        \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't need it anymore\n\tumount
        \"$OSD_DATA_DIR\"\n\n\t# copy back the content of the tmpfs into the original
        osd directory\n\tcp --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t#
        retain ownership of files to the ceph user/group\n\tchown --verbose --recursive
        ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm --recursive
        --force \"$TMP_DIR\"\nelse\n\t# 'ceph-volume raw list' (which the osd-prepare
        job uses to report OSDs on nodes)\n\t#  returns user-friendly device names
        which can change when systems reboot. To\n\t# keep OSD pods from crashing
        repeatedly after a reboot, we need to check if the\n\t# block device we have
        is still correct, and if it isn't correct, we need to\n\t# scan all the disks
        to find the right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device()
        {\n\t\t# jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
        python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
        sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
        == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device: '
        + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
        \ # don't keep processing once the disk is found\nsys.exit('no disk found
        with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
        > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
        '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device < \"$OSD_LIST\";
        then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device
        < \"$OSD_LIST\")\"\n\tfi\n\t[[ -z \"$DEVICE\" ]] && { echo \"no device\" ;
        exit 1 ; }\n\n\t# If a kernel device name change happens and a block device
        file\n\t# in the OSD directory becomes missing, this OSD fails to start\n\t#
        continuously. This problem can be resolved by confirming\n\t# the validity
        of the device file and recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
        [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ; then\n\t\trm
        $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports bluestore
        so we don't need to pass a store flag\n\tceph-volume raw activate --device
        \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
      env:
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sda
      - name: ROOK_METADATA_DEVICE
      - name: ROOK_WAL_DEVICE
      - name: ROOK_OSD_ID
        value: "2"
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: activate
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-2
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /etc/temp-ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /etc/ceph/admin-keyring-store/
        name: rook-ceph-admin-keyring
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bfbkd
        readOnly: true
    - args:
      - bluefs-bdev-expand
      - --path
      - /var/lib/ceph/osd/ceph-2
      command:
      - ceph-bluestore-tool
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: expand-bluefs
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-2
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bfbkd
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-2
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bfbkd
        readOnly: true
    nodeName: k8s-node-orange
    nodeSelector:
      kubernetes.io/hostname: k8s-node-orange
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    shareProcessNamespace: true
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/storage/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/storage/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - hostPath:
        path: /run/udev
        type: ""
      name: run-udev
    - hostPath:
        path: /var/lib/rook/storage/9dff65cc-5b8f-413f-a1ed-8bfb6b93684a_843fddd0-103a-491c-992a-43a322c2a381
        type: DirectoryOrCreate
      name: activate-osd
    - name: rook-ceph-admin-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-admin-keyring
    - name: kube-api-access-bfbkd
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:59:34Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-08-13T03:11:07Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-10-08T13:22:36Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-10-08T13:22:36Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-08-13T03:10:39Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://8193f6dcbe87916a2fd2ea6b2d922764cdb0fd5a2a36a4c2e18ef3a11a62bb2e
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState:
        terminated:
          containerID: containerd://087fa33ddc6c95b2644c7f715de8ed4daeffc7a9e858da4c3d52800fc04d7eef
          exitCode: 255
          finishedAt: "2024-09-23T15:59:18Z"
          reason: Unknown
          startedAt: "2024-09-23T15:48:29Z"
      name: log-collector
      ready: true
      restartCount: 4
      started: true
      state:
        running:
          startedAt: "2024-09-23T16:00:00Z"
    - containerID: containerd://08e484d892d205406b810669f8c9838f9488ae387f2f6bab690087e8d34420cb
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState:
        terminated:
          containerID: containerd://d1455d7bf81aa87c5e3114a1d3ab6837d4936f1d480c3c35ade255f2c36c3ea8
          exitCode: 134
          finishedAt: "2024-10-08T13:21:50Z"
          reason: Error
          startedAt: "2024-09-23T16:00:00Z"
      name: osd
      ready: true
      restartCount: 5
      started: true
      state:
        running:
          startedAt: "2024-10-08T13:21:50Z"
    hostIP: 10.0.90.12
    hostIPs:
    - ip: 10.0.90.12
    initContainerStatuses:
    - containerID: containerd://d01de63e00de8acca79316aa99fe022d48ccba2e1ca0213aa8d48c38e133a746
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 4
      started: false
      state:
        terminated:
          containerID: containerd://d01de63e00de8acca79316aa99fe022d48ccba2e1ca0213aa8d48c38e133a746
          exitCode: 0
          finishedAt: "2024-09-23T15:59:34Z"
          reason: Completed
          startedAt: "2024-09-23T15:59:34Z"
    - containerID: containerd://8c5d8b462dc4788ac34e9b012475373c9e78700f74e0bcb84f625eda7218ebac
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: activate
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://8c5d8b462dc4788ac34e9b012475373c9e78700f74e0bcb84f625eda7218ebac
          exitCode: 0
          finishedAt: "2024-09-23T15:59:39Z"
          reason: Completed
          startedAt: "2024-09-23T15:59:37Z"
    - containerID: containerd://a028000526d5836e2fd3c4ad0e71bf25d6ce6e033fe24611faaab87bc4c09a9d
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: expand-bluefs
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://a028000526d5836e2fd3c4ad0e71bf25d6ce6e033fe24611faaab87bc4c09a9d
          exitCode: 0
          finishedAt: "2024-09-23T15:59:59Z"
          reason: Completed
          startedAt: "2024-09-23T15:59:39Z"
    - containerID: containerd://efe4ea1f0006c2ad18be674f2e2177992cd40e1918fc4d006c36eca87543e6db
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://efe4ea1f0006c2ad18be674f2e2177992cd40e1918fc4d006c36eca87543e6db
          exitCode: 0
          finishedAt: "2024-09-23T15:59:59Z"
          reason: Completed
          startedAt: "2024-09-23T15:59:59Z"
    phase: Running
    podIP: 10.42.186.176
    podIPs:
    - ip: 10.42.186.176
    qosClass: Burstable
    startTime: "2024-08-13T03:10:39Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: f6e198bdedc88e7aedb277c4d64674aadbba2c4934dd7a878ccc903dc311a3b7
      cni.projectcalico.org/podIP: 10.42.3.61/32
      cni.projectcalico.org/podIPs: 10.42.3.61/32
      prometheus.io/port: "9102"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-08-19T12:46:01Z"
    generateName: rook-ceph-osd-3-6ff6bf9b6c-
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "3"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: storage
      ceph-osd-id: "3"
      ceph_daemon_id: "3"
      ceph_daemon_type: osd
      device-class: hdd
      failure-domain: k8s-node-blue
      osd: "3"
      osd-store: bluestore
      pod-template-hash: 6ff6bf9b6c
      portable: "false"
      rook.io/operator-namespace: storage
      rook_cluster: storage
      topology-location-host: k8s-node-blue
      topology-location-root: default
    name: rook-ceph-osd-3-6ff6bf9b6c-9mc46
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-osd-3-6ff6bf9b6c
      uid: f63a0382-0631-4b72-88fc-1252dfa9ccab
    resourceVersion: "1349044684"
    uid: f9c52c2c-07fe-48e4-af8d-76b332df9aa1
  spec:
    affinity: {}
    containers:
    - args:
      - --foreground
      - --id
      - "3"
      - --fsid
      - 9dff65cc-5b8f-413f-a1ed-8bfb6b93684a
      - --setuser
      - ceph
      - --setgroup
      - ceph
      - --crush-location=root=default host=k8s-node-blue
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --ms-learn-addr-from-peer=false
      command:
      - ceph-osd
      env:
      - name: TZ
        value: Australia/Sydney
      - name: ROOK_NODE_NAME
        value: k8s-node-blue
      - name: ROOK_CLUSTER_ID
        value: 9f4e59e5-14c0-4d39-ac52-c4fcf6bd6c10
      - name: ROOK_CLUSTER_NAME
        value: storage
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: storage
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: k8s-node-blue
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.4
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_OSD_RESTART_INTERVAL
        value: "0"
      - name: ROOK_OSD_UUID
        value: 7cda2bdb-9f22-44f8-a723-578b5f7b07f5
      - name: ROOK_OSD_ID
        value: "3"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sda
      - name: ROOK_CV_MODE
        value: raw
      - name: ROOK_OSD_DEVICE_CLASS
        value: hdd
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.3.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: osd
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      startupProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.3.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 720
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-3
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2hw2b
        readOnly: true
      workingDir: /var/log/ceph
    - command:
      - /bin/bash
      - -x
      - -e
      - -m
      - -c
      - "\nCEPH_CLIENT_ID=ceph-osd.3\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
        edit the logrotate file to only rotate a specific daemon log\n# otherwise
        we will logrotate log files without reloading certain daemons\n# this might
        happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
        \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
        --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
        rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed --in-place
        \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif [ \"$LOG_MAX_SIZE\"
        != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE at the 4th line of the
        logrotate config file with 4 spaces to maintain indentation\n\tsed --in-place
        \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\" \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile
        true; do\n\t# we don't force the logrorate but we let the logrotate binary
        handle the rotation based on user's input for periodicity and size\n\tlogrotate
        --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep 15m\ndone\n"
      env:
      - name: TZ
        value: Australia/Sydney
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: log-collector
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2hw2b
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2hw2b
        readOnly: true
    - command:
      - /bin/bash
      - -c
      - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables are
        unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=9dff65cc-5b8f-413f-a1ed-8bfb6b93684a\nOSD_UUID=7cda2bdb-9f22-44f8-a723-578b5f7b07f5\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
        \"ceph.conf\" must have the \"fsid\" global configuration to activate encrypted
        OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
        This limitation will be removed later. After that, we can remove this\n# fsid
        injection code. Probably a good time is when to remove Quincy support.\n#
        https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
        /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
        = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif not
        config.has_section('global'):\n    config['global'] = {}\n\nif not config.has_option('global','fsid'):\n
        \   config['global']['fsid'] = '$CEPH_FSID'\n\nwith open('/etc/ceph/ceph.conf',
        'w') as configfile:\n    config.write(configfile)\n\"\n\n# create new keyring\nceph
        -n client.admin auth get-or-create osd.\"$OSD_ID\" mon 'allow profile osd'
        mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
        \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary directory\n\t#
        this is needed because when the init container exits, the tmpfs goes away
        and its content with it\n\t# this will result in the emptydir to be empty
        when accessed by the main osd container\n\tcp --verbose --no-dereference \"$OSD_DATA_DIR\"/*
        \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't need it anymore\n\tumount
        \"$OSD_DATA_DIR\"\n\n\t# copy back the content of the tmpfs into the original
        osd directory\n\tcp --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t#
        retain ownership of files to the ceph user/group\n\tchown --verbose --recursive
        ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm --recursive
        --force \"$TMP_DIR\"\nelse\n\t# 'ceph-volume raw list' (which the osd-prepare
        job uses to report OSDs on nodes)\n\t#  returns user-friendly device names
        which can change when systems reboot. To\n\t# keep OSD pods from crashing
        repeatedly after a reboot, we need to check if the\n\t# block device we have
        is still correct, and if it isn't correct, we need to\n\t# scan all the disks
        to find the right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device()
        {\n\t\t# jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
        python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
        sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
        == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device: '
        + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
        \ # don't keep processing once the disk is found\nsys.exit('no disk found
        with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
        > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
        '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device < \"$OSD_LIST\";
        then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device
        < \"$OSD_LIST\")\"\n\tfi\n\t[[ -z \"$DEVICE\" ]] && { echo \"no device\" ;
        exit 1 ; }\n\n\t# If a kernel device name change happens and a block device
        file\n\t# in the OSD directory becomes missing, this OSD fails to start\n\t#
        continuously. This problem can be resolved by confirming\n\t# the validity
        of the device file and recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
        [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ; then\n\t\trm
        $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports bluestore
        so we don't need to pass a store flag\n\tceph-volume raw activate --device
        \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
      env:
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sda
      - name: ROOK_METADATA_DEVICE
      - name: ROOK_WAL_DEVICE
      - name: ROOK_OSD_ID
        value: "3"
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: activate
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-3
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /etc/temp-ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /etc/ceph/admin-keyring-store/
        name: rook-ceph-admin-keyring
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2hw2b
        readOnly: true
    - args:
      - bluefs-bdev-expand
      - --path
      - /var/lib/ceph/osd/ceph-3
      command:
      - ceph-bluestore-tool
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: expand-bluefs
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-3
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2hw2b
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-3
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2hw2b
        readOnly: true
    nodeName: k8s-node-blue
    nodeSelector:
      kubernetes.io/hostname: k8s-node-blue
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    shareProcessNamespace: true
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/storage/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/storage/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - hostPath:
        path: /run/udev
        type: ""
      name: run-udev
    - hostPath:
        path: /var/lib/rook/storage/9dff65cc-5b8f-413f-a1ed-8bfb6b93684a_7cda2bdb-9f22-44f8-a723-578b5f7b07f5
        type: DirectoryOrCreate
      name: activate-osd
    - name: rook-ceph-admin-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-admin-keyring
    - name: kube-api-access-2hw2b
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-08-19T23:16:50Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-08-19T12:58:53Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-10-10T22:15:12Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-10-10T22:15:12Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-08-19T12:57:27Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://6413d562493ba8fb5d34f20f994612aea3c09b2d1cc3ceb284a651c3fd56b2d5
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState:
        terminated:
          containerID: containerd://a1a4ec38e3b8bb055718ab31c8662e3c90174b8e6a14411b0687ba09a2cf9f11
          exitCode: 255
          finishedAt: "2024-08-19T23:16:01Z"
          reason: Unknown
          startedAt: "2024-08-19T12:58:53Z"
      name: log-collector
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2024-08-19T23:17:23Z"
    - containerID: containerd://841410df1416188e2724e56bf13835b9df7095d8caab4a67c9abc8a8e40f9dad
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState:
        terminated:
          containerID: containerd://9924e04726136db5e116664f906107f9161668e5605f05eb8e500ab40da528b8
          exitCode: 255
          finishedAt: "2024-08-19T23:16:01Z"
          reason: Unknown
          startedAt: "2024-08-19T12:58:53Z"
      name: osd
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2024-08-19T23:17:23Z"
    hostIP: 10.0.90.10
    hostIPs:
    - ip: 10.0.90.10
    initContainerStatuses:
    - containerID: containerd://82b6528fd2d0545d040b37cc90d5b8722f74515769eeb67d49a8601323184e7d
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 1
      started: false
      state:
        terminated:
          containerID: containerd://82b6528fd2d0545d040b37cc90d5b8722f74515769eeb67d49a8601323184e7d
          exitCode: 0
          finishedAt: "2024-08-19T23:16:50Z"
          reason: Completed
          startedAt: "2024-08-19T23:16:50Z"
    - containerID: containerd://891680b49b54e9f189b8cc732c742208dfb4da52c5e97f29d57f4bae31614651
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: activate
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://891680b49b54e9f189b8cc732c742208dfb4da52c5e97f29d57f4bae31614651
          exitCode: 0
          finishedAt: "2024-08-19T23:17:08Z"
          reason: Completed
          startedAt: "2024-08-19T23:16:50Z"
    - containerID: containerd://39997b0f10adba6a66357538081025e06dfb547052183447f026f87920eab160
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: expand-bluefs
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://39997b0f10adba6a66357538081025e06dfb547052183447f026f87920eab160
          exitCode: 0
          finishedAt: "2024-08-19T23:17:21Z"
          reason: Completed
          startedAt: "2024-08-19T23:17:09Z"
    - containerID: containerd://4cfdf70eec9fee285723992a69e28287fd9b980464244939c1e306cb692184b4
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://4cfdf70eec9fee285723992a69e28287fd9b980464244939c1e306cb692184b4
          exitCode: 0
          finishedAt: "2024-08-19T23:17:22Z"
          reason: Completed
          startedAt: "2024-08-19T23:17:22Z"
    phase: Running
    podIP: 10.42.3.61
    podIPs:
    - ip: 10.42.3.61
    qosClass: Burstable
    startTime: "2024-08-19T12:57:28Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: d7ce947a38a2c9338dd897b1cca20aff4b6eeb1f0572e5801e9eec7286abb2fc
      cni.projectcalico.org/podIP: 10.42.186.163/32
      cni.projectcalico.org/podIPs: 10.42.186.163/32
      prometheus.io/port: "9102"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-08-13T03:13:06Z"
    generateName: rook-ceph-osd-4-66b56f6d49-
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "4"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: storage
      ceph-osd-id: "4"
      ceph_daemon_id: "4"
      ceph_daemon_type: osd
      device-class: ssd
      failure-domain: k8s-node-orange
      osd: "4"
      osd-store: bluestore
      pod-template-hash: 66b56f6d49
      portable: "false"
      rook.io/operator-namespace: storage
      rook_cluster: storage
      topology-location-host: k8s-node-orange
      topology-location-root: default
    name: rook-ceph-osd-4-66b56f6d49-xdfgj
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-osd-4-66b56f6d49
      uid: c5a11282-b61b-47ae-a07b-fdcbc614a558
    resourceVersion: "1330835618"
    uid: 9497392d-fda4-4fff-ab32-3809faa9cb10
  spec:
    affinity: {}
    containers:
    - args:
      - --foreground
      - --id
      - "4"
      - --fsid
      - 9dff65cc-5b8f-413f-a1ed-8bfb6b93684a
      - --setuser
      - ceph
      - --setgroup
      - ceph
      - --crush-location=root=default host=k8s-node-orange
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --ms-learn-addr-from-peer=false
      command:
      - ceph-osd
      env:
      - name: TZ
        value: Australia/Sydney
      - name: ROOK_NODE_NAME
        value: k8s-node-orange
      - name: ROOK_CLUSTER_ID
        value: 9f4e59e5-14c0-4d39-ac52-c4fcf6bd6c10
      - name: ROOK_CLUSTER_NAME
        value: storage
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: storage
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: k8s-node-orange
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.4
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_OSD_RESTART_INTERVAL
        value: "0"
      - name: ROOK_OSD_UUID
        value: a0e357d2-fdbe-4e2f-abad-ddaec726749c
      - name: ROOK_OSD_ID
        value: "4"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sdc
      - name: ROOK_CV_MODE
        value: raw
      - name: ROOK_OSD_DEVICE_CLASS
        value: ssd
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.4.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: osd
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      startupProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.4.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 720
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-4
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dx5jx
        readOnly: true
      workingDir: /var/log/ceph
    - command:
      - /bin/bash
      - -x
      - -e
      - -m
      - -c
      - "\nCEPH_CLIENT_ID=ceph-osd.4\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
        edit the logrotate file to only rotate a specific daemon log\n# otherwise
        we will logrotate log files without reloading certain daemons\n# this might
        happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
        \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
        --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
        rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed --in-place
        \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif [ \"$LOG_MAX_SIZE\"
        != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE at the 4th line of the
        logrotate config file with 4 spaces to maintain indentation\n\tsed --in-place
        \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\" \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile
        true; do\n\t# we don't force the logrorate but we let the logrotate binary
        handle the rotation based on user's input for periodicity and size\n\tlogrotate
        --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep 15m\ndone\n"
      env:
      - name: TZ
        value: Australia/Sydney
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: log-collector
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dx5jx
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dx5jx
        readOnly: true
    - command:
      - /bin/bash
      - -c
      - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables are
        unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=9dff65cc-5b8f-413f-a1ed-8bfb6b93684a\nOSD_UUID=a0e357d2-fdbe-4e2f-abad-ddaec726749c\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
        \"ceph.conf\" must have the \"fsid\" global configuration to activate encrypted
        OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
        This limitation will be removed later. After that, we can remove this\n# fsid
        injection code. Probably a good time is when to remove Quincy support.\n#
        https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
        /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
        = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif not
        config.has_section('global'):\n    config['global'] = {}\n\nif not config.has_option('global','fsid'):\n
        \   config['global']['fsid'] = '$CEPH_FSID'\n\nwith open('/etc/ceph/ceph.conf',
        'w') as configfile:\n    config.write(configfile)\n\"\n\n# create new keyring\nceph
        -n client.admin auth get-or-create osd.\"$OSD_ID\" mon 'allow profile osd'
        mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
        \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary directory\n\t#
        this is needed because when the init container exits, the tmpfs goes away
        and its content with it\n\t# this will result in the emptydir to be empty
        when accessed by the main osd container\n\tcp --verbose --no-dereference \"$OSD_DATA_DIR\"/*
        \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't need it anymore\n\tumount
        \"$OSD_DATA_DIR\"\n\n\t# copy back the content of the tmpfs into the original
        osd directory\n\tcp --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t#
        retain ownership of files to the ceph user/group\n\tchown --verbose --recursive
        ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm --recursive
        --force \"$TMP_DIR\"\nelse\n\t# 'ceph-volume raw list' (which the osd-prepare
        job uses to report OSDs on nodes)\n\t#  returns user-friendly device names
        which can change when systems reboot. To\n\t# keep OSD pods from crashing
        repeatedly after a reboot, we need to check if the\n\t# block device we have
        is still correct, and if it isn't correct, we need to\n\t# scan all the disks
        to find the right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device()
        {\n\t\t# jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
        python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
        sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
        == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device: '
        + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
        \ # don't keep processing once the disk is found\nsys.exit('no disk found
        with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
        > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
        '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device < \"$OSD_LIST\";
        then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device
        < \"$OSD_LIST\")\"\n\tfi\n\t[[ -z \"$DEVICE\" ]] && { echo \"no device\" ;
        exit 1 ; }\n\n\t# If a kernel device name change happens and a block device
        file\n\t# in the OSD directory becomes missing, this OSD fails to start\n\t#
        continuously. This problem can be resolved by confirming\n\t# the validity
        of the device file and recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
        [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ; then\n\t\trm
        $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports bluestore
        so we don't need to pass a store flag\n\tceph-volume raw activate --device
        \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
      env:
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sdc
      - name: ROOK_METADATA_DEVICE
      - name: ROOK_WAL_DEVICE
      - name: ROOK_OSD_ID
        value: "4"
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: activate
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-4
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /etc/temp-ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /etc/ceph/admin-keyring-store/
        name: rook-ceph-admin-keyring
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dx5jx
        readOnly: true
    - args:
      - bluefs-bdev-expand
      - --path
      - /var/lib/ceph/osd/ceph-4
      command:
      - ceph-bluestore-tool
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: expand-bluefs
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-4
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dx5jx
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-4
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dx5jx
        readOnly: true
    nodeName: k8s-node-orange
    nodeSelector:
      kubernetes.io/hostname: k8s-node-orange
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    shareProcessNamespace: true
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/storage/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/storage/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - hostPath:
        path: /run/udev
        type: ""
      name: run-udev
    - hostPath:
        path: /var/lib/rook/storage/9dff65cc-5b8f-413f-a1ed-8bfb6b93684a_a0e357d2-fdbe-4e2f-abad-ddaec726749c
        type: DirectoryOrCreate
      name: activate-osd
    - name: rook-ceph-admin-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-admin-keyring
    - name: kube-api-access-dx5jx
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:59:28Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-08-13T03:13:17Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T16:00:22Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T16:00:22Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-08-13T03:13:06Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://718f5f292e891a84ffb66555180f28ff995cfb00f10e4ed3640b6033e02703a2
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState:
        terminated:
          containerID: containerd://9a21df674845f58f76048ac4119a93b61b8eb3965888c6b8a1b2383917aca8e8
          exitCode: 255
          finishedAt: "2024-09-23T15:59:18Z"
          reason: Unknown
          startedAt: "2024-09-23T15:48:25Z"
      name: log-collector
      ready: true
      restartCount: 4
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:59:56Z"
    - containerID: containerd://35a707873530a21d0a9b0917aea6f2fa78e4341727bc1c2ef4a431907e349309
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState:
        terminated:
          containerID: containerd://99a5d767bb469c64039c612535ab3537b6cc2c86e29e05d6ae7371e3d6cc8123
          exitCode: 255
          finishedAt: "2024-09-23T15:59:18Z"
          reason: Unknown
          startedAt: "2024-09-23T15:48:24Z"
      name: osd
      ready: true
      restartCount: 4
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:59:56Z"
    hostIP: 10.0.90.12
    hostIPs:
    - ip: 10.0.90.12
    initContainerStatuses:
    - containerID: containerd://2d84a90b9783a6a19a5e1733c48404ed971e391b4327f0f4671391453e533309
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 4
      started: false
      state:
        terminated:
          containerID: containerd://2d84a90b9783a6a19a5e1733c48404ed971e391b4327f0f4671391453e533309
          exitCode: 0
          finishedAt: "2024-09-23T15:59:29Z"
          reason: Completed
          startedAt: "2024-09-23T15:59:28Z"
    - containerID: containerd://cc31e4d3e650ee16bc2b525c0615948a857c677647068df79918560498d6d84d
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: activate
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://cc31e4d3e650ee16bc2b525c0615948a857c677647068df79918560498d6d84d
          exitCode: 0
          finishedAt: "2024-09-23T15:59:39Z"
          reason: Completed
          startedAt: "2024-09-23T15:59:32Z"
    - containerID: containerd://d6e02999b468c47332b01f8d3cd9ce472c986e1db9dbdcefeb7d4a7436d7d61f
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: expand-bluefs
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://d6e02999b468c47332b01f8d3cd9ce472c986e1db9dbdcefeb7d4a7436d7d61f
          exitCode: 0
          finishedAt: "2024-09-23T15:59:55Z"
          reason: Completed
          startedAt: "2024-09-23T15:59:39Z"
    - containerID: containerd://7e342606fa19962c3b94267b0432e1c3980b7f02a79a6d8f51d3528e872f5075
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://7e342606fa19962c3b94267b0432e1c3980b7f02a79a6d8f51d3528e872f5075
          exitCode: 0
          finishedAt: "2024-09-23T15:59:55Z"
          reason: Completed
          startedAt: "2024-09-23T15:59:55Z"
    phase: Running
    podIP: 10.42.186.163
    podIPs:
    - ip: 10.42.186.163
    qosClass: Burstable
    startTime: "2024-08-13T03:13:06Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 557968ab88127f9a2fea7f4588f91486952e0de643bc58b0b80bb5bf65a0b62c
      cni.projectcalico.org/podIP: 10.42.205.123/32
      cni.projectcalico.org/podIPs: 10.42.205.123/32
      prometheus.io/port: "9102"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-09-23T15:32:45Z"
    generateName: rook-ceph-osd-5-5c8c84d49b-
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "5"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: storage
      ceph-osd-id: "5"
      ceph_daemon_id: "5"
      ceph_daemon_type: osd
      device-class: hdd
      failure-domain: k8s-node-yellow
      osd: "5"
      osd-store: bluestore
      pod-template-hash: 5c8c84d49b
      portable: "false"
      rook.io/operator-namespace: storage
      rook_cluster: storage
      topology-location-host: k8s-node-yellow
      topology-location-root: default
    name: rook-ceph-osd-5-5c8c84d49b-s8njl
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-osd-5-5c8c84d49b
      uid: 55c2a415-5320-4575-8dd7-7b86e5957fb6
    resourceVersion: "1330808118"
    uid: 915fdc46-50f5-4341-a569-c3b738b10d73
  spec:
    affinity: {}
    containers:
    - args:
      - --foreground
      - --id
      - "5"
      - --fsid
      - 9dff65cc-5b8f-413f-a1ed-8bfb6b93684a
      - --setuser
      - ceph
      - --setgroup
      - ceph
      - --crush-location=root=default host=k8s-node-yellow
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --ms-learn-addr-from-peer=false
      command:
      - ceph-osd
      env:
      - name: TZ
        value: Australia/Sydney
      - name: ROOK_NODE_NAME
        value: k8s-node-yellow
      - name: ROOK_CLUSTER_ID
        value: 9f4e59e5-14c0-4d39-ac52-c4fcf6bd6c10
      - name: ROOK_CLUSTER_NAME
        value: storage
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: storage
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: k8s-node-yellow
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.4
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_OSD_RESTART_INTERVAL
        value: "0"
      - name: ROOK_OSD_UUID
        value: dc89a37c-418e-438d-b392-1f22ba9074ef
      - name: ROOK_OSD_ID
        value: "5"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sdd
      - name: ROOK_CV_MODE
        value: raw
      - name: ROOK_OSD_DEVICE_CLASS
        value: hdd
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.5.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: osd
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      startupProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.5.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 720
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-5
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7wfsz
        readOnly: true
      workingDir: /var/log/ceph
    - command:
      - /bin/bash
      - -x
      - -e
      - -m
      - -c
      - "\nCEPH_CLIENT_ID=ceph-osd.5\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
        edit the logrotate file to only rotate a specific daemon log\n# otherwise
        we will logrotate log files without reloading certain daemons\n# this might
        happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
        \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
        --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
        rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed --in-place
        \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif [ \"$LOG_MAX_SIZE\"
        != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE at the 4th line of the
        logrotate config file with 4 spaces to maintain indentation\n\tsed --in-place
        \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\" \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile
        true; do\n\t# we don't force the logrorate but we let the logrotate binary
        handle the rotation based on user's input for periodicity and size\n\tlogrotate
        --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep 15m\ndone\n"
      env:
      - name: TZ
        value: Australia/Sydney
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: log-collector
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7wfsz
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7wfsz
        readOnly: true
    - command:
      - /bin/bash
      - -c
      - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables are
        unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=9dff65cc-5b8f-413f-a1ed-8bfb6b93684a\nOSD_UUID=dc89a37c-418e-438d-b392-1f22ba9074ef\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
        \"ceph.conf\" must have the \"fsid\" global configuration to activate encrypted
        OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
        This limitation will be removed later. After that, we can remove this\n# fsid
        injection code. Probably a good time is when to remove Quincy support.\n#
        https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
        /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
        = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif not
        config.has_section('global'):\n    config['global'] = {}\n\nif not config.has_option('global','fsid'):\n
        \   config['global']['fsid'] = '$CEPH_FSID'\n\nwith open('/etc/ceph/ceph.conf',
        'w') as configfile:\n    config.write(configfile)\n\"\n\n# create new keyring\nceph
        -n client.admin auth get-or-create osd.\"$OSD_ID\" mon 'allow profile osd'
        mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
        \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary directory\n\t#
        this is needed because when the init container exits, the tmpfs goes away
        and its content with it\n\t# this will result in the emptydir to be empty
        when accessed by the main osd container\n\tcp --verbose --no-dereference \"$OSD_DATA_DIR\"/*
        \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't need it anymore\n\tumount
        \"$OSD_DATA_DIR\"\n\n\t# copy back the content of the tmpfs into the original
        osd directory\n\tcp --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t#
        retain ownership of files to the ceph user/group\n\tchown --verbose --recursive
        ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm --recursive
        --force \"$TMP_DIR\"\nelse\n\t# 'ceph-volume raw list' (which the osd-prepare
        job uses to report OSDs on nodes)\n\t#  returns user-friendly device names
        which can change when systems reboot. To\n\t# keep OSD pods from crashing
        repeatedly after a reboot, we need to check if the\n\t# block device we have
        is still correct, and if it isn't correct, we need to\n\t# scan all the disks
        to find the right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device()
        {\n\t\t# jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
        python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
        sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
        == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device: '
        + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
        \ # don't keep processing once the disk is found\nsys.exit('no disk found
        with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
        > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
        '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device < \"$OSD_LIST\";
        then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device
        < \"$OSD_LIST\")\"\n\tfi\n\t[[ -z \"$DEVICE\" ]] && { echo \"no device\" ;
        exit 1 ; }\n\n\t# If a kernel device name change happens and a block device
        file\n\t# in the OSD directory becomes missing, this OSD fails to start\n\t#
        continuously. This problem can be resolved by confirming\n\t# the validity
        of the device file and recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
        [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ; then\n\t\trm
        $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports bluestore
        so we don't need to pass a store flag\n\tceph-volume raw activate --device
        \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
      env:
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sdd
      - name: ROOK_METADATA_DEVICE
      - name: ROOK_WAL_DEVICE
      - name: ROOK_OSD_ID
        value: "5"
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: activate
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-5
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /etc/temp-ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /etc/ceph/admin-keyring-store/
        name: rook-ceph-admin-keyring
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7wfsz
        readOnly: true
    - args:
      - bluefs-bdev-expand
      - --path
      - /var/lib/ceph/osd/ceph-5
      command:
      - ceph-bluestore-tool
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: expand-bluefs
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-5
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7wfsz
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-5
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7wfsz
        readOnly: true
    nodeName: k8s-node-yellow
    nodeSelector:
      kubernetes.io/hostname: k8s-node-yellow
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    shareProcessNamespace: true
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/storage/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/storage/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - hostPath:
        path: /run/udev
        type: ""
      name: run-udev
    - hostPath:
        path: /var/lib/rook/storage/9dff65cc-5b8f-413f-a1ed-8bfb6b93684a_dc89a37c-418e-438d-b392-1f22ba9074ef
        type: DirectoryOrCreate
      name: activate-osd
    - name: rook-ceph-admin-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-admin-keyring
    - name: kube-api-access-7wfsz
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:32:46Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:32:49Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:33:06Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:33:06Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:32:45Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://8f3e49354b08ec37f646f2c3d646af885aacd9c97f896f283ee6f2a06ae00b19
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: log-collector
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:32:50Z"
    - containerID: containerd://c96c233d0bb3e93432447badb83e7d8fc6486a1ba60e79e48cf42321535e0b08
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: osd
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:32:50Z"
    hostIP: 10.0.90.14
    hostIPs:
    - ip: 10.0.90.14
    initContainerStatuses:
    - containerID: containerd://74bb2c0e50d1b276853320585831a9453955934cad8d781b2f4d65c4429e4408
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://74bb2c0e50d1b276853320585831a9453955934cad8d781b2f4d65c4429e4408
          exitCode: 0
          finishedAt: "2024-09-23T15:32:46Z"
          reason: Completed
          startedAt: "2024-09-23T15:32:46Z"
    - containerID: containerd://5295a94982e588ccd4d8ee9b1bfe567074b59e067ab38a0eeb7830ecd0f79faf
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: activate
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://5295a94982e588ccd4d8ee9b1bfe567074b59e067ab38a0eeb7830ecd0f79faf
          exitCode: 0
          finishedAt: "2024-09-23T15:32:47Z"
          reason: Completed
          startedAt: "2024-09-23T15:32:47Z"
    - containerID: containerd://a7d0312c014fda8fe906c8328e93931e7499e6242983fbbe66e7b7eeb9ad7f7e
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: expand-bluefs
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://a7d0312c014fda8fe906c8328e93931e7499e6242983fbbe66e7b7eeb9ad7f7e
          exitCode: 0
          finishedAt: "2024-09-23T15:32:48Z"
          reason: Completed
          startedAt: "2024-09-23T15:32:48Z"
    - containerID: containerd://e6ac6fc536dd5b927ce7b92d4503e0968783297fba182abba04929779e84da4d
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://e6ac6fc536dd5b927ce7b92d4503e0968783297fba182abba04929779e84da4d
          exitCode: 0
          finishedAt: "2024-09-23T15:32:49Z"
          reason: Completed
          startedAt: "2024-09-23T15:32:49Z"
    phase: Running
    podIP: 10.42.205.123
    podIPs:
    - ip: 10.42.205.123
    qosClass: Burstable
    startTime: "2024-09-23T15:32:45Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 16ca4dd5bf8c5cb230cf7abab576b253ab23a5951d425c49e41f1f55cd792d71
      cni.projectcalico.org/podIP: 10.42.3.35/32
      cni.projectcalico.org/podIPs: 10.42.3.35/32
      prometheus.io/port: "9102"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-08-19T12:46:34Z"
    generateName: rook-ceph-osd-6-7bbdf58cd7-
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "6"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: storage
      ceph-osd-id: "6"
      ceph_daemon_id: "6"
      ceph_daemon_type: osd
      device-class: hdd
      failure-domain: k8s-node-blue
      osd: "6"
      osd-store: bluestore
      pod-template-hash: 7bbdf58cd7
      portable: "false"
      rook.io/operator-namespace: storage
      rook_cluster: storage
      topology-location-host: k8s-node-blue
      topology-location-root: default
    name: rook-ceph-osd-6-7bbdf58cd7-xgs6k
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-osd-6-7bbdf58cd7
      uid: 367bf2d5-5072-4ef8-b74a-dfd6badb928e
    resourceVersion: "1349044607"
    uid: a2f8e28c-fd61-4e4a-bdcd-97c2a0f5a648
  spec:
    affinity: {}
    containers:
    - args:
      - --foreground
      - --id
      - "6"
      - --fsid
      - 9dff65cc-5b8f-413f-a1ed-8bfb6b93684a
      - --setuser
      - ceph
      - --setgroup
      - ceph
      - --crush-location=root=default host=k8s-node-blue
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --ms-learn-addr-from-peer=false
      command:
      - ceph-osd
      env:
      - name: TZ
        value: Australia/Sydney
      - name: ROOK_NODE_NAME
        value: k8s-node-blue
      - name: ROOK_CLUSTER_ID
        value: 9f4e59e5-14c0-4d39-ac52-c4fcf6bd6c10
      - name: ROOK_CLUSTER_NAME
        value: storage
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: storage
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: k8s-node-blue
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.4
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_OSD_RESTART_INTERVAL
        value: "0"
      - name: ROOK_OSD_UUID
        value: 9ac61cde-f01d-430c-b857-3cb5cd15d102
      - name: ROOK_OSD_ID
        value: "6"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sda
      - name: ROOK_CV_MODE
        value: raw
      - name: ROOK_OSD_DEVICE_CLASS
        value: hdd
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.6.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: osd
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      startupProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.6.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 720
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-6
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2r9vk
        readOnly: true
      workingDir: /var/log/ceph
    - command:
      - /bin/bash
      - -x
      - -e
      - -m
      - -c
      - "\nCEPH_CLIENT_ID=ceph-osd.6\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
        edit the logrotate file to only rotate a specific daemon log\n# otherwise
        we will logrotate log files without reloading certain daemons\n# this might
        happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
        \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
        --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
        rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed --in-place
        \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif [ \"$LOG_MAX_SIZE\"
        != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE at the 4th line of the
        logrotate config file with 4 spaces to maintain indentation\n\tsed --in-place
        \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\" \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile
        true; do\n\t# we don't force the logrorate but we let the logrotate binary
        handle the rotation based on user's input for periodicity and size\n\tlogrotate
        --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep 15m\ndone\n"
      env:
      - name: TZ
        value: Australia/Sydney
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: log-collector
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2r9vk
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2r9vk
        readOnly: true
    - command:
      - /bin/bash
      - -c
      - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables are
        unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=9dff65cc-5b8f-413f-a1ed-8bfb6b93684a\nOSD_UUID=9ac61cde-f01d-430c-b857-3cb5cd15d102\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
        \"ceph.conf\" must have the \"fsid\" global configuration to activate encrypted
        OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
        This limitation will be removed later. After that, we can remove this\n# fsid
        injection code. Probably a good time is when to remove Quincy support.\n#
        https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
        /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
        = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif not
        config.has_section('global'):\n    config['global'] = {}\n\nif not config.has_option('global','fsid'):\n
        \   config['global']['fsid'] = '$CEPH_FSID'\n\nwith open('/etc/ceph/ceph.conf',
        'w') as configfile:\n    config.write(configfile)\n\"\n\n# create new keyring\nceph
        -n client.admin auth get-or-create osd.\"$OSD_ID\" mon 'allow profile osd'
        mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
        \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary directory\n\t#
        this is needed because when the init container exits, the tmpfs goes away
        and its content with it\n\t# this will result in the emptydir to be empty
        when accessed by the main osd container\n\tcp --verbose --no-dereference \"$OSD_DATA_DIR\"/*
        \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't need it anymore\n\tumount
        \"$OSD_DATA_DIR\"\n\n\t# copy back the content of the tmpfs into the original
        osd directory\n\tcp --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t#
        retain ownership of files to the ceph user/group\n\tchown --verbose --recursive
        ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm --recursive
        --force \"$TMP_DIR\"\nelse\n\t# 'ceph-volume raw list' (which the osd-prepare
        job uses to report OSDs on nodes)\n\t#  returns user-friendly device names
        which can change when systems reboot. To\n\t# keep OSD pods from crashing
        repeatedly after a reboot, we need to check if the\n\t# block device we have
        is still correct, and if it isn't correct, we need to\n\t# scan all the disks
        to find the right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device()
        {\n\t\t# jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
        python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
        sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
        == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device: '
        + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
        \ # don't keep processing once the disk is found\nsys.exit('no disk found
        with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
        > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
        '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device < \"$OSD_LIST\";
        then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device
        < \"$OSD_LIST\")\"\n\tfi\n\t[[ -z \"$DEVICE\" ]] && { echo \"no device\" ;
        exit 1 ; }\n\n\t# If a kernel device name change happens and a block device
        file\n\t# in the OSD directory becomes missing, this OSD fails to start\n\t#
        continuously. This problem can be resolved by confirming\n\t# the validity
        of the device file and recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
        [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ; then\n\t\trm
        $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports bluestore
        so we don't need to pass a store flag\n\tceph-volume raw activate --device
        \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
      env:
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sda
      - name: ROOK_METADATA_DEVICE
      - name: ROOK_WAL_DEVICE
      - name: ROOK_OSD_ID
        value: "6"
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: activate
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-6
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /etc/temp-ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /etc/ceph/admin-keyring-store/
        name: rook-ceph-admin-keyring
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2r9vk
        readOnly: true
    - args:
      - bluefs-bdev-expand
      - --path
      - /var/lib/ceph/osd/ceph-6
      command:
      - ceph-bluestore-tool
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: expand-bluefs
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-6
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2r9vk
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-6
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2r9vk
        readOnly: true
    nodeName: k8s-node-blue
    nodeSelector:
      kubernetes.io/hostname: k8s-node-blue
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    shareProcessNamespace: true
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/storage/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/storage/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - hostPath:
        path: /run/udev
        type: ""
      name: run-udev
    - hostPath:
        path: /var/lib/rook/storage/9dff65cc-5b8f-413f-a1ed-8bfb6b93684a_9ac61cde-f01d-430c-b857-3cb5cd15d102
        type: DirectoryOrCreate
      name: activate-osd
    - name: rook-ceph-admin-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-admin-keyring
    - name: kube-api-access-2r9vk
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-08-19T23:16:51Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-08-19T12:59:02Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-10-10T22:15:10Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-10-10T22:15:10Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-08-19T12:57:27Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://682dd62d994414cbf514cb053c5293b07c5182102592c83c46289216e0d42ca2
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState:
        terminated:
          containerID: containerd://4fdbe618571d75a0142439c2ef76f47c8c8ee134153f5ef32c83040587fe9e9c
          exitCode: 255
          finishedAt: "2024-08-19T23:16:01Z"
          reason: Unknown
          startedAt: "2024-08-19T12:59:02Z"
      name: log-collector
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2024-08-19T23:17:26Z"
    - containerID: containerd://6721c4c9aa802638b26fa8e0227a04342b05ea874e6a03d6daba79a1643a27c5
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState:
        terminated:
          containerID: containerd://d0382060b37aa21ee18cbd44a8c1a2a190cb0dc1cf17a7562f1d1f2ad489a9f7
          exitCode: 255
          finishedAt: "2024-08-19T23:16:01Z"
          reason: Unknown
          startedAt: "2024-08-19T12:59:02Z"
      name: osd
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2024-08-19T23:17:26Z"
    hostIP: 10.0.90.10
    hostIPs:
    - ip: 10.0.90.10
    initContainerStatuses:
    - containerID: containerd://06465564be635f252e7cdf8998cb785659ddd7167afda4240c1d2465b26ebc00
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 1
      started: false
      state:
        terminated:
          containerID: containerd://06465564be635f252e7cdf8998cb785659ddd7167afda4240c1d2465b26ebc00
          exitCode: 0
          finishedAt: "2024-08-19T23:16:50Z"
          reason: Completed
          startedAt: "2024-08-19T23:16:50Z"
    - containerID: containerd://d591bfa45b9f0515db8e91ba15aacd97add6541157aac69d70a28c06a989475f
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: activate
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://d591bfa45b9f0515db8e91ba15aacd97add6541157aac69d70a28c06a989475f
          exitCode: 0
          finishedAt: "2024-08-19T23:17:08Z"
          reason: Completed
          startedAt: "2024-08-19T23:16:51Z"
    - containerID: containerd://8c544e49f58903724c7123704a94685a5257935e9641a745f20598a63085a7ec
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: expand-bluefs
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://8c544e49f58903724c7123704a94685a5257935e9641a745f20598a63085a7ec
          exitCode: 0
          finishedAt: "2024-08-19T23:17:25Z"
          reason: Completed
          startedAt: "2024-08-19T23:17:08Z"
    - containerID: containerd://ce90326a2e7d490f35ae11c1aaae95b1831c98caa7229feddf0aff9d15d04978
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://ce90326a2e7d490f35ae11c1aaae95b1831c98caa7229feddf0aff9d15d04978
          exitCode: 0
          finishedAt: "2024-08-19T23:17:25Z"
          reason: Completed
          startedAt: "2024-08-19T23:17:25Z"
    phase: Running
    podIP: 10.42.3.35
    podIPs:
    - ip: 10.42.3.35
    qosClass: Burstable
    startTime: "2024-08-19T12:57:28Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: c6b158cc2c980941899a41024d33b6eeeae5118a5038968627b0ccb19005006c
      cni.projectcalico.org/podIP: 10.42.3.47/32
      cni.projectcalico.org/podIPs: 10.42.3.47/32
      prometheus.io/port: "9102"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-08-19T13:07:44Z"
    generateName: rook-ceph-osd-7-7bd869fdbd-
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "7"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: storage
      ceph-osd-id: "7"
      ceph_daemon_id: "7"
      ceph_daemon_type: osd
      device-class: ssd
      failure-domain: k8s-node-blue
      osd: "7"
      osd-store: bluestore
      pod-template-hash: 7bd869fdbd
      portable: "false"
      rook.io/operator-namespace: storage
      rook_cluster: storage
      topology-location-host: k8s-node-blue
      topology-location-root: default
    name: rook-ceph-osd-7-7bd869fdbd-gnw7t
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-osd-7-7bd869fdbd
      uid: 795995c8-3b51-403b-b288-410c08d9a0d8
    resourceVersion: "1349044757"
    uid: b27799a0-08b2-43c8-9e5a-f178a66df48a
  spec:
    affinity: {}
    containers:
    - args:
      - --foreground
      - --id
      - "7"
      - --fsid
      - 9dff65cc-5b8f-413f-a1ed-8bfb6b93684a
      - --setuser
      - ceph
      - --setgroup
      - ceph
      - --crush-location=root=default host=k8s-node-blue
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --ms-learn-addr-from-peer=false
      command:
      - ceph-osd
      env:
      - name: TZ
        value: Australia/Sydney
      - name: ROOK_NODE_NAME
        value: k8s-node-blue
      - name: ROOK_CLUSTER_ID
        value: 9f4e59e5-14c0-4d39-ac52-c4fcf6bd6c10
      - name: ROOK_CLUSTER_NAME
        value: storage
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: storage
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: k8s-node-blue
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.4
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_OSD_RESTART_INTERVAL
        value: "0"
      - name: ROOK_OSD_UUID
        value: 434448af-b741-46db-a80b-37926e7ee43c
      - name: ROOK_OSD_ID
        value: "7"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sdb
      - name: ROOK_CV_MODE
        value: raw
      - name: ROOK_OSD_DEVICE_CLASS
        value: ssd
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.7.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: osd
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      startupProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.7.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 720
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-7
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4tqcf
        readOnly: true
      workingDir: /var/log/ceph
    - command:
      - /bin/bash
      - -x
      - -e
      - -m
      - -c
      - "\nCEPH_CLIENT_ID=ceph-osd.7\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
        edit the logrotate file to only rotate a specific daemon log\n# otherwise
        we will logrotate log files without reloading certain daemons\n# this might
        happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
        \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
        --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
        rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed --in-place
        \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif [ \"$LOG_MAX_SIZE\"
        != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE at the 4th line of the
        logrotate config file with 4 spaces to maintain indentation\n\tsed --in-place
        \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\" \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile
        true; do\n\t# we don't force the logrorate but we let the logrotate binary
        handle the rotation based on user's input for periodicity and size\n\tlogrotate
        --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep 15m\ndone\n"
      env:
      - name: TZ
        value: Australia/Sydney
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: log-collector
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4tqcf
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4tqcf
        readOnly: true
    - command:
      - /bin/bash
      - -c
      - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables are
        unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=9dff65cc-5b8f-413f-a1ed-8bfb6b93684a\nOSD_UUID=434448af-b741-46db-a80b-37926e7ee43c\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
        \"ceph.conf\" must have the \"fsid\" global configuration to activate encrypted
        OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
        This limitation will be removed later. After that, we can remove this\n# fsid
        injection code. Probably a good time is when to remove Quincy support.\n#
        https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
        /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
        = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif not
        config.has_section('global'):\n    config['global'] = {}\n\nif not config.has_option('global','fsid'):\n
        \   config['global']['fsid'] = '$CEPH_FSID'\n\nwith open('/etc/ceph/ceph.conf',
        'w') as configfile:\n    config.write(configfile)\n\"\n\n# create new keyring\nceph
        -n client.admin auth get-or-create osd.\"$OSD_ID\" mon 'allow profile osd'
        mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
        \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary directory\n\t#
        this is needed because when the init container exits, the tmpfs goes away
        and its content with it\n\t# this will result in the emptydir to be empty
        when accessed by the main osd container\n\tcp --verbose --no-dereference \"$OSD_DATA_DIR\"/*
        \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't need it anymore\n\tumount
        \"$OSD_DATA_DIR\"\n\n\t# copy back the content of the tmpfs into the original
        osd directory\n\tcp --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t#
        retain ownership of files to the ceph user/group\n\tchown --verbose --recursive
        ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm --recursive
        --force \"$TMP_DIR\"\nelse\n\t# 'ceph-volume raw list' (which the osd-prepare
        job uses to report OSDs on nodes)\n\t#  returns user-friendly device names
        which can change when systems reboot. To\n\t# keep OSD pods from crashing
        repeatedly after a reboot, we need to check if the\n\t# block device we have
        is still correct, and if it isn't correct, we need to\n\t# scan all the disks
        to find the right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device()
        {\n\t\t# jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
        python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
        sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
        == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device: '
        + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
        \ # don't keep processing once the disk is found\nsys.exit('no disk found
        with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
        > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
        '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device < \"$OSD_LIST\";
        then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device
        < \"$OSD_LIST\")\"\n\tfi\n\t[[ -z \"$DEVICE\" ]] && { echo \"no device\" ;
        exit 1 ; }\n\n\t# If a kernel device name change happens and a block device
        file\n\t# in the OSD directory becomes missing, this OSD fails to start\n\t#
        continuously. This problem can be resolved by confirming\n\t# the validity
        of the device file and recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
        [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ; then\n\t\trm
        $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports bluestore
        so we don't need to pass a store flag\n\tceph-volume raw activate --device
        \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
      env:
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sdb
      - name: ROOK_METADATA_DEVICE
      - name: ROOK_WAL_DEVICE
      - name: ROOK_OSD_ID
        value: "7"
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: activate
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-7
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /etc/temp-ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /etc/ceph/admin-keyring-store/
        name: rook-ceph-admin-keyring
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4tqcf
        readOnly: true
    - args:
      - bluefs-bdev-expand
      - --path
      - /var/lib/ceph/osd/ceph-7
      command:
      - ceph-bluestore-tool
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: expand-bluefs
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-7
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4tqcf
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-7
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4tqcf
        readOnly: true
    nodeName: k8s-node-blue
    nodeSelector:
      kubernetes.io/hostname: k8s-node-blue
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    shareProcessNamespace: true
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/storage/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/storage/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - hostPath:
        path: /run/udev
        type: ""
      name: run-udev
    - hostPath:
        path: /var/lib/rook/storage/9dff65cc-5b8f-413f-a1ed-8bfb6b93684a_434448af-b741-46db-a80b-37926e7ee43c
        type: DirectoryOrCreate
      name: activate-osd
    - name: rook-ceph-admin-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-admin-keyring
    - name: kube-api-access-4tqcf
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-08-19T23:16:50Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-08-19T13:07:49Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-10-10T22:15:15Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-10-10T22:15:15Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-08-19T13:07:44Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://cff55fc8bd0326b59e8b55fe92207229d2ef29bc98fd4514737ef8e7c99c6e6a
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState:
        terminated:
          containerID: containerd://924853cb750a67ee6bf842413e7321558d987f51f0b1d72fb01a2bb8e9ca9e34
          exitCode: 255
          finishedAt: "2024-08-19T23:16:01Z"
          reason: Unknown
          startedAt: "2024-08-19T13:07:49Z"
      name: log-collector
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2024-08-19T23:17:17Z"
    - containerID: containerd://bf9761f8b910d4a76b0d7619ff54fb5a1d63209c78737449e8d8add44e073a11
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState:
        terminated:
          containerID: containerd://e175524b11c47447ee580c40196eb171746894fec4f2059f94bf320b6797e1c3
          exitCode: 255
          finishedAt: "2024-08-19T23:16:01Z"
          reason: Unknown
          startedAt: "2024-08-19T13:07:49Z"
      name: osd
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2024-08-19T23:17:17Z"
    hostIP: 10.0.90.10
    hostIPs:
    - ip: 10.0.90.10
    initContainerStatuses:
    - containerID: containerd://3ee6308d0a46047f625d34c9fb5581d30228b708e718efb883970e05b2ade87d
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 1
      started: false
      state:
        terminated:
          containerID: containerd://3ee6308d0a46047f625d34c9fb5581d30228b708e718efb883970e05b2ade87d
          exitCode: 0
          finishedAt: "2024-08-19T23:16:49Z"
          reason: Completed
          startedAt: "2024-08-19T23:16:49Z"
    - containerID: containerd://676695e9d63d0f84e90e6cfd8c44f34d1cbbda2ee1a291e7a4865940ce3aa5b5
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: activate
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://676695e9d63d0f84e90e6cfd8c44f34d1cbbda2ee1a291e7a4865940ce3aa5b5
          exitCode: 0
          finishedAt: "2024-08-19T23:17:07Z"
          reason: Completed
          startedAt: "2024-08-19T23:16:50Z"
    - containerID: containerd://ea2f5af53826dfe0bd114f4354d28d8628cfd8608a927d94fdba0c4d66f31813
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: expand-bluefs
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://ea2f5af53826dfe0bd114f4354d28d8628cfd8608a927d94fdba0c4d66f31813
          exitCode: 0
          finishedAt: "2024-08-19T23:17:15Z"
          reason: Completed
          startedAt: "2024-08-19T23:17:08Z"
    - containerID: containerd://ee408588aaec005ce6c2b5dd2abac9343d9a9c52d95b6f5fde6d047f34ebdb37
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://ee408588aaec005ce6c2b5dd2abac9343d9a9c52d95b6f5fde6d047f34ebdb37
          exitCode: 0
          finishedAt: "2024-08-19T23:17:16Z"
          reason: Completed
          startedAt: "2024-08-19T23:17:16Z"
    phase: Running
    podIP: 10.42.3.47
    podIPs:
    - ip: 10.42.3.47
    qosClass: Burstable
    startTime: "2024-08-19T13:07:44Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 059d221052846c93847eee3907b97de541f0b8d50b50264b901939f617353635
      cni.projectcalico.org/podIP: 10.42.205.121/32
      cni.projectcalico.org/podIPs: 10.42.205.121/32
      prometheus.io/port: "9102"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-09-23T15:32:45Z"
    generateName: rook-ceph-osd-8-77689d978b-
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "8"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: storage
      ceph-osd-id: "8"
      ceph_daemon_id: "8"
      ceph_daemon_type: osd
      device-class: ssd
      failure-domain: k8s-node-yellow
      osd: "8"
      osd-store: bluestore
      pod-template-hash: 77689d978b
      portable: "false"
      rook.io/operator-namespace: storage
      rook_cluster: storage
      topology-location-host: k8s-node-yellow
      topology-location-root: default
    name: rook-ceph-osd-8-77689d978b-jr7db
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-osd-8-77689d978b
      uid: 865ddf39-616d-4009-b05c-bb08ae518857
    resourceVersion: "1330808112"
    uid: 93e84441-3c09-42de-9e71-2f3477a01c57
  spec:
    affinity: {}
    containers:
    - args:
      - --foreground
      - --id
      - "8"
      - --fsid
      - 9dff65cc-5b8f-413f-a1ed-8bfb6b93684a
      - --setuser
      - ceph
      - --setgroup
      - ceph
      - --crush-location=root=default host=k8s-node-yellow
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --ms-learn-addr-from-peer=false
      command:
      - ceph-osd
      env:
      - name: TZ
        value: Australia/Sydney
      - name: ROOK_NODE_NAME
        value: k8s-node-yellow
      - name: ROOK_CLUSTER_ID
        value: 9f4e59e5-14c0-4d39-ac52-c4fcf6bd6c10
      - name: ROOK_CLUSTER_NAME
        value: storage
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: storage
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: k8s-node-yellow
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.4
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_OSD_RESTART_INTERVAL
        value: "0"
      - name: ROOK_OSD_UUID
        value: 7ce42038-77de-4e55-b9b6-bd63148810d3
      - name: ROOK_OSD_ID
        value: "8"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sda
      - name: ROOK_CV_MODE
        value: raw
      - name: ROOK_OSD_DEVICE_CLASS
        value: ssd
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.8.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: osd
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      startupProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.8.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 720
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-8
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nl9xv
        readOnly: true
      workingDir: /var/log/ceph
    - command:
      - /bin/bash
      - -x
      - -e
      - -m
      - -c
      - "\nCEPH_CLIENT_ID=ceph-osd.8\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
        edit the logrotate file to only rotate a specific daemon log\n# otherwise
        we will logrotate log files without reloading certain daemons\n# this might
        happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
        \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
        --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
        rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed --in-place
        \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif [ \"$LOG_MAX_SIZE\"
        != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE at the 4th line of the
        logrotate config file with 4 spaces to maintain indentation\n\tsed --in-place
        \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\" \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile
        true; do\n\t# we don't force the logrorate but we let the logrotate binary
        handle the rotation based on user's input for periodicity and size\n\tlogrotate
        --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep 15m\ndone\n"
      env:
      - name: TZ
        value: Australia/Sydney
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: log-collector
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nl9xv
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nl9xv
        readOnly: true
    - command:
      - /bin/bash
      - -c
      - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables are
        unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=9dff65cc-5b8f-413f-a1ed-8bfb6b93684a\nOSD_UUID=7ce42038-77de-4e55-b9b6-bd63148810d3\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
        \"ceph.conf\" must have the \"fsid\" global configuration to activate encrypted
        OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
        This limitation will be removed later. After that, we can remove this\n# fsid
        injection code. Probably a good time is when to remove Quincy support.\n#
        https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
        /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
        = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif not
        config.has_section('global'):\n    config['global'] = {}\n\nif not config.has_option('global','fsid'):\n
        \   config['global']['fsid'] = '$CEPH_FSID'\n\nwith open('/etc/ceph/ceph.conf',
        'w') as configfile:\n    config.write(configfile)\n\"\n\n# create new keyring\nceph
        -n client.admin auth get-or-create osd.\"$OSD_ID\" mon 'allow profile osd'
        mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
        \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary directory\n\t#
        this is needed because when the init container exits, the tmpfs goes away
        and its content with it\n\t# this will result in the emptydir to be empty
        when accessed by the main osd container\n\tcp --verbose --no-dereference \"$OSD_DATA_DIR\"/*
        \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't need it anymore\n\tumount
        \"$OSD_DATA_DIR\"\n\n\t# copy back the content of the tmpfs into the original
        osd directory\n\tcp --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t#
        retain ownership of files to the ceph user/group\n\tchown --verbose --recursive
        ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm --recursive
        --force \"$TMP_DIR\"\nelse\n\t# 'ceph-volume raw list' (which the osd-prepare
        job uses to report OSDs on nodes)\n\t#  returns user-friendly device names
        which can change when systems reboot. To\n\t# keep OSD pods from crashing
        repeatedly after a reboot, we need to check if the\n\t# block device we have
        is still correct, and if it isn't correct, we need to\n\t# scan all the disks
        to find the right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device()
        {\n\t\t# jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
        python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
        sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
        == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device: '
        + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
        \ # don't keep processing once the disk is found\nsys.exit('no disk found
        with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
        > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
        '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device < \"$OSD_LIST\";
        then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device
        < \"$OSD_LIST\")\"\n\tfi\n\t[[ -z \"$DEVICE\" ]] && { echo \"no device\" ;
        exit 1 ; }\n\n\t# If a kernel device name change happens and a block device
        file\n\t# in the OSD directory becomes missing, this OSD fails to start\n\t#
        continuously. This problem can be resolved by confirming\n\t# the validity
        of the device file and recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
        [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ; then\n\t\trm
        $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports bluestore
        so we don't need to pass a store flag\n\tceph-volume raw activate --device
        \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
      env:
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sda
      - name: ROOK_METADATA_DEVICE
      - name: ROOK_WAL_DEVICE
      - name: ROOK_OSD_ID
        value: "8"
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: activate
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-8
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /etc/temp-ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /etc/ceph/admin-keyring-store/
        name: rook-ceph-admin-keyring
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nl9xv
        readOnly: true
    - args:
      - bluefs-bdev-expand
      - --path
      - /var/lib/ceph/osd/ceph-8
      command:
      - ceph-bluestore-tool
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: expand-bluefs
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-8
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nl9xv
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-8
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nl9xv
        readOnly: true
    nodeName: k8s-node-yellow
    nodeSelector:
      kubernetes.io/hostname: k8s-node-yellow
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    shareProcessNamespace: true
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/storage/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/storage/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - hostPath:
        path: /run/udev
        type: ""
      name: run-udev
    - hostPath:
        path: /var/lib/rook/storage/9dff65cc-5b8f-413f-a1ed-8bfb6b93684a_7ce42038-77de-4e55-b9b6-bd63148810d3
        type: DirectoryOrCreate
      name: activate-osd
    - name: rook-ceph-admin-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-admin-keyring
    - name: kube-api-access-nl9xv
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:32:46Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:32:49Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:33:05Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:33:05Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:32:45Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://c686f1a5776f8064e60f53360d1f1e328d0dff4cc3bfbaa2782de84ab45d1414
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: log-collector
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:32:50Z"
    - containerID: containerd://ff9be09f2627f87bca39d052a45d7e9fa8a29251afa36c9a4c08039056d53f17
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: osd
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:32:50Z"
    hostIP: 10.0.90.14
    hostIPs:
    - ip: 10.0.90.14
    initContainerStatuses:
    - containerID: containerd://a823ddff7b1f34b5cf113c0f3eb1f2fbc9635efe9fbf094512584e67948b0b94
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://a823ddff7b1f34b5cf113c0f3eb1f2fbc9635efe9fbf094512584e67948b0b94
          exitCode: 0
          finishedAt: "2024-09-23T15:32:46Z"
          reason: Completed
          startedAt: "2024-09-23T15:32:46Z"
    - containerID: containerd://54e880919e24eef045fe69199de46e350dd53c41c73b2d1ea910355f40c37d30
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: activate
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://54e880919e24eef045fe69199de46e350dd53c41c73b2d1ea910355f40c37d30
          exitCode: 0
          finishedAt: "2024-09-23T15:32:47Z"
          reason: Completed
          startedAt: "2024-09-23T15:32:47Z"
    - containerID: containerd://8cff1768a27141d486604441ceca4b760ae39c5659c9f0d8e9576778eb0c1cbe
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: expand-bluefs
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://8cff1768a27141d486604441ceca4b760ae39c5659c9f0d8e9576778eb0c1cbe
          exitCode: 0
          finishedAt: "2024-09-23T15:32:48Z"
          reason: Completed
          startedAt: "2024-09-23T15:32:48Z"
    - containerID: containerd://cf6c0f392e1abd6f062c747891dc2cdb3dc0fc3b130de4e62afaf71a01af9f99
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://cf6c0f392e1abd6f062c747891dc2cdb3dc0fc3b130de4e62afaf71a01af9f99
          exitCode: 0
          finishedAt: "2024-09-23T15:32:49Z"
          reason: Completed
          startedAt: "2024-09-23T15:32:49Z"
    phase: Running
    podIP: 10.42.205.121
    podIPs:
    - ip: 10.42.205.121
    qosClass: Burstable
    startTime: "2024-09-23T15:32:45Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 91a06d60c6758d3ecdb03db349cac4483f4e15e81b40e687722727b106d2f3dc
      cni.projectcalico.org/podIP: 10.42.205.120/32
      cni.projectcalico.org/podIPs: 10.42.205.120/32
      prometheus.io/port: "9102"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-09-23T15:32:45Z"
    generateName: rook-ceph-osd-9-697bb9b4fc-
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "9"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: storage
      ceph-osd-id: "9"
      ceph_daemon_id: "9"
      ceph_daemon_type: osd
      device-class: ssd
      failure-domain: k8s-node-yellow
      osd: "9"
      osd-store: bluestore
      pod-template-hash: 697bb9b4fc
      portable: "false"
      rook.io/operator-namespace: storage
      rook_cluster: storage
      topology-location-host: k8s-node-yellow
      topology-location-root: default
    name: rook-ceph-osd-9-697bb9b4fc-gjvpt
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-osd-9-697bb9b4fc
      uid: 5fdb687e-6f43-4ff8-b7a2-3bec4c37ebf8
    resourceVersion: "1330808109"
    uid: ab3f847d-5f9d-4214-b590-29204d06155d
  spec:
    affinity: {}
    containers:
    - args:
      - --foreground
      - --id
      - "9"
      - --fsid
      - 9dff65cc-5b8f-413f-a1ed-8bfb6b93684a
      - --setuser
      - ceph
      - --setgroup
      - ceph
      - --crush-location=root=default host=k8s-node-yellow
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --ms-learn-addr-from-peer=false
      command:
      - ceph-osd
      env:
      - name: TZ
        value: Australia/Sydney
      - name: ROOK_NODE_NAME
        value: k8s-node-yellow
      - name: ROOK_CLUSTER_ID
        value: 9f4e59e5-14c0-4d39-ac52-c4fcf6bd6c10
      - name: ROOK_CLUSTER_NAME
        value: storage
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: storage
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: k8s-node-yellow
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.4
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_OSD_RESTART_INTERVAL
        value: "0"
      - name: ROOK_OSD_UUID
        value: 235bb9b3-60b7-4a47-b0d9-f8d15d173dce
      - name: ROOK_OSD_ID
        value: "9"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sdb
      - name: ROOK_CV_MODE
        value: raw
      - name: ROOK_OSD_DEVICE_CLASS
        value: ssd
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.9.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: osd
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      startupProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.9.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 720
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-9
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vzh9q
        readOnly: true
      workingDir: /var/log/ceph
    - command:
      - /bin/bash
      - -x
      - -e
      - -m
      - -c
      - "\nCEPH_CLIENT_ID=ceph-osd.9\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
        edit the logrotate file to only rotate a specific daemon log\n# otherwise
        we will logrotate log files without reloading certain daemons\n# this might
        happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
        \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
        --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
        rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed --in-place
        \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif [ \"$LOG_MAX_SIZE\"
        != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE at the 4th line of the
        logrotate config file with 4 spaces to maintain indentation\n\tsed --in-place
        \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\" \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile
        true; do\n\t# we don't force the logrorate but we let the logrotate binary
        handle the rotation based on user's input for periodicity and size\n\tlogrotate
        --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep 15m\ndone\n"
      env:
      - name: TZ
        value: Australia/Sydney
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: log-collector
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vzh9q
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vzh9q
        readOnly: true
    - command:
      - /bin/bash
      - -c
      - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables are
        unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=9dff65cc-5b8f-413f-a1ed-8bfb6b93684a\nOSD_UUID=235bb9b3-60b7-4a47-b0d9-f8d15d173dce\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
        \"ceph.conf\" must have the \"fsid\" global configuration to activate encrypted
        OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
        This limitation will be removed later. After that, we can remove this\n# fsid
        injection code. Probably a good time is when to remove Quincy support.\n#
        https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
        /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
        = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif not
        config.has_section('global'):\n    config['global'] = {}\n\nif not config.has_option('global','fsid'):\n
        \   config['global']['fsid'] = '$CEPH_FSID'\n\nwith open('/etc/ceph/ceph.conf',
        'w') as configfile:\n    config.write(configfile)\n\"\n\n# create new keyring\nceph
        -n client.admin auth get-or-create osd.\"$OSD_ID\" mon 'allow profile osd'
        mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
        \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary directory\n\t#
        this is needed because when the init container exits, the tmpfs goes away
        and its content with it\n\t# this will result in the emptydir to be empty
        when accessed by the main osd container\n\tcp --verbose --no-dereference \"$OSD_DATA_DIR\"/*
        \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't need it anymore\n\tumount
        \"$OSD_DATA_DIR\"\n\n\t# copy back the content of the tmpfs into the original
        osd directory\n\tcp --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t#
        retain ownership of files to the ceph user/group\n\tchown --verbose --recursive
        ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm --recursive
        --force \"$TMP_DIR\"\nelse\n\t# 'ceph-volume raw list' (which the osd-prepare
        job uses to report OSDs on nodes)\n\t#  returns user-friendly device names
        which can change when systems reboot. To\n\t# keep OSD pods from crashing
        repeatedly after a reboot, we need to check if the\n\t# block device we have
        is still correct, and if it isn't correct, we need to\n\t# scan all the disks
        to find the right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device()
        {\n\t\t# jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
        python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
        sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
        == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device: '
        + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
        \ # don't keep processing once the disk is found\nsys.exit('no disk found
        with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
        > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
        '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device < \"$OSD_LIST\";
        then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device
        < \"$OSD_LIST\")\"\n\tfi\n\t[[ -z \"$DEVICE\" ]] && { echo \"no device\" ;
        exit 1 ; }\n\n\t# If a kernel device name change happens and a block device
        file\n\t# in the OSD directory becomes missing, this OSD fails to start\n\t#
        continuously. This problem can be resolved by confirming\n\t# the validity
        of the device file and recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
        [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ; then\n\t\trm
        $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports bluestore
        so we don't need to pass a store flag\n\tceph-volume raw activate --device
        \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
      env:
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sdb
      - name: ROOK_METADATA_DEVICE
      - name: ROOK_WAL_DEVICE
      - name: ROOK_OSD_ID
        value: "9"
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: activate
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-9
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /etc/temp-ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /etc/ceph/admin-keyring-store/
        name: rook-ceph-admin-keyring
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vzh9q
        readOnly: true
    - args:
      - bluefs-bdev-expand
      - --path
      - /var/lib/ceph/osd/ceph-9
      command:
      - ceph-bluestore-tool
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: expand-bluefs
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-9
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vzh9q
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 200m
          memory: 1536Mi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-9
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vzh9q
        readOnly: true
    nodeName: k8s-node-yellow
    nodeSelector:
      kubernetes.io/hostname: k8s-node-yellow
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    shareProcessNamespace: true
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/storage/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/storage/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - hostPath:
        path: /run/udev
        type: ""
      name: run-udev
    - hostPath:
        path: /var/lib/rook/storage/9dff65cc-5b8f-413f-a1ed-8bfb6b93684a_235bb9b3-60b7-4a47-b0d9-f8d15d173dce
        type: DirectoryOrCreate
      name: activate-osd
    - name: rook-ceph-admin-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-admin-keyring
    - name: kube-api-access-vzh9q
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:32:46Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:32:49Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:33:05Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:33:05Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:32:45Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://3ffcdc9b41752c1e197fc16663d87ee337d220e540257aabd0620041aaa221ce
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: log-collector
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:32:50Z"
    - containerID: containerd://70b2dd8a330b25a49515be5ffec364f3dcb5c6455755c167505ad220f3258386
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: osd
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:32:50Z"
    hostIP: 10.0.90.14
    hostIPs:
    - ip: 10.0.90.14
    initContainerStatuses:
    - containerID: containerd://cf5ad47969a20a7f59174862ef4569e1f78201c590ef9a95817f35d086ec249e
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://cf5ad47969a20a7f59174862ef4569e1f78201c590ef9a95817f35d086ec249e
          exitCode: 0
          finishedAt: "2024-09-23T15:32:46Z"
          reason: Completed
          startedAt: "2024-09-23T15:32:46Z"
    - containerID: containerd://ccdfd98454abbd06e648bac98c722ff9e235322bf0a744b5a8c73f3363d0f97c
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: activate
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://ccdfd98454abbd06e648bac98c722ff9e235322bf0a744b5a8c73f3363d0f97c
          exitCode: 0
          finishedAt: "2024-09-23T15:32:47Z"
          reason: Completed
          startedAt: "2024-09-23T15:32:47Z"
    - containerID: containerd://6f8ed4c53c6d249b9781c315851915b54e390915f1740a521f88903a6357ba00
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: expand-bluefs
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://6f8ed4c53c6d249b9781c315851915b54e390915f1740a521f88903a6357ba00
          exitCode: 0
          finishedAt: "2024-09-23T15:32:48Z"
          reason: Completed
          startedAt: "2024-09-23T15:32:48Z"
    - containerID: containerd://aeb2b3cadba5c311d5103633f62d180bec593afe347b86528c5a0baa2843677c
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://aeb2b3cadba5c311d5103633f62d180bec593afe347b86528c5a0baa2843677c
          exitCode: 0
          finishedAt: "2024-09-23T15:32:49Z"
          reason: Completed
          startedAt: "2024-09-23T15:32:49Z"
    phase: Running
    podIP: 10.42.205.120
    podIPs:
    - ip: 10.42.205.120
    qosClass: Burstable
    startTime: "2024-09-23T15:32:45Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: e21033a6afd12d71135cc2f4dff362af621b1c3e8a9b9f6f54835965a80e6043
      cni.projectcalico.org/podIP: 10.42.3.2/32
      cni.projectcalico.org/podIPs: 10.42.3.2/32
    creationTimestamp: "2024-08-19T23:18:29Z"
    generateName: rook-ceph-rgw-ceph-objectstore-a-cd95fbc46-
    labels:
      app: rook-ceph-rgw
      app.kubernetes.io/component: cephobjectstores.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: ceph-objectstore
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-rgw
      app.kubernetes.io/part-of: ceph-objectstore
      ceph_daemon_id: ceph-objectstore
      ceph_daemon_type: rgw
      pod-template-hash: cd95fbc46
      rgw: ceph-objectstore
      rook.io/operator-namespace: storage
      rook_cluster: storage
      rook_object_store: ceph-objectstore
    name: rook-ceph-rgw-ceph-objectstore-a-cd95fbc46-xs6cs
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-rgw-ceph-objectstore-a-cd95fbc46
      uid: 7c0aeeb4-600c-4147-ba69-1eff4a35f534
    resourceVersion: "1349044561"
    uid: dbcfc254-511d-42ab-b6e2-4da5596183ed
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app: rook-ceph-rgw
                ceph_daemon_id: ceph-objectstore
                rgw: ceph-objectstore
                rook_cluster: storage
                rook_object_store: ceph-objectstore
            topologyKey: kubernetes.io/hostname
          weight: 50
    containers:
    - args:
      - --fsid=9dff65cc-5b8f-413f-a1ed-8bfb6b93684a
      - --keyring=/etc/ceph/keyring-store/keyring
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=rgw.ceph.objectstore.a
      - --setuser=ceph
      - --setgroup=ceph
      - --foreground
      - --rgw-frontends=beast port=8080
      - --host=$(POD_NAME)
      - --rgw-mime-types-file=/etc/ceph/rgw/mime.types
      - --rgw-realm=ceph-objectstore
      - --rgw-zonegroup=ceph-objectstore
      - --rgw-zone=ceph-objectstore
      command:
      - radosgw
      env:
      - name: TZ
        value: Australia/Sydney
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.4
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_MSGR2
        value: msgr2_false_encryption_false_compression_false
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: rgw
      readinessProbe:
        exec:
          command:
          - bash
          - -c
          - |
            #!/usr/bin/env bash

            PROBE_TYPE="readiness"
            PROBE_PORT="8080"
            PROBE_PROTOCOL="HTTP"

            # standard bash codes start at 126 and progress upward. pick error codes from 125 downward for
            # script as to allow curl to output new error codes and still return a distinctive number.
            USAGE_ERR_CODE=125
            PROBE_ERR_CODE=124
            # curl error codes: 1-123

            STARTUP_TYPE='startup'
            READINESS_TYPE='readiness'

            RGW_URL="$PROBE_PROTOCOL://0.0.0.0:$PROBE_PORT"

            function check() {
              local URL="$1"
              # --insecure - don't validate ssl if using secure port only
              # --silent - don't output progress info
              # --output /dev/stderr - output HTML header to stdout (good for debugging)
              # --write-out '%{response_code}' - print the HTTP response code to stdout
              curl --insecure --silent --output /dev/stderr --write-out '%{response_code}' "$URL"
            }

            http_response="$(check "$RGW_URL")"
            retcode=$?

            if [[ $retcode -ne 0 ]]; then
              # if this is the startup probe, always returning failure. if startup probe passes, all subsequent
              # probes can rely on the assumption that the health check was once succeeding without errors.
              # if this is the readiness probe, we know that curl was previously working correctly in the
              # startup probe, so curl error most likely means some new error with the RGW.
              echo "RGW health check failed with error code: $retcode. the RGW likely cannot be reached by clients" >/dev/stderr
              exit $retcode
            fi

            RGW_RATE_LIMITING_RESPONSE=503
            RGW_MISCONFIGURATION_RESPONSE=500

            if [[ $http_response -ge 200 ]] && [[ $http_response -lt 400 ]]; then
              # 200-399 are successful responses. same behavior as Kubernetes' HTTP probe
              exit 0

            elif [[ $http_response -eq $RGW_RATE_LIMITING_RESPONSE ]]; then
              # S3's '503: slow down' code is not an error but an indication that RGW is throttling client
              # traffic. failing the readiness check here would only cause an increase in client connections on
              # other RGWs and likely cause those to fail also in a cascade. i.e., a special healthy response.
              echo "INFO: RGW is rate limiting" 2>/dev/stderr
              exit 0

            elif [[ $http_response -eq $RGW_MISCONFIGURATION_RESPONSE ]]; then
              # can't specifically determine if the RGW is running or not. most likely a misconfiguration.
              case "$PROBE_TYPE" in
              "$STARTUP_TYPE")
                # fail until we can accurately get a valid healthy response when runtime starts.
                echo 'FAIL: HTTP code 500 suggests an RGW misconfiguration.' >/dev/stderr
                exit $PROBE_ERR_CODE
                ;;
              "$READINESS_TYPE")
                # config likely modified at runtime which could result in all RGWs failing this check.
                # occasional client failures are still better than total failure, so ignore this
                echo 'WARN: HTTP code 500 suggests an RGW misconfiguration' >/dev/stderr
                exit 0
                ;;
              *)
                # prior arg validation means this path should never be activated, but keep to be safe
                echo "ERROR: probe type is unknown: $PROBE_TYPE" >/dev/stderr
                exit $USAGE_ERR_CODE
                ;;
              esac

            else
              # anything else is a failing response. same behavior as Kubernetes' HTTP probe
              echo "FAIL: received an HTTP error code: $http_response"
              exit $PROBE_ERR_CODE

            fi
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 3
        timeoutSeconds: 5
      resources:
        requests:
          cpu: 150m
          memory: 200Mi
      securityContext:
        privileged: false
      startupProbe:
        exec:
          command:
          - bash
          - -c
          - |
            #!/usr/bin/env bash

            PROBE_TYPE="startup"
            PROBE_PORT="8080"
            PROBE_PROTOCOL="HTTP"

            # standard bash codes start at 126 and progress upward. pick error codes from 125 downward for
            # script as to allow curl to output new error codes and still return a distinctive number.
            USAGE_ERR_CODE=125
            PROBE_ERR_CODE=124
            # curl error codes: 1-123

            STARTUP_TYPE='startup'
            READINESS_TYPE='readiness'

            RGW_URL="$PROBE_PROTOCOL://0.0.0.0:$PROBE_PORT"

            function check() {
              local URL="$1"
              # --insecure - don't validate ssl if using secure port only
              # --silent - don't output progress info
              # --output /dev/stderr - output HTML header to stdout (good for debugging)
              # --write-out '%{response_code}' - print the HTTP response code to stdout
              curl --insecure --silent --output /dev/stderr --write-out '%{response_code}' "$URL"
            }

            http_response="$(check "$RGW_URL")"
            retcode=$?

            if [[ $retcode -ne 0 ]]; then
              # if this is the startup probe, always returning failure. if startup probe passes, all subsequent
              # probes can rely on the assumption that the health check was once succeeding without errors.
              # if this is the readiness probe, we know that curl was previously working correctly in the
              # startup probe, so curl error most likely means some new error with the RGW.
              echo "RGW health check failed with error code: $retcode. the RGW likely cannot be reached by clients" >/dev/stderr
              exit $retcode
            fi

            RGW_RATE_LIMITING_RESPONSE=503
            RGW_MISCONFIGURATION_RESPONSE=500

            if [[ $http_response -ge 200 ]] && [[ $http_response -lt 400 ]]; then
              # 200-399 are successful responses. same behavior as Kubernetes' HTTP probe
              exit 0

            elif [[ $http_response -eq $RGW_RATE_LIMITING_RESPONSE ]]; then
              # S3's '503: slow down' code is not an error but an indication that RGW is throttling client
              # traffic. failing the readiness check here would only cause an increase in client connections on
              # other RGWs and likely cause those to fail also in a cascade. i.e., a special healthy response.
              echo "INFO: RGW is rate limiting" 2>/dev/stderr
              exit 0

            elif [[ $http_response -eq $RGW_MISCONFIGURATION_RESPONSE ]]; then
              # can't specifically determine if the RGW is running or not. most likely a misconfiguration.
              case "$PROBE_TYPE" in
              "$STARTUP_TYPE")
                # fail until we can accurately get a valid healthy response when runtime starts.
                echo 'FAIL: HTTP code 500 suggests an RGW misconfiguration.' >/dev/stderr
                exit $PROBE_ERR_CODE
                ;;
              "$READINESS_TYPE")
                # config likely modified at runtime which could result in all RGWs failing this check.
                # occasional client failures are still better than total failure, so ignore this
                echo 'WARN: HTTP code 500 suggests an RGW misconfiguration' >/dev/stderr
                exit 0
                ;;
              *)
                # prior arg validation means this path should never be activated, but keep to be safe
                echo "ERROR: probe type is unknown: $PROBE_TYPE" >/dev/stderr
                exit $USAGE_ERR_CODE
                ;;
              esac

            else
              # anything else is a failing response. same behavior as Kubernetes' HTTP probe
              echo "FAIL: received an HTTP error code: $http_response"
              exit $PROBE_ERR_CODE

            fi
        failureThreshold: 33
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-rgw-ceph-objectstore-a-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/rgw/ceph-ceph-objectstore
        name: ceph-daemon-data
      - mountPath: /etc/ceph/rgw
        name: rook-ceph-rgw-ceph-objectstore-mime-types
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bpgk6
        readOnly: true
      workingDir: /var/log/ceph
    - command:
      - /bin/bash
      - -x
      - -e
      - -m
      - -c
      - "\nCEPH_CLIENT_ID=ceph-client.rgw.ceph.objectstore.a\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
        edit the logrotate file to only rotate a specific daemon log\n# otherwise
        we will logrotate log files without reloading certain daemons\n# this might
        happen when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
        \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
        --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
        rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed --in-place
        \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif [ \"$LOG_MAX_SIZE\"
        != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE at the 4th line of the
        logrotate config file with 4 spaces to maintain indentation\n\tsed --in-place
        \"4i \\ \\ \\ \\ maxsize $LOG_MAX_SIZE\" \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile
        true; do\n\t# we don't force the logrorate but we let the logrotate binary
        handle the rotation based on user's input for periodicity and size\n\tlogrotate
        --verbose \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep 15m\ndone\n"
      env:
      - name: TZ
        value: Australia/Sydney
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: log-collector
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bpgk6
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bpgk6
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      - /var/lib/ceph/rgw/ceph-ceph-objectstore
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        requests:
          cpu: 150m
          memory: 200Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-rgw-ceph-objectstore-a-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/rgw/ceph-ceph-objectstore
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bpgk6
        readOnly: true
    nodeName: k8s-node-blue
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-rgw
    serviceAccountName: rook-ceph-rgw
    shareProcessNamespace: true
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 5
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - name: rook-ceph-rgw-ceph-objectstore-a-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-rgw-ceph-objectstore-a-keyring
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/storage/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/storage/crash
        type: ""
      name: rook-ceph-crash
    - emptyDir: {}
      name: ceph-daemon-data
    - configMap:
        defaultMode: 420
        name: rook-ceph-rgw-ceph-objectstore-mime-types
      name: rook-ceph-rgw-ceph-objectstore-mime-types
    - name: kube-api-access-bpgk6
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-08-19T23:18:30Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-08-19T23:18:31Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-10-10T22:15:08Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-10-10T22:15:08Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-08-19T23:18:30Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://2c501f632297301dbf84c2cb896e40985b2e514240bf92b6820425cac7ef9566
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: log-collector
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-08-19T23:18:32Z"
    - containerID: containerd://523fab45b2abd217e7fdf2569af6b6eac5e6ce5e1de8b4ef0a7971b07015389a
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: rgw
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-08-19T23:18:31Z"
    hostIP: 10.0.90.10
    hostIPs:
    - ip: 10.0.90.10
    initContainerStatuses:
    - containerID: containerd://23c395431d29ee01953e3d13166c2bc459a4071f4922b8a8e96f3a97768fffe2
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://23c395431d29ee01953e3d13166c2bc459a4071f4922b8a8e96f3a97768fffe2
          exitCode: 0
          finishedAt: "2024-08-19T23:18:30Z"
          reason: Completed
          startedAt: "2024-08-19T23:18:30Z"
    - containerID: containerd://1a657c5f6b33176d11c2fc5eef1cdd37e2348a67732270033bf4065047e6f31c
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://1a657c5f6b33176d11c2fc5eef1cdd37e2348a67732270033bf4065047e6f31c
          exitCode: 0
          finishedAt: "2024-08-19T23:18:31Z"
          reason: Completed
          startedAt: "2024-08-19T23:18:30Z"
    phase: Running
    podIP: 10.42.3.2
    podIPs:
    - ip: 10.42.3.2
    qosClass: Burstable
    startTime: "2024-08-19T23:18:29Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 6c6e3cdfe9924d3d64ada70a21dcad45ca04234c7d1e31c2c603324e2f637a87
      cni.projectcalico.org/podIP: 10.42.186.160/32
      cni.projectcalico.org/podIPs: 10.42.186.160/32
    creationTimestamp: "2024-08-11T06:00:29Z"
    generateName: rook-ceph-tools-56bf45f7bf-
    labels:
      app: rook-ceph-tools
      pod-template-hash: 56bf45f7bf
    name: rook-ceph-tools-56bf45f7bf-zqchd
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-tools-56bf45f7bf
      uid: f9c3c2d9-3af2-4528-887f-ebd593e2c1dc
    resourceVersion: "1330833705"
    uid: dc24b508-21ae-485b-b6f5-3f2d8844dd2b
  spec:
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        # Replicate the script from toolbox.sh inline so the ceph image
        # can be run directly, instead of requiring the rook toolbox
        CEPH_CONFIG="/etc/ceph/ceph.conf"
        MON_CONFIG="/etc/rook/mon-endpoints"
        KEYRING_FILE="/etc/ceph/keyring"

        # create a ceph config file in its default location so ceph/rados tools can be used
        # without specifying any arguments
        write_endpoints() {
          endpoints=$(cat ${MON_CONFIG})

          # filter out the mon names
          # external cluster can have numbers or hyphens in mon names, handling them in regex
          # shellcheck disable=SC2001
          mon_endpoints=$(echo "${endpoints}"| sed 's/[a-z0-9_-]\+=//g')

          DATE=$(date)
          echo "$DATE writing mon endpoints to ${CEPH_CONFIG}: ${endpoints}"
            cat <<EOF > ${CEPH_CONFIG}
        [global]
        mon_host = ${mon_endpoints}

        [client.admin]
        keyring = ${KEYRING_FILE}
        EOF
        }

        # watch the endpoints config file and update if the mon endpoints ever change
        watch_endpoints() {
          # get the timestamp for the target of the soft link
          real_path=$(realpath ${MON_CONFIG})
          initial_time=$(stat -c %Z "${real_path}")
          while true; do
            real_path=$(realpath ${MON_CONFIG})
            latest_time=$(stat -c %Z "${real_path}")

            if [[ "${latest_time}" != "${initial_time}" ]]; then
              write_endpoints
              initial_time=${latest_time}
            fi

            sleep 10
          done
        }

        # read the secret from an env var (for backward compatibility), or from the secret file
        ceph_secret=${ROOK_CEPH_SECRET}
        if [[ "$ceph_secret" == "" ]]; then
          ceph_secret=$(cat /var/lib/rook-ceph-mon/secret.keyring)
        fi

        # create the keyring file
        cat <<EOF > ${KEYRING_FILE}
        [${ROOK_CEPH_USERNAME}]
        key = ${ceph_secret}
        EOF

        # write the initial config file
        write_endpoints

        # continuously update the mon endpoints if they fail over
        watch_endpoints
      env:
      - name: TZ
        value: Australia/Sydney
      - name: ROOK_CEPH_USERNAME
        valueFrom:
          secretKeyRef:
            key: ceph-username
            name: rook-ceph-mon
      image: quay.io/ceph/ceph:v18.2.4
      imagePullPolicy: IfNotPresent
      name: rook-ceph-tools
      resources:
        limits:
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 128Mi
      securityContext:
        capabilities:
          drop:
          - ALL
        runAsGroup: 2016
        runAsNonRoot: true
        runAsUser: 2016
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /etc/ceph
        name: ceph-config
      - mountPath: /etc/rook
        name: mon-endpoint-volume
      - mountPath: /var/lib/rook-ceph-mon
        name: ceph-admin-secret
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wdwvg
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wdwvg
        readOnly: true
    nodeName: k8s-node-orange
    preemptionPolicy: PreemptLowerPriority
    priority: 100
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 5
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - name: ceph-admin-secret
      secret:
        defaultMode: 420
        items:
        - key: ceph-secret
          path: secret.keyring
        optional: false
        secretName: rook-ceph-mon
    - configMap:
        defaultMode: 420
        items:
        - key: data
          path: mon-endpoints
        name: rook-ceph-mon-endpoints
      name: mon-endpoint-volume
    - emptyDir: {}
      name: ceph-config
    - name: kube-api-access-wdwvg
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:59:36Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-08-11T06:00:31Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:59:38Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:59:38Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-08-11T06:00:29Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://73ec787d22e02e7f1bb13e443e6ba3be67b2674ab8eb321534286b760f4fd600
      image: quay.io/ceph/ceph:v18.2.4
      imageID: quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906
      lastState:
        terminated:
          containerID: containerd://81aa963053a7ce081f5128b7781e2444ec2208f294b124a5170fd1637797b06e
          exitCode: 255
          finishedAt: "2024-09-23T15:59:18Z"
          reason: Unknown
          startedAt: "2024-09-23T15:47:51Z"
      name: rook-ceph-tools
      ready: true
      restartCount: 4
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:59:37Z"
    hostIP: 10.0.90.12
    hostIPs:
    - ip: 10.0.90.12
    initContainerStatuses:
    - containerID: containerd://a294b1b800caeab61154df67d856148ff9e879bbb7c22fe6c77f0f27723b7f80
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 4
      started: false
      state:
        terminated:
          containerID: containerd://a294b1b800caeab61154df67d856148ff9e879bbb7c22fe6c77f0f27723b7f80
          exitCode: 0
          finishedAt: "2024-09-23T15:59:36Z"
          reason: Completed
          startedAt: "2024-09-23T15:59:35Z"
    phase: Running
    podIP: 10.42.186.160
    podIPs:
    - ip: 10.42.186.160
    qosClass: Burstable
    startTime: "2024-08-11T06:00:29Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: f6105d1dd35fd72e7faab18c3968e136124d5ada1cad5c82028cd2c015b0d4df
      cni.projectcalico.org/podIP: 10.42.186.141/32
      cni.projectcalico.org/podIPs: 10.42.186.141/32
    creationTimestamp: "2024-08-19T12:46:02Z"
    generateName: snap-scheduler-snapscheduler-79bc9d5f9f-
    labels:
      app.kubernetes.io/instance: snap-scheduler
      app.kubernetes.io/name: snapscheduler
      backube/snapscheduler-affinity: manager
      pod-template-hash: 79bc9d5f9f
    name: snap-scheduler-snapscheduler-79bc9d5f9f-v77sr
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: snap-scheduler-snapscheduler-79bc9d5f9f
      uid: ab15cfc4-2671-4a54-85bb-b36f3d31e92e
    resourceVersion: "1330834339"
    uid: bef63299-799f-4cc0-b19c-ad1ce9432b43
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: backube/snapscheduler-affinity
                operator: In
                values:
                - manager
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - args:
      - --secure-listen-address=0.0.0.0:8443
      - --upstream=http://127.0.0.1:8080/
      - --logtostderr=true
      - --v=0
      env:
      - name: TZ
        value: Australia/Sydney
      image: quay.io/brancz/kube-rbac-proxy:v0.17.1@sha256:89d0be6da831f45fb53e7e40d216555997ccf6e27d66f62e50eb9a69ff9c9801
      imagePullPolicy: IfNotPresent
      name: kube-rbac-proxy
      ports:
      - containerPort: 8443
        name: https
        protocol: TCP
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-fblc4
        readOnly: true
    - args:
      - --health-probe-bind-address=:8081
      - --metrics-bind-address=127.0.0.1:8080
      - --leader-elect
      command:
      - /manager
      env:
      - name: TZ
        value: Australia/Sydney
      image: quay.io/backube/snapscheduler:3.4.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 8081
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 20
        successThreshold: 1
        timeoutSeconds: 1
      name: manager
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: 8081
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-fblc4
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-fblc4
        readOnly: true
    nodeName: k8s-node-orange
    nodeSelector:
      kubernetes.io/arch: amd64
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 100
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
    serviceAccount: snap-scheduler-snapscheduler
    serviceAccountName: snap-scheduler-snapscheduler
    terminationGracePeriodSeconds: 10
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - name: kube-api-access-fblc4
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:59:36Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-08-19T12:46:04Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:59:50Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-09-23T15:59:50Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-08-19T12:46:02Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://3cd1a42cf3de944774afdfbc7f421f5e39577beafe9ced11d5cf5c19f4aa0acd
      image: sha256:1cc34085032615fd12cd7cd56c8ae69d2d1bbc881d1e753cd392883d2ca1c9aa
      imageID: quay.io/brancz/kube-rbac-proxy@sha256:89d0be6da831f45fb53e7e40d216555997ccf6e27d66f62e50eb9a69ff9c9801
      lastState:
        terminated:
          containerID: containerd://469b7e55f855811413326a21c7974c02786d5e0c873070c8d4292187969959b6
          exitCode: 255
          finishedAt: "2024-09-23T15:59:18Z"
          reason: Unknown
          startedAt: "2024-09-23T15:47:47Z"
      name: kube-rbac-proxy
      ready: true
      restartCount: 3
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:59:38Z"
    - containerID: containerd://19ef84c6b6dc9f6deed49c6a5285735218b3f684dd3d779c0816b7910d9c49d8
      image: quay.io/backube/snapscheduler:3.4.0
      imageID: quay.io/backube/snapscheduler@sha256:3f0dcddb9d51fcd511fad1d38c52b22d49933be78c7729666c7e6b5521ca1df9
      lastState:
        terminated:
          containerID: containerd://c7ad12f89588f2bee819a00e987824fa03fcdb8a6098fe5b2c07e8ca0d4050d5
          exitCode: 255
          finishedAt: "2024-09-23T15:59:18Z"
          reason: Unknown
          startedAt: "2024-09-23T15:47:50Z"
      name: manager
      ready: true
      restartCount: 10
      started: true
      state:
        running:
          startedAt: "2024-09-23T15:59:39Z"
    hostIP: 10.0.90.12
    hostIPs:
    - ip: 10.0.90.12
    initContainerStatuses:
    - containerID: containerd://74569305964e775a02b7d1ef7a12b933d918977c4fc96069017d22fc31462d03
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 3
      started: false
      state:
        terminated:
          containerID: containerd://74569305964e775a02b7d1ef7a12b933d918977c4fc96069017d22fc31462d03
          exitCode: 0
          finishedAt: "2024-09-23T15:59:36Z"
          reason: Completed
          startedAt: "2024-09-23T15:59:36Z"
    phase: Running
    podIP: 10.42.186.141
    podIPs:
    - ip: 10.42.186.141
    qosClass: Burstable
    startTime: "2024-08-19T12:46:02Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: c13db8d5867c236d44559eaef757e6e5337265e48ea51e755c65d0acb372f568
      cni.projectcalico.org/podIP: 10.42.3.45/32
      cni.projectcalico.org/podIPs: 10.42.3.45/32
      kubectl.kubernetes.io/default-container: manager
    creationTimestamp: "2024-08-22T01:16:47Z"
    generateName: volsync-856d96bddc-
    labels:
      app.kubernetes.io/instance: volsync
      app.kubernetes.io/name: volsync
      control-plane: volsync-controller
      pod-template-hash: 856d96bddc
    name: volsync-856d96bddc-hfdx2
    namespace: storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: volsync-856d96bddc
      uid: 5f5776e1-1c5b-4806-b90d-4615398fa8af
    resourceVersion: "1349044333"
    uid: bdc3a298-3e74-4c62-8008-ba2ba14efc99
  spec:
    containers:
    - args:
      - --secure-listen-address=0.0.0.0:8443
      - --upstream=http://127.0.0.1:8080/
      - --logtostderr=true
      - --tls-min-version=VersionTLS12
      - --v=0
      - --ignore-paths=/metrics
      env:
      - name: TZ
        value: Australia/Sydney
      image: quay.io/brancz/kube-rbac-proxy:v0.17.1
      imagePullPolicy: IfNotPresent
      name: kube-rbac-proxy
      ports:
      - containerPort: 8443
        name: https
        protocol: TCP
      resources:
        limits:
          cpu: 500m
          memory: 128Mi
        requests:
          cpu: 5m
          memory: 64Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-m7rhz
        readOnly: true
    - args:
      - --health-probe-bind-address=:8081
      - --metrics-bind-address=127.0.0.1:8080
      - --leader-elect
      - --rclone-container-image=quay.io/backube/volsync:0.10.0
      - --restic-container-image=quay.io/backube/volsync:0.10.0
      - --rsync-container-image=quay.io/backube/volsync:0.10.0
      - --rsync-tls-container-image=quay.io/backube/volsync:0.10.0
      - --syncthing-container-image=quay.io/backube/volsync:0.10.0
      - --scc-name=volsync-privileged-mover
      command:
      - /manager
      env:
      - name: TZ
        value: Australia/Sydney
      image: quay.io/backube/volsync:0.10.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 8081
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 20
        successThreshold: 1
        timeoutSeconds: 1
      name: manager
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: 8081
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 64Mi
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: timezone
        readOnly: true
        subPath: Australia/Sydney
      - mountPath: /usr/share/zoneinfo
        name: timezone
        readOnly: true
      - mountPath: /tmp
        name: tempdir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-m7rhz
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - bootstrap
      image: quay.io/k8tz/k8tz:0.8.0@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      imagePullPolicy: IfNotPresent
      name: provide-timezone
      resources:
        requests:
          cpu: 10m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/zoneinfo
        name: timezone
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-m7rhz
        readOnly: true
    nodeName: k8s-node-blue
    preemptionPolicy: PreemptLowerPriority
    priority: 100
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: volsync
    serviceAccountName: volsync
    terminationGracePeriodSeconds: 10
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        sizeLimit: 100Mi
      name: timezone
    - emptyDir:
        medium: Memory
      name: tempdir
    - name: kube-api-access-m7rhz
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-08-22T01:16:49Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-08-22T01:16:49Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-10-10T22:15:04Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-10-10T22:15:04Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-08-22T01:16:47Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://c455e17fdce99282d5b70636ec519d9f1c9c7d7c5f812fe5ac19614cc5f1af47
      image: quay.io/brancz/kube-rbac-proxy:v0.17.1
      imageID: quay.io/brancz/kube-rbac-proxy@sha256:89d0be6da831f45fb53e7e40d216555997ccf6e27d66f62e50eb9a69ff9c9801
      lastState: {}
      name: kube-rbac-proxy
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-08-22T01:16:50Z"
    - containerID: containerd://4a20b18d617997ec5f559b6f7730d9fbbe1aa53f0387dec1179f474c49d08592
      image: quay.io/backube/volsync:0.10.0
      imageID: quay.io/backube/volsync@sha256:48ce43289339a144a2eed356d58e77dc22382d46713dec944c439536531699cc
      lastState: {}
      name: manager
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-08-22T01:16:50Z"
    hostIP: 10.0.90.10
    hostIPs:
    - ip: 10.0.90.10
    initContainerStatuses:
    - containerID: containerd://252c1f842ac987f69a8702dd722ae9286254cbd4839ff45305c1ff537b4e3abc
      image: sha256:9e4d26636084203ae2dc70b8f4749188c5b96baa1b5231d1616dc1736e07386c
      imageID: quay.io/k8tz/k8tz@sha256:7bba4420c8decfad816cc77a180f121cfc1e0e51058dec7662837b4ba41812b7
      lastState: {}
      name: provide-timezone
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://252c1f842ac987f69a8702dd722ae9286254cbd4839ff45305c1ff537b4e3abc
          exitCode: 0
          finishedAt: "2024-08-22T01:16:49Z"
          reason: Completed
          startedAt: "2024-08-22T01:16:49Z"
    phase: Running
    podIP: 10.42.3.45
    podIPs:
    - ip: 10.42.3.45
    qosClass: Burstable
    startTime: "2024-08-22T01:16:47Z"
kind: List
metadata:
  resourceVersion: ""
